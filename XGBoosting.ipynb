{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoosting",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM7I1287zih7osbl0G63fZr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ferrariagustinpablo/Python-Machine-Learning-notebooks/blob/main/XGBoosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxxcWp0RLuyZ"
      },
      "source": [
        "# XGBoosting for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpuyNC4oI-cZ"
      },
      "source": [
        "For supervised learning. Relies on labeled data.\n",
        "\n",
        "**AUC**: Area Under the ROC curve, is the most versatile and common evaluation metric used to judge a quality of a **binary classification problem**. Higher AUC meand a more sensitive, better performing model.\n",
        "\n",
        "When dealing with multi-class classification problem it is common to use the accuracy score (higher is better) and a look at the overall confusion matrix to evaluate quality of a model.\n",
        "\n",
        "Supervised learning considerations:\n",
        "Features must be categorical or numerical. When numerical, values should be scaled. Categorical features almost be encoded like one-hot encoding (dummies)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1ns3nGMLyOH"
      },
      "source": [
        "XGboosting is an incredibly popular machine learning library.\n",
        "\n",
        "Speed and performance.\n",
        "Core algorithm is parallelizable. Makes feasible to train models in big data.\n",
        "\n",
        "Consistently outperforms all other single-algorithm methods in machine learning competitions.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQopqPeZTxfL"
      },
      "source": [
        "## Code for XGBoosting for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7h0yb4aI6PY"
      },
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create arrays for the features and the target: X, y\n",
        "X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
        "\n",
        "# Create the training and test sets\n",
        "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Instantiate the XGBClassifier: xg_cl\n",
        "# binary:logistic - use when you want probability rather than just decision.\n",
        "xg_cl = xgb.XGBoostClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "xg_cl.fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels of the test set: preds\n",
        "preds = xg_cl.predict(X_test)\n",
        "\n",
        "# Compute the accuracy: accuracy\n",
        "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
        "print(\"accuracy: %f\" % (accuracy))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRFPdtKbT1MG"
      },
      "source": [
        "## Simple DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmCum8wVQWlU"
      },
      "source": [
        "Decision trees as base learners\n",
        "\n",
        "Base learner: Individual learning algorithm in an ensemble algorithm.\n",
        "\n",
        "XGBoost is an ensemble learning algorithm.\n",
        "\n",
        "Individual decision trees tend to overfit. (Low bias, high variance) we need low bias and low variance.\n",
        "\n",
        "XGBoost uses CART. Cart trees unlike common decision trees, contain in each leaf a **real-valued score**. Regardless if they are used for classification or regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caCa45-2SXOR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9a23b373-56df-4ca8-8ea3-cf3e7cbc5aba"
      },
      "source": [
        "import sklearn.datasets\n",
        "\n",
        "df = sklearn.datasets.load_breast_cancer()\n",
        "df.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XqRxsYXTGo6"
      },
      "source": [
        "X, y = df.data, df.target\n",
        "print(X[:5])\n",
        "print(y[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJX1emxfTd_M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "860d76b6-1161-4002-d05a-e3bd1a85652b"
      },
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create the training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Instantiate the classifier: dt_clf_4\n",
        "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "dt_clf_4.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set: y_pred_4\n",
        "y_pred_4 = dt_clf_4.predict(X_test)\n",
        "\n",
        "# Compute the accuracy of the predictions: accuracy\n",
        "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
        "print(\"accuracy:\", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.9649122807017544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ3jBydugxKa"
      },
      "source": [
        "## XGBoosting with CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo7z6Qc-UDLX"
      },
      "source": [
        "XGBoosting is really a ensemble meta-algorithm.\n",
        "\n",
        "The goal is to convert many weak learners into an arbirarily strong learner. A weak learning is a model that is slightly better than chance. We only need a model that is slightly more accurate than pure randomnes will be considered a weak learner.\n",
        "\n",
        "Convert a collection of weak learner into a strong learner.\n",
        "\n",
        "Strong learner: Any algorithm than can be tuned to achieve good performance.\n",
        "\n",
        "How does it work: Iteratively learning a set of weak models on subsets of the data. Weighting each weak prediction according to each wek learner performance. Combine all weighted predictions to obtain a single weighted prediction.\n",
        "\n",
        "We are also going to use Cross-Validation. CV is a robust method for estimating the expected performance of a ML model on unseen data, by generating many non-overlapping train/test splits and reporting the averate test set performance across all data splits.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNDMDzCsg14P"
      },
      "source": [
        "### XGB CV of accuracy (error)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdg8CoklT76d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "782bb73d-2d19-406c-c7d7-3b7131d352a3"
      },
      "source": [
        "\n",
        "# We have to create a matrix if we are going to use cross validation object of XGB\n",
        "# Create the DMatrix from X and y: churn_dmatrix\n",
        "df_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
        "\n",
        "# Perform cross-validation: cv_results\n",
        "cv_results = xgb.cv(dtrain=df_dmatrix, params=params, \n",
        "                    nfold=5, num_boost_round=5, \n",
        "                    metrics=\"error\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)\n",
        "\n",
        "# Print the accuracy\n",
        "print('CV Accuracy score for binary classification problem Brest Cancer:',(1-cv_results[\"test-error-mean\"]).iloc[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   train-error-mean  train-error-std  test-error-mean  test-error-std\n",
            "0          0.030759         0.005573         0.079165        0.032645\n",
            "1          0.022409         0.004267         0.065083        0.021454\n",
            "2          0.017578         0.005566         0.061559        0.019432\n",
            "3          0.013183         0.004819         0.051017        0.022643\n",
            "4          0.011864         0.001765         0.049263        0.020664\n",
            "CV Accuracy score for binary classification problem Brest Cancer: 0.9507372000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfQYYm2Ng7kv"
      },
      "source": [
        "### XGB CV of AUC. (The best for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXEPdJL-g_Dg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1fe7a168-2945-473b-af3f-c56b28d4351c"
      },
      "source": [
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
        "\n",
        "# Perform cross_validation: cv_results\n",
        "cv_results = xgb.cv(dtrain=df_dmatrix, params=params, \n",
        "                    nfold=3, num_boost_round=5, \n",
        "                    metrics=\"auc\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)\n",
        "\n",
        "# Print the AUC\n",
        "print((cv_results[\"test-auc-mean\"]).iloc[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
            "0        0.987225       0.001301       0.961473      0.024760\n",
            "1        0.993244       0.004295       0.969078      0.022616\n",
            "2        0.995224       0.003751       0.972491      0.024377\n",
            "3        0.997125       0.002042       0.971354      0.025405\n",
            "4        0.997610       0.001871       0.974002      0.026527\n",
            "0.9740019999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTygTvIQhT3v"
      },
      "source": [
        "## When to use XGBoosting in classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt-BcZcBhWpk"
      },
      "source": [
        "When I should or shouldn't use XGBoosting. \n",
        "For any Machine Learning model that fits the following criteria:\n",
        "\n",
        "1- You have a large number of training samples. >1000 samples, < 100 features\n",
        "2- The number of features < number of training samples.\n",
        "3- Mixture of categorical and numeric features or just numeric features.\n",
        "\n",
        "NOT USE XGB:\n",
        "\n",
        "1- Not ideally suited for image recognition, Computer Vision or NLP.\n",
        "(More suitable using deep learning models)\n",
        "2- Small training set or the number of features > number of training samples.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6O1nC_Ul6dJ"
      },
      "source": [
        "# XGBoosting for regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MXvkmo4l8_m"
      },
      "source": [
        "Common regression metrics: RMSE and MAE(Mean absolute error but less common)\n",
        "\n",
        "Common regression models: Decision Trees and Linear Regression\n",
        "\n",
        "Objective(loss) functions and base learners. Loss function match difference between our predicted model and the real one. \n",
        "\n",
        "Goal: Fin the smallest possible loss function.\n",
        "\n",
        "Loss function names in xgboost:\n",
        " 1- reg:linear - use for regression problems\n",
        "\n",
        " 2- reg:logistic - use for classification problems if you want only a decision not a probability.\n",
        "\n",
        " 3- binary:logistic - use when you want probability rather than just decision.\n",
        "\n",
        "Xgboost want base learners that when combined create final prediction that is non-linear\n",
        "Each base learner should be good at distinguishing or predicting different parts of the datasets (slightly better than randomness). All predictions combined, bad predictions cancel out and become one very good prediction.\n",
        "\n",
        "Two kinds of base learners: Decision Trees and Linear Regression.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7U-aAR3sp1l"
      },
      "source": [
        "## XGboosting with Trees for regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGcJaJ8RtL6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1c094c07-a848-478a-c49b-b9fd79ab5116"
      },
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Import Boston Dataset in datasets of sklearn\n",
        "load_boston = datasets.load_boston()\n",
        "X, y = load_boston.data, load_boston.target\n",
        "\n",
        "# Create the training and test sets\n",
        "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Instantiate the XGBRegressor: xg_reg\n",
        "xg_reg = xgb.XGBRegressor(objective=\"reg:linear\", n_estimators=10, seed=123)\n",
        "\n",
        "# Fit the regressor to the training set\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set: preds\n",
        "preds = xg_reg.predict(X_test)\n",
        "\n",
        "# Compute the rmse: rmse\n",
        "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "print(\"RMSE: %f\" % (rmse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16:52:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "RMSE: 9.749041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0w9oAavvHqE"
      },
      "source": [
        "## XGBoosting Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzoAbDlxvHNW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "37a558e4-aa65-4134-9a37-594ee2ecf155"
      },
      "source": [
        "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
        "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
        "DM_test =  xgb.DMatrix(data=X_test, label=y_test)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
        "\n",
        "# Train the model: xg_reg\n",
        "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
        "\n",
        "# Predict the labels of the test set: preds\n",
        "preds = xg_reg.predict(DM_test)\n",
        "\n",
        "# Compute and print the RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
        "print(\"RMSE: %f\" % (rmse))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16:56:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "RMSE: 6.631172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6sQUTXcv41v"
      },
      "source": [
        "### Evaluate regressor model with Cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enrpqErswcYw"
      },
      "source": [
        "### RMSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYwu6M0bwAxw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "38938d1c-3379-4159-eb26-a877aa0732a2"
      },
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
        "\n",
        "# Perform cross-validation: cv_results\n",
        "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)\n",
        "\n",
        "# Extract and print final round boosting round metric\n",
        "print((cv_results[\"test-rmse-mean\"]).tail(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16:58:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[16:58:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[16:58:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[16:58:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
            "0        17.120438        0.057830       17.151866       0.295723\n",
            "1        12.353698        0.034427       12.510376       0.372386\n",
            "2         9.017977        0.038795        9.245965       0.314345\n",
            "3         6.690101        0.047236        7.060159       0.317659\n",
            "4         5.069411        0.048644        5.571861       0.252100\n",
            "4    5.571861\n",
            "Name: test-rmse-mean, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIFLL0h2wdhy"
      },
      "source": [
        "### MAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FzwCik9wesV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "32853316-5681-4798-aa94-70993ddb5ec1"
      },
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
        "\n",
        "# Perform cross-validation: cv_results\n",
        "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)\n",
        "\n",
        "# Extract and print final round boosting round metric\n",
        "print((cv_results[\"test-mae-mean\"]).tail(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[17:00:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[17:00:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[17:00:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[17:00:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "   train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
            "0       15.584812       0.087903      15.567934      0.345122\n",
            "1       11.036514       0.069404      11.044831      0.347553\n",
            "2        7.827224       0.052691       7.886081      0.315104\n",
            "3        5.596108       0.044331       5.718952      0.288004\n",
            "4        4.062843       0.052193       4.285985      0.175467\n",
            "4    4.285985\n",
            "Name: test-mae-mean, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgXXWolM1YLq"
      },
      "source": [
        "# Regularization in XGBoost\n",
        "\n",
        "Regularization is a control on model complexity\n",
        "Want models that are both accurate and as simple as possible.\n",
        "\n",
        "Gamma: a parameter for tree based learners.\n",
        "\n",
        "Alpha or L1: regularization on leaf weights (rather than on feature weights), larger values mean more regularization. In linear or logisticRegression.\n",
        "\n",
        "Lambda or L2: Much smoother penalty than L1. Causes the leaf weights to smoothly decrease instead of enforcing strong sparsity constraints on the leaf weights as L1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq2gfXcY2d7B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "c1ddbb15-375a-49fa-d180-fca2c71b4457"
      },
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "reg_params = [1, 10, 100]\n",
        "\n",
        "# Create the initial parameter dictionary for varying l2 strength: params\n",
        "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
        "\n",
        "# Create an empty list for storing rmses as a function of l2 complexity\n",
        "rmses_l2 = []\n",
        "\n",
        "# Iterate over reg_params\n",
        "for reg in reg_params:\n",
        "\n",
        "    # Update l2 strength\n",
        "    params[\"lambda\"] = reg\n",
        "    \n",
        "    # Pass this updated param dictionary into cv\n",
        "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "    \n",
        "    # Append best rmse (final round) to rmses_l2\n",
        "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
        "\n",
        "# Look at best rmse per l2 param\n",
        "print(\"Best rmse as a function of l2:\")\n",
        "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\",\"rmse\"]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19:34:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:34:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:34:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:34:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:34:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:34:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Best rmse as a function of l2:\n",
            "    l2       rmse\n",
            "0    1   6.022222\n",
            "1   10   7.201520\n",
            "2  100  10.692149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpvkBxsV6iDo"
      },
      "source": [
        "# Plot Tree function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6ONU8m16dgQ"
      },
      "source": [
        "XGBoost has a plot_tree() function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the plot_tree() function along with the number of trees you want to plot using the num_trees argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1__c5ik_6hIZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "d3b38d96-43f9-4622-8ac8-175eaf30e444"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
        "\n",
        "# Train the model: xg_reg. # 10 boosting trees\n",
        "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
        "\n",
        "# Plot the first tree\n",
        "xgb.plot_tree(xg_reg, num_trees=0)\n",
        "plt.show()\n",
        "\n",
        "# Plot the fifth tree\n",
        "xgb.plot_tree(xg_reg, num_trees=4)\n",
        "plt.show()\n",
        "\n",
        "# Plot the last (and best) tree sideways. \n",
        "# To plot it sideways you should write additional keyword argument rankdir=\"LR\"\n",
        "xgb.plot_tree(xg_reg, num_trees=9, rankdir='LR')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19:34:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAB+CAYAAACHx8KbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3Rc13Wvv+mDaejAoIMACBYQFNhJkQSLJUoWRUmWZEuMqeYoTp4jJ857+eetpZXlJH7r+b3YcZInO7ItOWqkKIqkJFaxib2BFSQIEEQddGAwAKb3ue+Pq7liByiRBAndb61ZLNPO3XPO756zzz57KwRBQEZGRkbm7qAc7QbIyMjIfJuQRVdGRkbmLiKLroyMjMxdRBZdGRkZmbuILLoyMjIydxFZdGVkZGTuIuphnpfjyWRkZGRujXYg/0ZPDie6MjJ3DEEQiEajhEIhwuGw9IhEIkQiEQKBAMFgkHA4jCAIRCIRYrEYCoUCtVqNQqFAo9Gg1WrR6/VoNBrUajVqtRqNRiM9p9FoUCgUo325MjKALLoyd4D4gZtIJEI4HMbtdmO32xkcHKS/v5/+/n78fj/BYJBIJAJwhVDGhVOv16PT6dBoNCiVSpRKJSqVShJgQRAIh8OEQiECgQDhcJhoNHqFgAuCgFKpRKfTodfrSUpKIiMjg+TkZFJTU0lJSUGr1UoiLouzzJ1GMcyJNNm9IHNT4v0nGo3S399Pd3c3NpuNrq4uBgYGADCZTGRmZpKcnExaWhrp6eno9XpJVLVa7YjFTqFQcCunKMPhMMFgkGAwSCAQYHBwkL6+PgYHBxkaGmJoaIhIJIJOpyMtLY2CggIKCwvJyMhAp9NJ3ykjcwvc1L0gi67MLSMIAqFQCLvdTl1dHXV1dTidTiwWC7m5uYwbNw6r1UpKSgpqtRqlUnlPziIFQSAWixGLxXC5XPT09NDW1obNZsPpdKLVasnPz2fy5Mnk5eVhNBrvuWuQuSeRRVfmmxNf0jc3N1NVVUV7ezuJiYlMmjSJCRMmkJycTEJCwpgQpfi1ulwumpqaqK2tpauri+TkZGbMmMGUKVPGzLXK3BFk0ZX5+giCgNfr5fDhw5w6dYr09HTmzJnD+PHj0ev1wNhefl/uPuno6KCqqoqGhgYKCwv5zne+Q2Zm5pi+fpmvhSy6Ml+PcDjMkSNHOHjwINOnT2f+/PlYLJYbikx8Y0sQBLRaLZFIBLVajSAIBAIBFAoFer3+mvfH3RXRaBS9Xo9SKYaPxyMYQNxoU6lUhEIhacMtHvWgVCpJSEggFosRDAbRarWoVCopMiL+nfHIiPhrA4EAKpVK8t0GAgGUSuVNfcyCIBAMBqmpqWHPnj3k5eWxYsUKTCaTLL4ycWTRlbk1BEHA5/Px7rvvYjabeeqpp0YkKoFAgD/96U/k5uZitVrZsmUL//AP/0Brays7d+7E7XazcOFC5s2bJ73H5XLh9Xp55513MBgMVFRUsGjRIhQKBZ9//jlvvPEGkUiE73znO0ycOBGHw4HNZuO1115j9erVfP755xQUFPCrX/2KrVu3MjAwgE6nY+XKlbz//vuEw2FycnJYunQp//Vf/4UgCMycORODwUBVVRX9/f08/fTT+Hw+9u/fTzAY5IUXXiA3N3dYG0UiEQ4fPsyhQ4d48cUXycvLk4VXBoYRXflEmsw1xGIx3n//fcrKyvjhD3+I2WweVkwEQaCuro4jR45QUlJCSkoKQ0NDAGRlZfHKK68wd+5czpw5gyAI9PX1sWvXLk6fPk1VVRW5ubk899xzbNu2TQojmzt3LuvWreMXv/gFjz32GAsWLODFF19EoVDQ3d1Nfn4+v/3tb/nlL39JNBqlqqqKF198kYaGBs6ePUt3dzcvv/wy+/fv5/jx4+j1en7wgx+wefNmxo0bx8svv0xRURH19fVUVVUxZ84csrKyqK+vH9ZG8RjhRYsW8cILL/Dee+/hdDq/ufFlxjyy6MpcQ2dnJ+FwmAULFkhL/ZGQmpqK1WolPz8fq9WKWi2GgRuNRjQaDS0tLSxatIja2lrWr1/PlClTWLx4MePHj6e1tZXa2lr6+vqIxWIAJCUloVaraW1tpaSkhKSkJAYHBzGZTFKExKlTp/j1r39NY2MjCoVCirm12WwkJCSg1+sJBoN0dHRgMpmwWCwMDQ1hMBg4ceIEJ0+eZNKkSUydOpVPPvmEEydOUFJSMuJrVigU5OfnM2/ePI4cOXJL4Wwy307kwxEy1+B0OklLS7slwY37a41GI0ajEa/XKz0XCoU4ePAg06ZNo6ysjGg0ilqt5sSJE+Tk5FBRUcErr7xCW1sbmZmZqFQq6b0XLlwgNzcXrVbLwMAABw8eZNWqVRgMBh544AHmzJlDT08P7e3txGIxIpEI0WiUzMxMGhoaCIfDaDQa0tPT6enpwe/3YzQaCYfDzJkzB61Wy4YNG1CpVLz88st0dnayf/9+XnzxxVu69uzsbE6ePDni98h8e1H9/Oc/v9nzN31SZmyi0+nYuXMn06ZNkzaZhkMQBDo6Oti7dy/z5s2jqamJL774gilTplBXV8f/+l//i3PnzjE0NMT06dNJS0ujpKQEQRBwu92cPn2a5uZmFixYgCAIHDlyhMLCQr744guWLFmCWq3mV7/6Fdu3b+fQoUMUFBSwb98+uru7aWho4PHHH8dut9PS0kI0GmXFihWcPXuWtrY20tLSWLp0KQcPHqStrY0pU6bgcDg4e/YsDQ0NFBQUYDQaaW9vp7W1lcmTJ1NYWDhie0WjUTZv3szUqVPJysqS/boyLuA3N3pS3kiTuYa46B0/fpyXXnqJlJSUEfl0PR4PAwMDpKen4/F48Pl8mM1mdDoddrsdALPZTGpq6hWfFw6H6e7uRq/Xk5aWRjgcxu/3Y7FY8Hg8mM1mAPr7+/F4PABkZmYSCARwu90kJSVhsVgIhUL09fWRlpaGXq/H5/MxODhIRkYGGo0Gp9OJz+cjMzNTOtyh0WjIyMggFovR19eHSqUiIyNjRLP8eNTFpk2b8Hq9rFq1SnKpyHyrkaMXZG4dQRCorq5m8+bNVFZWMmfOHHQ6nTyL+5JIJEJjYyObN29m/PjxLF++HI1GM9rNkrk3kEVX5ushCAIul4sdO3bQ2NjIjBkzmD17NomJiffksd47TTzeuKamhoMHD6LX61m+fDn5+fnfOlvI3BRZdGW+PvH+4XK5OH78OGfOnCEhIYGKigomTpwo5VcYq8TdJk1NTZw6dYqenh6Ki4tZuHCh5L+VBVfmKmTRlbk9xA8E2O12zpw5Q21tLZFIhJycHEpLSyksLCQpKUlKxXg/iZEgCNL1eb1e2tvbaWhooLW1FUEQyM3N5YEHHmDcuHGym0VmOGTRlbn9xPuN2+2ms7OTS5cuYbPZ8Hg86HQ6MjMzycrKIjc3l6SkJBITE68bCXG3xOvqfh5PaON0Ounp6aGjo4Pe3l7cbjdarZbc3FyKi4sZN24cycnJ991NRGZUkUVX5s4TnylGo1E8Hg89PT10dXXR2dnJ0NAQPp8PAK1Wi8lkIjk5GYvFgtlsxmQyYTQaMZnMaLValEqFlLQ8vnyPC178z3i/vTw9Y/z7I5EIAZ8Pr8eDy+vF7XbjdrsZHBzE6XQSDAaJxWLodDoSExOxWq3k5ORgtVpJTk6+Ih2ljMzXQBZdmdElLobxZOIej4fBwUFcLhdutxuPx4PL5WXXLgPjx3tIShq8RkzjnxPnchFWKL4S6Xh1iUyfD8uZM0R+8APMqamYzWZJ6C+vSCELq8wdQBZdmXubaFTgs8/A54Pnnwel8ut3O0lEYzHYvRvFpUvw538OBsNtaq2MzLDIoitz7xKLwc6d0NEBL74IWu1t/vCDB+H0aVF4zWaQZ7Yydx45y5jMvYkgwL590NgIL7wAt/1sgVIJlZUwbx68+SYMDYlfKiMzisiiKzMqCAIcPQrnz8OPfiTOcO/IJFShgDlzYNky+M//hP5+WXhlRhVZdGXuOoIAZ87A4cOi4BoMd3jVr1DAAw/AE0/AW2+B3S4Lr8yoIYuuzF1FEODCBdGP++Mfg8l0l75YoYCyMvje9+APf4De3rv0xTIyVyKLrsxdQxBE/+1nn4n7WhbLXd7XUihgwgR49ll4+21ReOUZr8xdRhZdmbuCIIDNBh9+KApuWtooBRLEhffpp+GPfwSHQxZembuKLLoydxxBgO5u+OADePllyMwc5cgthQImTvzK1TA4KAuvzF1DFl2ZO4ogiAEDb78NK1dCXt43F9yhoSF27Ngh1VK7nEgkwtatW6VjxzdEoYDJk2H5cnFzze2WhVfmrjB2c/LJjDqCIIbG/vGPAkuWOEhMVODxaAmFQlgsFhwOB2azGZVKxcDAAElJSSQkJEinyoLBIA6HA0EQUKlUxGIxLBaLVAwyGo3S19dHQkICGo2GoaEhLBYLeXl50nMgFsY0GAwMDQ3h9XoJhUKMGzdO/J6pUyEYFF0Nf/VXYDRK7Y9GozgcDlQqFQqFguTkZAKBAIODg9e0VUZmpMg10mTuCIIgTh5//3t45BGB6uo1nD59Co1GQ01NDadPn6ampoa9e/dy8eJFOjs70el0ZGRkSELW19fH66+/jkaj4c0330StVrNnzx7y8vL44IMPEASBo0ePEgwG2blzJ729vWi1WlavXk12djb/+q//ilarZefOnRQWFvLmm2/S0NBAR0cHM2bMEL9HoYCsLIhEYMcOMbTsy/zAkUiEf/zHf0StVvP+++8zY8YMfve73+F0Otm6dSuzZ8+Wq0XIXI+b1kiT3QsydwSfT5w8LloE06YpWbHicTo7OxkcHJTKlcdiMbq6ulCr1Vy8ePGa9Ismk4mMjAweffRRcnNzWbZsGU6nU6pcYbFYaGxsJBqNYjQaqa+vv6IicWJiIgsXLsTpdOLxeAgEAlitVqnmmoRSCQsXwrhx4k5fJAKARqPBYrEwbdo0TCYT7e3tuN1uvv/97xMIBKSZtIzMrSCLrsxtJxCA//ovmDlTPAymUIiFJE0mEx0dHWRkZJCbm0t5eTmvv/46M2bM4OGHH2bHjh1UVVXh9/sBpMxkgUBAylAW/3cgEMBkMrFq1Sp2797NxIkTmTt3Lrt37yYYDOL3+wkGg4RCIYLBIEajUZpJP/LII9c2WqmERx4BnU6MaYtGicVi0vtDoRA6nY5wOExzczORSASLxXKXLSszFpDdCzK3lVAI3n0XiothyRJRyy6fwZaUlGC1WpkwYQKNjY2oVCrJd7p06VI2b95MSUkJiYmJeDweotEoJpNJysOr1+sxGAzodDqSkpJoa2tjyZIlBINBfD4fs2fPBsQy8snJySQlJaHVajGbzbS0tJCQkMD27duZO3futWWGlEoxquHIERgaIpyXRyAQwGw2o9frKSwspLy8nIsXL7J06VLy8vJkn67M9ZBLsMvcHcJhcXWemAgrVogaBuKMdfPmzQiCwPLly9HeIJWY3+/n4sWLTJ06FZVKdVvbVl9fz7Zt20hLS8Pj8fAXf/EX16/tJgjiVP3NN2HBAnG6LgurzK0hp3aUufNEo/Dxx6I+PfssXK6ZgiDg9/tRq9U3FNw7jSAI+Hw+otEoBoNBiki4IUND8Lvfwfe/DyUlsvDK3ApyakeZO0ssBps2ia6Fp5++UnBBTCxuMBhGTXDjbTAajVgsFtRq9fBugcREeOUVceouHxeWuY3IoivzjYjFxEgrh0Os+jBmIqgUCrBa4bnnxF1Bj2e0WyQzRpBFV+aWEISvJn2CAPv3Q0sLrFp1m6s+3AsoFFBaCosXwzvviIco4gaQZ74yXxNZdGVuiVAIzp4VQ1mPHYPqajGfgl4/2i27Q8SToBcUwPr10N4uJkMPBke7ZTL3KfJGmsyIEQQ4dEisZfbqq+LBrZ/8RCw9NuYJh+F//29Ytw76+mDvXjF3g7zBJnMt8kaazO0hGhXdm62t8E//JLoTEhJGu1V3AUEQY3ffe0/MwN7fD1u3jnarZO5TZNGVGTH19bBli/j3SETc2G9rG9023TWsVrHApcEgivAnn4DXO9qtkrkPkUX3W4YgCNd9xGIxYrHYdZ8DMUph9WoYGBBTFPziF6LujBs3yhd0N4gnPv/jH+H996GiAmpq4OzZm9ryZvaU+fYi+3THCIIgEI1GCQQChEIhAoEATqcTp9OJz+fD7/fj9/uvyCUQDoeJxWJEv8wzoFAopBjW+EEGnU6HVqslHE7jF7+YRHFxO//tv+kpL0/GYEhAq9Wi1+vRaDRj5khs3JbxXA+hUAi3243T6cTr9RKw2VD+4Q9kFRZS/cgjBEIhQqEQkUhEeu/l9gRQq9VoNBrJnnG7JSQkkJCQgMVikY4s63Q6dDrdmLLptwz5RNpYIT6LikQiDA4O0tvbS3d3N52dnbjdbgKBAIIgSIM2MTGRpKQkDAYDBoMBvV5/zaBWKpUolUopX200GkUQBMLh8BUCPTioxuMRUCjacbudDA4OSolootEoOp1OygqWm5tLRkYG6enpaLXa4U9/3WXifT4SiRAMBrHb7djtdrq6urDb7VJGMkASSZPJRHJyMgaDgYSEBIx6PSnRKMHERDRfviYuklfbM/5dcXvGbRu/Efr9flwuF0NDQ5K9Y7EYGo0Go9FISkoKOTk5pKenk5GRgdFolG6O95JdZSRk0b0f+WpZH2NgYACbzcalS5fo6ekhHA5jMBhITU0lOzubvLw8LBYLRqORhOvsbN2OgflVN7m2S4TDYTweDx6PB7vdTmdnJz09PdjtdgRBwGq1UlJSQklJCRkZGVIO2rslGHFbulwuuru7uXTpknSjAkhJSSElJYWsrCyysrIwmUwYjUb014mDuxNtvt4YDIfDeL1evF4vQ0NDdHZ20t/fT09PDz6fD4PBQE5ODuPHjycvL4/U1FSUXya7kIV41JFF935CEAQCgQAtLS1UV1fT3t6OSqWioKCAiRMnkpubi9FolGaP9+IAi/ep+BK9p6eHlpYWmpqacDgcJCUlMX36dCZNmoTZbJbE4na3IRAIYLPZqKmpoa2tjXA4THp6OpMmTaKgoICUlBRptn8v2vFq4j7hSCSC2+2ms7OTpqYmOjo6cDqdZGZmUlFRwYQJEzCZTHfErjIjQhbde534YOrq6mL//v20traSk5PDtGnTGDduHGaz+b4QheGIC2FXVxfV1dVcvHiR5ORkKisrKS0tHVlOhGE+PxqNYrPZOHjwIJ2dnWRlZfHAAw8wbtw4EhMTb3v2snsBQRDwer10dnZy/vx5GhoaMJlMzJs3jylTpqDT6cZE/7mPkEX3XkYQBNra2tiyZQuhUIjKykrKysrG/EC5XCD37duH3W7noYceoqKi4vopF4chHA5z/vx5du/ejclkYuHChZSWlkpJdsayLS8nbteuri4OHTpES0sL06dPp7KyEoPB8K2xwygji+69SjAYZNu2bTQ1NbFixQpKSkpuOhMTBIH29nZcLhdutxuPx0NeXh4TJ0685rVer5fa2loMBgOTJk1CqVQSjUa5dOkSXq+XKVOmoNfrpRm2xWJBr9dTW1uLXq9n/PjxuFwuLl68iNVqpaCggL6+PlpaWigsLCQzM5O2tja6u7uZOHEiFouFhoYGXC6XdNO4dOkSgUCAyZMno9VqbzjgBUGgr6+PrVu34vf7WblyJcnJySMSCEEQ6O3t5aOPPsJsNvPd736XzMzMmy6tY7EYDQ0N6HQ6rFYrvb295Ofn43Q6uXjxomSzq+ufxVNUDgwMkJubiyAItLa20t3dzaRJk0hOTpZeU1tbS0ZGBnl5eQwMDNDU1ERubi5ZWVkoFAq8Xi8DAwPk5ORcYTeNRkNdXR2xWIyysjKi0Sg1NTUYjUZKS0tRKpX09vZiMBiGrVwhCAIej4eDBw9y+vRpVqxYQXl5uex2uPPcVHTlyhGjRCgU4p133sFoNPLCCy+QkZEx7GDw+Xz84Q9/ID09nc8++0wafOPHj5ciDi5evIhSqeTdd9/FarUSiUTIzs5GqVRy5swZtmzZglKp5PTp01RUVOB2u3n99dcpLi6WfMgnT57EZDLxwQcfkJ6ezsaNGyksLOSNN96gqKiINWvWMG7cON58800KCgrYuHEjSUlJrF+/HqPRyOHDh9FoNGzbtg2v10tLSwuTJ0++oYgqFApMJhMPPPAAWq2WNWvWMHny5GFnZoIg0NzczDvvvMMTTzzBww8/PKwvUxAEenp6WLduHfn5+Rw4cIBPP/2UZcuWUVtbSygUYu/evYTDYcZ9GYTscrm4dOkSZrOZ1atXc+jQISorK7lw4QK7d++msLAQrVYrieD777+PUqlky5YtlJaW8tvf/pacnBxWr17Ngw8+iFqtZu3atVRVVWGxWFi/fj0JCQkcO3aMgYEBzpw5Q19fHz09PRw/fhy3201VVRUGg4FYLMY///M/k5OTQ25u7k37i0KhQKfTUVJSQllZGZs3b8btdlNUVCTPeO8scmHKew1BEDhw4AApKSk8+eSTN50FXv6eixcvcvz4cUKhEKWlpcyfP5/y8nLC4TCnT5/m888/l+p6nTt3Dp/Pd8VGVXt7O+PGjWPx4sVUV1cTCoU4fvw4WVlZxGIxTp48yezZs5k1axYHDx7E4XAwc+ZM8vPzOXXqFJFIhNmzZ6NWq6mpqcFsNjN//nz6+vqoq6sjLy+PRYsWceHCBXw+H3q9nqSkJIIjTA6jVCqpqKjgiSee4KOPPhr2IIHf72ft2rW8+uqrTJw4cUQbi4IgcOLECc6ePYtOp2PhwoXodDoApk2bxvz58yV7OJ1O9u3bx+HDh0lJSSEhIYHKykrJ/bF3717UajUul0sS3HA4TG1tLfPmzSMvL48LFy4QCoVIT08Hvpplx0P1Ojo6yM/PZ/HixZw/f57GxkYmT57MokWLOHbsGG1tbUybNo3Zs2dTVVUl+ahvBYVCQVpaGj/+8Y85d+4czc3Nt/R+mduLLLqjgCAIVFdXs3Tp0hEv9RQKBcXFxZSXl7N48WIeffRRMjMz+fWvf01PTw+nTp0iJyeH0tJSIpGIFPHwxz/+EYfDAcDcuXPp7Ozk008/lVwHFouF5ORkSazjQfyCILBo0SLWrl3L+fPnsVgsTJkyhTVr1tDc3Ex+fj6JiYl8+OGH9PT0MG3aNAYGBtiwYQNOpxODwYDdbqempoakpKQR20ahUDB58mR8Ph+eYXLYtra2kp+fLy3ZR4JSqWTixInMnj2bCRMmXOE/VigUUkjegw8+SFNTE21tbZSVlZGdnY1CobjC/ePxeLBarbhcLlavXg0gxeYqlUrUajXBYJCUlBT279+PRqPB4/FQX19PWVkZgiAwc+ZM+vv72bBhAy6XiyVLlnD69Gm2b99OIBDg0UcfZfv27Rw8eJBQKIRKpfra7oGEhASWLVvG8ePHv9b7ZW4Pt75jIXNbUCqVRL4s9X2rxAdfcXGxVJX2lVde4dKlS+zYsYMHHngAk8lEVlYWGo1GOp2WmprKa6+9Rl1dHR6Ph/b2dnp6ejhx4gQJCQlkZGTgcDjo6OiguLiYJUuWoFQq+c1vfkNJSQmVlZWEw2GamprIz8/n1VdfZWhoiMbGRnJzc3nttddoaWmhv7+fCxcuMG/ePCZPnsxbb73FY489NuLri0dzDCcuarWacDj8tWx4PWw2G3v27KGyspJYLMb06dMpLS2lurqapqYmKisrr3h9YWEhKpWKnJwc6uvr8Xq9qFQqEhMTcblc2O12ioqK6O3t5Z/+6Z/4l3/5F86dO0dHRwfnz5+nvr4egNdee43GxkYGBwcpKiripz/9KQcPHiQ5OZmKigqmT5/Ohx9+SF5e3je6PkEQpBurzOghW38UUCgUzJ07l61bt/Liiy+OaBDEfZGRSASbzcaBAwcAeOKJJyQXwuTJk5kwYQIAK1as4N1332X69OkYDAY+/fRTKisr2bFjByqViueffx6r1QpAWloa5eXl6HQ6Nm3ahFqt5rHHHmPHjh309/fzwAMPkJ+fz9q1awkEAlJxyQ8++IBYLMbKlStxu918+umnKJVK/uzP/gxBENi8eTNNTU08+eSTI7aNIAgcP36c9PR0DAbDTV9bUFDAp59+SkNDA+PHj7+ljTe3201vby9Hjx4lGAxy/vx5mpub2bp1Kzt27OC5557j6aefllwo8VNkVVVVko/30Ucf5eOPP6a5uZmnn36aw4cPk52dzfe//302bdpEVlYWM2fOpLu7mw8++ICcnBwWLVrEI488gs1m4+TJk0QiEd555x2USiUrV66kpaWFgwcPotVqeeqppzh58iS1tbWYzWYefPBBafNSEAQmTpw44lWEIAi4XC527tzJSy+9NKL3yNwZ5OiFUSIajfLxxx/j8Xh47rnnMJlMI/JHXo/rvW8kiVUUCsVdS8AykmuLRqPs27ePM2fO8Fd/9VfD2iQuoG+//TaLFi1i7ty5wx45vpXrvfpzbpetbqfdR9pnbDYba9as4aGHHmLWrFnyRtqdRQ4Zu1eJRqMcPnyYAwcOsHTpUmbMmDGiTbWxRjQapbm5mS1btpCRkcFTTz2F0Wgc0XsFQcDtdrNx40YGBwf57ne/S3Fx8Tc+aDEWiMVi9PX1sWvXLvr6+nj22WfJz8//1tvlLiCL7r2MIAgMDAywY8cOWltbmT59OrNnzyYpKemePeZ7O4jHs9bU1HDo0CG0Wi2PPfYYhYWFX2ujKBaLYbPZ2LFjBy6Xi1mzZlFRUUFiYuKYtuPlxMey3+/n0qVLHD58GJ/Px+LFi6VwPJm7giy69wNxn9vx48eprq5GrVZTVlbGlClTSEtLu+/T/MUzpDmdTpqbmzlz5gw9PT2UlpaycOFCrFbrNxbH+Abc4OAgx44do66uDpVKxYQJEygrKyMjIwO9Xn9f2/Fq4rkY4hua586dw263U1hYyPz588nPz79vckuMIWTRvZ+I+zbtdjsXLlyQYl5TUlKYMGEC+fn5WK3WezJlYpy4wIbDYQYGBujo6KC+vp7u7m4UCgUlJSWUl5eTm5t7R487RyIRBgYGqKuro7a2lsHBQUwmEwUFBRQVFZGVlYXFYpHCsO5FW8aJ31Ci0Sh+v5+enh7a2tqkqAe9Xs+ECROYNGkS2dnZ30o31d9ivkcAACAASURBVD2ELLr3K/HfJhAI0NPTQ0NDA21tbfT39yMIAklJyaSlFZCdnUtWlhGTyYTJZLrhbO6bziKvRzAYlFIQ9vf309nZSV9fHw6HQwpny8nJYcKECeTk5EiHCO6mIFyeP9fhcGCz2Whubqa3txefz4dWqyUlJYX09HSysrJIS0vDaDRKOYiv5+64Pekyb2xTn8+H1+slODREc1cXfXY7/f39+Hw+1Gq1dMS4pKSE9PR0KaWnLLT3BLLojiXE5WQUuz3Exo0Ozp4NU1lpJxyuwePx4Pf7EQQBjUYjVSewWCyYzWYSEhKkZNvxmXJ8tqxVqYjEYkS/LDMTjUalMKlwOEwgEMDtduN2u/H5fIS+rJYQT5puNBpJS0sjOztbSmB+L1eUuDxNot/vp7+/H7vdTk9PD/39/VIi83gy8Xjyd6PRiNlsxmAwSIng44/LZ8wqlY5YLEwsFpVmqJFI5Aqbxg+AuN1uKXl5KBQCEBOnGwyUXbqE0uEg6fvfJ2P2bEyJifdVOspvKbLojgXiP5PTCbt3w8WLYp3EefPEirwKxVfL+vgAjpebiYtlvFxP/BGNRqVKEakHDuCcOpVocrJU+SAuNnHxNplMWCwWSbzjVRXGYqRA3JaX2ysUCuH1enG73Xi9Xun/wuEw4XBYKtMTjQrs3ZvGvHkOjEakk2zxEkhxuyYkJGA2mzGZTFeU8bnCptEoNDXB55+LneDRR6GkBJRKufz7vYssuvc7giAWhNyzBxoaYOFCmD0bdLrbOO7efBMefxyGSaIiMzzRKPzbv8Grr0Ji4m34QEEQHzabKL5+PyxbJhbLvCoTmsw9wU1FVz6Rdg8jCOBwwK5d0NICixbBE0/cZrGNI06Vb/OHytwWFArxMW4c/OVfQmcn7NgB27fDQw/BlCmgVssz3/sEWXTvMeK6Z7eL46qzEyor4ZlnxEnNHRtXsujeHyiVkJcHf/7n0N0tdpIdO2DJEpg+XRbf+wBZdO8hBAH6+8UVZGcnLF0Kzz13h8U2jiy69xcKBWRnw0sviXfonTth3z5RfCsq7lKnkfk6yKI7ysR1bnBQFNvWVlFsf/AD0Y1w15BF9/5EqYTMTFi1Srxj79oFe/eK4jttmjzzvQeRRXcUEQQYGhInKY2Notg+8wxotaMwTmTRvb9RKCA9HVauFDcCdu78Snzlme89hSy6o4AggMcjjonqali8GJ588g5tkI0UpVIW3bGAQgFpaaL4xme+X3zx1cxXFt9RRxbdu4ggiNE+Bw9CVRXMmQN///eg198D40CpFGOdZMYGV4vvjh1izOF3viNvuI0ysujeBQQBQiFRaPftEyccf/d3fBk4/00+VyxGGT9ZdjXxww8jOhWmUkEs9vUbI3NHiVfIuLpCMSDlubgm30L87+np8MMfihtun38OBw6IvqypU2XxHQXkGml3EEEQJ4/nz8NvfgPt7fDaa7B8OZhMt6ev79mzB5vNdt3nmpub2b9//w3aJkgPQJ7p3uOcPXuWM2fOXPe5wcFBPvvssxvmchAEQTzllJEBL7wAL74I586JJzhqasTfXXYt3TXkE2l3iFhMPL25ebN4Kunxx8U+73D0s2vXLqmE99SpU7HZbAwMDDBnzhxqamoIBoMsXrxYmtXEYjGOHDmC3W4nFAphNBoRBIHKykpOnjzJpEmT6O3tpaOjg7lz53Lu3DlUKhUpKSmEQiECgYCU7Hvx4sW4XC5OnjzJ0NAQDz30EFarldjatRweGkIxZQoul+uK16WkpEhVgGWGJxqF//N/fFitW1m4sILu7m5ycnLw+/20trYyc+ZMOjo66O3tZdGiRZhMJkAUx5qaGurr66WcFrFYjHnz5tHS0kLyl0e0a2trmTFjBm1tbbhcLoqLi+no6CAlJQWHw4Hf72fOnDloNBqOHj2Kw+Fg1qxZlJaWftVIQRDjfLduBZcLHnsMSkvFFc91qKuro6uri0AgwIwZMzCZTBw5cgSABQsWDFta6VvGTU+kyTPd24wgQFcXvP22eGDoySfhlVfEqB6FAoxGI8eOHcPv92O323E4HGzatAmPx8Obb77Jzp07pXLdcRQKBW1tbYTDYfr6+giFQnR0dHDu3DlsNht1dXWsWbOG9PR0GhsbOXbsGKmpqQiCwMGDB2lubmZgYICGhgYuXLjAu+++S1paGlVVVVIGLYVKRWN9PYODg9TX11NTU8Pvf/97srOz2b9/P+fPnx8Nc963aDQ66urq6evro6+vD0EQePvttzEYDPzud79j7dq1pKenX+P28Xg82Gw2NBoNLS0tKBQK9uzZg8fjoaqqijVr1kiFLz/77DMyMjJISEhg9+7dDAwMcOHCBfx+P/v27WPz5s2Ew2EaGhoIBoNXNjAe5/vnfw7PPiv6vf7f/4Pm5uu6mbxeL6dPn0YQBHbu3MmGDRsIBAIMDAzw6aef3kFLjj1k0b1NCIKYjOajj+C998RENH/911BUJK7c4+j1epYtW8bGjRspKCjA7/fT2dmJy+Vi+vTplJWV8cEHHzAwMCAtFxUKBRqNhuTkZKxWK4mJiaSlpREMBqUEKcuWLWPDhg1Eo1Gys7NZvXo1oVBIem9iYiKpqakEAgEyMzM5dOgQeXl50ixLoVKh/bKSbWpqKh6PR5pF5efn09fXNxpmvW9RKpWsWLGCzz//nISEBPR6PV1dXdhsNsrLy1m6dCnr1q2jtbVVcvPEfyuLxUJ2djYWiwWr1UogEECr1aJUKlm+fDm7du2ioaGBOXPmsHr1ahwOB0qlUsxMZjKRkZFBMBgkKyuLkydPotPpyMzMvFFDIT9fPF78+OPi0uz3vxd9YfGcD4i+ZJPJJLWnu7ub4uJiJk2aRGdn51207P2PvF78hggCBAJw6BAcOybmR/je924ca6tQKJg/fz6ffPIJP/rRjxAEgdzcXPLy8rBardjtdlJTUzl79iwDAwM8//zzUjUEn8+H2+0mEAgwNDREMBikq6uLaDSKUqmkoKCA1tZWtFotZrMZm81GT0+P9L0OhwOTyUQ4HCY5OZnc3FwCgQAJCQkICgVDg4NoenpwOBwkJSUxZcoUPvnkE1pbW1myZMldtuz9joKJEyeybt0g48ePJzU1lUmTJpGamkpJSQmnTp0iLy+PtrY2Dhw4wKuvvoparWZwcJC+vj46Ozvp7e0lKSmJ/v5+dDodHR0dqFQqxo8fT19fH3q9HqvVSltbG319fbS3t2O327FYLNJvmJSUxIQJEwiHw5KwX9vUL3M7FBeLmw719bB2LaSmihsQGRkMDQ1ht9vp7e1lYGCA2bNns3v3bskVJjNyZJ/u10QQxFVYdbXoRpg8GR5+ePiIBL/fj9vt5vjx4yxfvhyFQoHL5cLn80nLRo1GI1WOePrppwGkmW984AiCgFKpJBKJoFKppFSESUlJOBwOKal1IBCQohtisRgqlYp169Yxf/58qqurMRgMfO9734MtWxhQq1HMmSNFPBiNRvr7+zEYDFgsljGXvvFOEY3Cr34V4vnnPZw+vZ/HH38ctVqNz+fD6XSSmJiI1+sFRH/99u3beeGFF1CpVFIKTo1GQzgcRq1WE41GUSgUUm7fYDBISkoKQ0NDqFQqdDodLpdL8gFrNBoikQhffPEF+fn5eL1eLly4wN/8zd+M/DcMh+HsWTHOt7gY94MP4vtyVRUOh0lNTZVm2CkpKV+rrt0YRk7teLuJxcSsX5s2gdks+m3T0kYWjdDR0cGBAwdYtmwZaWlp132NIAg4nU4pb+3tFDtBEDh8+DANDQ3odDqWLVtGamoqiu3bISkJHnzwtn3Xt5VoFH7xCydZWVt4+OEHKSwsvOFv6PF4UCgUGAyG2/4719TUcPLkSVQqFZWVlRQUFNzadwiCKL4nTog+36lTxUMW3zTWcewji+7tQhDEHAmbNoknLZ96Ssy2NyZu8p9/LsaxLVgw2i2577nt+XRHm8tP9Zw8KW5YPPjgKB+hvKeR8+l+U+J+2337xD738MMwc6YYXTNm+pxSKR+OkLk+CgUYDGLi9HnzxLwOv/41PPKImNdhTA2EO48susMQiYhx5Nu3Q3k5/I//8VV5nDGFLLoyw6FQgMUiZmVyOGDLFjGByOOPizG+cgmhESGL7g0QBDFqZv16cYn44x+P3G97XxEKQW8v9PWJF9fcLJbs0WpHu2X3HYIgnjPo7xezx9ls4oGYeIz2mCGe1+Gll8RBsmmTmFTniSfEvjOmLvb2I/t0+eoEZDy7ocslHtTp7BT9tsXFX0XVjDnsdlixAi5dEi++pET076amjnbL7jsEQVwR/cVfgNstujx/9CP45S/HaN+Br8J4GhpE8U1LE2e+6elj+KKHRT6RdiMEQRwcn38u9ptQCPbvFw/m5ObC3/4tjB8/xldNKSlija3BQXF6VlEhRjHI3DIKhWg+vV7sV263WGppTKNQiD7dCRPELE5Tp4rHMTduFGcv8U242lrZffUl32rRjUTg//5fcTayYYO449zTI4rtwoXfkhW2Uin66HQ6MePUk0+OkXCM0cFqFRN4gXgacc6cMXzDvhyFQszVO2MG/Pf/Lt64/+3fxE23Tz8Vl4xHj8qJdfgWuxeiUXjnHVFgvV5xsrdxo7i6/lYMkssZGBAzqfv9cPiw6IiU+VoIAmzbJt7HfvIT+NWvvqX3sHim/k2b4H/+T9H3O2kSfPihOBse24Ps/ggZu1r8L/+3IAhEIhEikQjRL9MPxmIx6TXxXLJKpRK1Wo1arb7ihMy1AeEK9uwR+8KXB4NoahKz3JWU3OYLG0WutuHlfw+Hw1K+XUGlQr1oEQqnk7BajcLlQqlUolKp0Gg0N7Xlt+GU2q32zYkTYxQVJbB4cQCPR7TPSPvmmLGnQiHGfXu94vIRoK5O3JFeuxYKC+HLk5WXc/m/o9GoZNf4eI996aJQKBQolUoUCgUqlQq1Wo1KpbrCfveqbe/qTFcQBKLRKIFAgGAwiMvlor+/H5fLhdvtxul04nK5rhCEuLGVSqX0iBsv/ufluWFjsZj0w8RfH/9RjEYjSUlJBAJF/PM/F2G3+ykvVzNzpo4HH9QxZ46WnBzFfXUTjg/6QCAgZX2K29TpdDI0NITf75fsGY1Gr2vP3L4+9JEIjdnZAJIdL7dl/DixVqslMTFReiQnJ5Oeno7BYECv19/2U3R3g1vtm7FY7Ia2FAQFFy/Oorj4HFpt4IZ9U6VSSWIc75tmsxmz2UxqairJycno9Xp0Ot19aVNiMdi1C2H3boIXLhCsqyPa0YFmyRKa/v7v6Y5GcbvdDA0N4fP5rumjwDVjPn7Dij9/uRjHYrErxnx80hDvp2azGYvFQnp6OiaTCZ1Oh16vv+LzbxN3/0RavAMHg0H6+vro6uqipaWFgYEBvF4vCoUCnU6H2WwmPT0di8WC2WyWjKPRaCSjXd6hrxbceA6C+DVc3bnjAyN+x/R4PAwNDdHdbcDp9DI0dBGvt49oNEg0GsVoNGKxWMjKyiI3Nxer1YrFYpHuoKPZ6ePXFg6H6e/vp6Ojg+bmZux2O263G7VajV6vJzk5mbS0tCsEMSEhQbrx3NCecRtedSO73JbxWUcoFGJoaIihoSFcLhcDAwPY7Xb8fj+RSASDwSAl1MnNzSUrKwuDwXAnOvctc3nftNvtdHR0YLPZcDgceDwelEql1Dfjdvw6fTMcVqDR3HrfdLvduFwuHA4Hg4ODBINBYrEYCQkJJCUlkZWVRU5OjtQ34zmO7wW7xm3rcrno7u7GZrPR1dWF0+kk5PWiiURIU6mYlZlJJCcHb2oqZrOZpKQkjEajZNerbXv52Lv8Om8kvHHbRiIRQqEQTqcTp9Mp3Tz7+/vxeDyEQiGUSiUWi4Xk5GTy8/OxWq1kZmai0+m+SX+986IbN3Z/fz8XL16kqamJvr4+VCoVaWlpZGVlUVBQIGU9iidjkRoxCh3m6uuORCLSXbe9vZ329nZ6e3sJBoMkJiZSWFjIhAkTyM7Oltp/J9sdb18gEKCtrY2amhpaW1ulZCdZWVkUFRWRmpo66ja9ekkYF+KWlhZsNhv9/f0A5OXlUVpaSnFxMUlJSXflRhZfCVzeN+12OyqVivT09Cv6psViISEh4aZL1LvB9fpmfOXS3t4uZRULBoMkJSVRWFhIaWmpdHO7G+2Ot9Hn89He3k5dXR02mw2fz4fJZCIzM1PKnBe/YV1dUmq0bSsIAoFAQFrVNDc3093dzcDAAEqlkuzsbIqLiykuLiY1NfUa98VNuDOiG29wU1MTp06doru7G5PJRElJCePGjSMnJwe9Xn8rDb2niN+5Q6EQAwMD2Gw2ampq6OvrIzExkYqKCsrKykhMTLyt4iEIAj6fj9raWk6cOIHL5SIzM5NJkyZRVFREUlLSyGqe3SPEb8h+v5+Ojg5qa2tpaGhArVYzdepUpk+fftuzVAmCgN/vv6Jvms3mMds3W1tbOX/+PHa7naSkJKZNm8bkyZNJTEy87XaNxWIMDAxw7tw5zp8/TygUwmq1UlpaSkFBASkpKVL/vF9tG3fXdXV10dDQQH19PaFQiMLCQmbMmEFBQcFw7p7bJ7rx13Z1dbF//36am5vJz89nxowZFBYW3vZMSfca8eV9b28vZ86coba2FrPZTGVlJRMnTkStVn/t6xcEgd7eXr744gtaWloYP348s2bNIicn574S2ZEQjUYZGBjg7NmznD17loSEBJYuXcqECRO+9pIu3jc7OzvZv38/LS0tFBQUSIPk29Q3T58+TV1dndQ3J02a9I1uMHGBr66u5vDhw0SjUcrLy5k6dSrp6en37c1rpMRLXcVv4r29vUyaNImFCxeSlpZ2vWu/PaIrCALd3d1s3rwZj8fD4sWLKSsrQ6fTjWmD34j4DK69vZ29e/fS39/PsmXLKC8vv25l3pt9jsvlYsuWLXR2drJkyRKmTp16f26c3CLxmZPNZmPXrl34/X6eeuqpW05BGO+bmzZtwufzsWjRIqlvwuj7O+828b7Z1tbG3r17cTgcPPLII0yZMuWW+iaIVYhPnDjBvn37GDduHJWVlVit1nvCPz8axFeiZ86c4dChQ+Tk5LB8+XKSk5Mvt8dNRVf185///Gbf8XMQZyb79+9ny5YtPPTQQzz++OPk5ubecGYXDodZt24d0WgUq9V63Q8OBAKsX78et9tNbm6u9Dlut5uf//znrFu3jgMHDjB37lxp8AiCwIEDBwgEAqSkpHDq1ClaW1vJy8vDZrOxceNG6urqKCkpYXBwkE8++YTTp09TXFzM+fPn2bZtG4FAgNzcXOx2O1u2bGHSpEn4fD4++ugjTpw4IZXBWbt2LdXV1RQUFFzjL4WvQlaSk5OpqKiguLiY3bt3c+HCBUpLS0c0OxUEgebmZv70pz8xdepUnn32WfLz829o10gkwieffCLZ7HqEQiE++eQT+vv7yc/PvyLCY2hoiM8++4zx48ejUCg4fPgwg4OD0m8kCALbtm3jwIEDdHV1UVxcjEKhoKenh71791JaWkpfXx/btm1j4sSJRKNRdu/ejSAIpKSksGXLFg4ePEhPTw85OTn88pe/ZPXq1ezatYsZM2ZgNBpvaMPp06eTkZHBunXrCIVCFBQUjGhpHI1G2bt3L1u3buXhhx9m+fLlV/TNq+0Yi8XYsmULPT09FBYWXvczw+EwW7ZsoaOj44pcuJFIhDfeeIO3336btrY2Zs2ahUKhwO/38/HHH3Ps2DHJv/7pp59SVVVFbm4uer2eqqoqurq6yMnJYf/+/ezevZvGxkZKSkr48MMP+e1vf8u5c+eYM2cOhw4dYvfu3Xg8HvLzxbF74cIFGhoaiEQivP7663z22WfU19czd+7c64ZGXd03d+7cSW1tLRMmTBjRiizeX/70pz/h8/lYuXIls2bNwmKx3FBwOzo6WL16NeXl5TcsYnrx4kU2b95MYWHhNeMqGo1Kdd8MBoM0w966dasU1XH27Fm2bt2K1+slNzeXAwcOsGvXLjQaDampqWzdupVDhw6RmpqKyWRi165d7Nu3D5VKRSAQ4OOPP+bixYuUlJTQ3d3Nhg0baGtro6ioiHfffZf//M//pLa2lrlz5163/ykUCrRaLXl5ecyZM4dQKMRHH30klTL60i4u4Dc3su2wvVoQBHbv3s2lS5f42c9+xpQpU4atCqtWqzGZTLS3txMMBmlvb8fr9eL1erHZbHg8Hqqrq2loaCA7O5vW1lZp0yUajbJixQp+8pOfUFpail6vlz63t7eX9evX09nZicfj4cSJE9TU1ACwfv16ysvLsdvtnDt3jvfee4+ZM2fyzDPPIAgC69ev54knnmDTpk0MDQ3R2NjI7t27icViHDhwgHA4zKxZs1i9ejUbNmyQdjI3b958zcbG9X4Iq9XKj3/8YwoLC3nrrbcIh8PDmZauri7WrFnDj370IxYsWIB2mCNwKpWKpKQkWltbpeKUHo8Hv98vVYatq6ujurqa/Px8ya7xDbjW1lZ27txJOBzG6XRy+PBhmpubr/iOwsJCnn/+ebZu3crQ0BCRSITPP/+co0ePEovFrrCbw+Fg//79dHV1AVBUVMTzzz8vFdr8zne+w2uvvUZZWZlUi+1GKJVKioqK+Nu//VsaGho4ePDgsHaPF0lsbm7mZz/7GWVlZcP2TYVCQUZGBk1NTYTDYTo6OnC5XNKGpdPppKWlhSNHjlBcXIzNZpNs6PV6SU5O5qc//SkrVqyQPvPEiRM4HA4WLVrEe++9JxUenThxIh9//DFut5tjx45RX18PQHp6OqtWreLEiRO0tbWh0Wj4yU9+wsqVKwmFQmzcuJEVK1bw2Wef4ff78Xg8fPLJJ9TV1aHX63n55Zd58cUXr7gh3MyuVquVv/zLv6SgoGDEfdPr9fL73/+e+fPns2rVKjHR/TDflZmZKRVQdTgcdHd3E41G6e3tlSoJf/jhhxQXF+N2u2lpaaGlpUUq3Hnx4kXWr1+Px+ORPrO3t5c9e/ZIkRwfffQRK1asYNu2bTQ0NLBnzx5WrFjBmjVrOH/+PA0NDSxYsID333+fkydP0t7ezqpVqxg3bhzvv/8+s2fPJhKJsH//flavXs3ChQtpbGyktrYWvV7PX//1X/ODH/xg2BVBXHznzJnDT3/6U/bt28fJkyeH7bMwgsMRg4ODnD59mr/7u7+7QgCHa1C8vMhbb72FwWDAbrfz0EMPUV9fT1tbG1OmTGFgYACfz0d3d7fYGLWaGTNmMG/ePGpqapg5c6YkRKFQiFOnTlFeXg6AxWLhgQceoLa2FoDx48dz4MABenp6SEtLo7q6GrPZTDQaZcmSJcRiMVJSUlCpVAwODlJRUSEJgVarJRQKYTKZsNlspKen4/f7yczMlMpMj+SaVSoVixYtwm63c/z4cRYuXHjD1wuCwObNm3nmmWcuv0OOyK4A77//PiB2yuXLl3Px4kUuXbrE/PnzJbv29vYiCIJUjr28vJykL/MqpKSkUFZWRiQSueLzy8rKGBwcJDMzE6PRyNmzZykoKMBms6FUKqmoqGD9+vUA0gZKvF1lZWU4HA6ysrJITEyUSoeXlZVdd7VwvevT6/WsWrWKf//3f2fWrFnXzI4vx+FwUF1dzc9+9rNb6ptxPv74YzweD729vXzve9+jrq6Oc+fO8dhjj0mlzPv6+qT4T4PBQCwW4+jRoyQmJpKdnY1arZZK6yQkJNDV1cWlS5cYN24chYWFfPzxxyQmJlJeXi7Vq5s8eTJ+vx+TyURaWhpqtZoTJ06gVqt55plnSElJYevWrdKm6dGjR6Xxkp2dTVZWFnv27GH+/Pkj7jcqlYrFixfT19fHyZMnefAmFUIEQWDXrl3MmjWLadOm3ZIbQaFQSJOJxMREpk+fjs/n48SJEzzyyCP09vZK0QJDQ0OAKNZarZa2tjZKLjudpFAoyM/Pl1YkLpeLSCRCSkoKWq2WCxcuYDAYyMjIwOv1UltbS1paGrm5uXR2dnLu3Dmam5t56623mDVrFjqdDp/PR0pKCrW1tdjtdtLS0sjJyaGlpQWlUklVVRVarZbs7GxphT3c9VosFl599VXeeOMNpkyZMmw5+mFnuj09PRQUFIy4U19OOBzm+PHjUgHFeGxne3s72dnZlJaWYjab6ejooKOjg87OTsLhMJFIhJqaGiZOnCh91tGjRwkEAgwODtLT0yOd/gEkv2BFRQVFRUW0trZisVh49tlnUSgUnDt37op43qvvYgsWLKCgoICjR4+i1WqlumSnT58ekVhcTlyYrp5BXk0kEmFoaIiioqJb9o2Fw2GOHj1KMBikt7cXh8OB0+mkra2NrKwsKcTlarsOhyAIeDwe9u7dy8svv4zb7ebEiRNEo1HsdrtU1+tG73W73ezbt49XXnkFrVaLIAicPHmS8vLyEV+jQqHAaDSSlpaGw+G46Ws7OjooLi4e0eC4mmg0KvUpu92O3W6XQrIyMjIoKirCarVeYcNYLMYPf/hDXnrpJY4dOyaJ6IwZMygvL+fIkSNSVd54DO7VM29BEAgGg+zZs4dnn32WpKQknnrqKV5++WXa2to4deoU0WiU6dOnEwgEOHXqFO3t7dLNIRQKMTg4iNPpvHGF3xugVCqZMWMGly5duunrBEGgra2NqVOnfi2/bWNjIx0dHYRCIRobG3G73fT19eH3+8nJyWHatGkMDAxItu3v72f79u3odDocDoc0WbiaeC3A+CNeEy5eNzBu93gtQI1Gw9y5c1m+fDkbNmzgpZdeoqenhwsXLmAymaQ4/2g0ilar5dlnn+Xll1+moaGBpqamEV9vXHiTkpKkG8nNGHamm5qaSldXF5FIZNilW5y4szkWi5Gfn8/06dN54okn+I//+A+++93vcunSJXw+nzSbXLly5VcNUqs5d+4cVqsVvV6Px+Ohrq5OEv4LFy4QCoWkeND/39619bRxddFl4lt8wXc7xhg7cRjHgAmYhqiJItwWQVUhWQIThAAAC4ZJREFUtVKDWuWl6v/pcx+qPFZqVUVVK7WqIqE+gNpETdOotsFYAV+IqesLmWB7PIzHl++hPSeGJly+UEjDLMlPtgdme80++5y999ocx0EQBFQqFWi1WsjlcgiCAL/fj2g0inK5jNdeew3xeByxWAwymQw2m41uyyuVCoC/ImWe5zExMQGe5zE8PIxcLnfgKbjtdhurq6twOp27fk4ul0Oj0SCfz8Pj8ez72jzPQxRF9PX1YXh4GO+99x4+/fRTTE5OIhqNguM48DwPs9mMDz/8kH731KlTKJfL9J6VSiUqlQqazSZEUcTy8jKsVis+++wzKJVKcByHcDiMS5cuIZ/P004snufpllej0aBaraJSqUAQBHzyySdQq9XgeR4zMzPY2NigQy0PAuJYDHvMurHb7fjxxx+f6dz2sqEgCHC73RgcHMT169dx8+ZNXL16FQ8ePKA21Ol0+OCDD+h3ybBHn89Hm03u3r0LhmHg9XohiiKuXbuGoaEhLCwsgOd5jI2N0RrwarUKQRBw8+ZNFAoFFItFGAwGxGIxBAIBCIIAnU4HQRCg1+uxtbUFo9GIQCCASCSCer2OVquFn3/+GSMjIwcuB2u320gkEs/NBxCQZySdTj8vO/9M1Ot18DwPg8EAt9uNGzdu4NGjR5ifnwfDMOB5HltbW6jVagiHw9sc68rKCjY3NyGKIkRRxNLSEq1Br9VqKJfLOH/+PLq7uxGLxdBqtRAKhfDLL7/gwYMHsNvtCIVCNDcTCAQwOjqKubk5mEwmOBwO1Go1hEIhrK6u0r8fjUaRTCZx48YNzM3N4cKFC2g0GntybydIva9er9/zs3sm0jQaDdbW1rCysoL+/v59ZT+bzSZYloVer8fk5CQymQw0Gg0CgQB4nse5c+eg0Wig1WrR29tLO6ZI6QnP82AYhk4eLZVKuHDhAnp7e9HX14dAIAC5XI5sNgudTgebzYaxsTFkMhk4HA5cu3YNwWAQ2WwWDMMgFArB7/cjmUxienoaZrMZKysrMBqNtPsoEonAZrNhYmICHMfRSDsUCu2b3O12G4uLi5ifn8fs7OyuZ7QymQxGoxG3bt3ad7UCOUfV6XSYmprC2toalEolgsEgOI7D+fPnoVarodPp4HK5oNFoqF3JYqDX66nDJ1OD7XY7qtUqtFotbWtVKBQYHByEz+eDy+WCz+eD2+2mdlMqlTh16hSq1SqUSiUcDgdEUaRtwgzDoNFooL+/f9+7BRJ1fPvtt7DZbBgZGdnVJlqtFul0GslkkiYH95MgKhaL0Gq1mJqawh9//EF3J+VyGQzDQKVSQafToaenZ1unVFdXF/L5PHK5HN5++204HA7k83lYrVZEIhF0d3djcnISTqeTOuW33noLgiAgl8tBq9XCZrPRyEwul8Pn86HZbCKdTlPeulwuJJNJhMNhBAIB2tnHMAz9vs/nO5DTbbf/GlT5008/4f3339+Tm06nE19++SU8Hg+tRd8Lm5ubkMlkCAQCcDgcyOVytJTS7XZDo9HQBJfD4aB6CaRRxeVywe12Y2BgAJVKBadPnwbHcXQqssvlwtDQEFKpFKamptDb2wuPx4NsNouZmRm4XC4YjUZUq1W888476O3thVarRblcxszMDCqVChKJBC5dugS/3w+/349Hjx5hbGwMDMOAZVmsra0hHA7j7Nmz+7pnsnP54osv4Pf7EQgEIJPJdk2k7atkrF6v45tvvkEul8Ps7CycTueJLBd5Hkhkf/v2baRSKXz00Uf7SjyQ7fft27cxOzuL/v7+EzvKut1ug2VZfPXVVzAYDLh+/ToUCsWe3yOJp0KhgNnZ2X2fj58UdHIzmUzi448/htls3hc3s9ksPv/8c4yOjiIcDv9fR4yvMlqtFtLpNL7++msMDAxgenqaBKWHU6fbarUQj8fx3Xff4cyZM3jzzTfpin5SSd5qtVAul3H37l3cv38fY2NjeOONNw5UY9tut7G+vo5bt25Bq9VienoaLpfrxNRBktKk+fl5xGIxTE9PY3R09ED1pK1WC4uLi/j+++/R09NDuXlSbPgsEG7euXMH9+/fx/j4OCYmJg7MTZ7n8cMPPyCRSODq1au09O+k2pV0rGUyGczNzYHnebz77rs7I+PDbQMWRRHRaBQLCwsQRREjIyO4ePEijexe5R+D2IrjOCQSCdy7dw8syyIUCuHKlSv0cP7/QbPZxNLSEj2jvHz5MoLBID0jelXsSmxIkix37txBsVjE+Pg4Xn/99X9oHxwEoigiEolgYWEBzWYTIyMjGB4e3rbreFXsuBOd3FxeXsavv/4KlmVpNdCLcJPsQubn57G4uAin04nx8XH4fD4a/b6qdgWeNvGQTtRoNAqDwYBwOAyGYZ4VIBy+9gLJHpZKJUQiEcRiMVQqFfh8PgwODqKvrw86ne6F2mJfFrRaLXAch0KhgEQigYcPH6JWq8Hj8eDy5cu7NjMcFMSu+Xwe9+7dw9LSEuRyOYLBIAYGBmCxWF7IKR0XSIvq5uYmrYksFAo4c+YMrly5gnPnzh26DUulEn7//XeqD+D1ejEwMAC32/3KcLPZbKJWq1Furq6uguM4eL1ejI+Pw+12H+p9kmaFVCqF3377DalUCqdPn0YwGITf76f8/K8fkRG+lstlpNNpLC8vI5vNQqVSYXR0FBcvXtxL1+LfVxlrNpuoVqtIpVKIx+PIZrNoNptU3s/j8cBut8NgMGzrFHoZSN8pD0ccbLFYxPr6OtLpNEqlEhqNBqxWKxiGgd/vh8lk+tf1EMg2ZmNjA/F4HPF4HE+ePKEJHp/PB6fTCZPJtE0Y+7hsutOOlUoF+Xwe6XQamUwGLMtCpVLB5/NhaGgIPT09UKvV//r/2/xbszWVStGHp9lswmw208RsJzeP246dOAg3SWKI8OEoVMbq9TpVb3v48CH9je12O86ePQun0wmbzUY1a1+m455Oqc1GowGWZZHP55HJZLC+vo7NzU0olUp4PB4EAgH09fVBq9Xud0E5Wj1dcj2e57fplRINy66uLuh0OphMJphMJlgsFlgsFmg0GqhUKqhUKigUikNTKiLOSxRF1Ot1KvbNsixKpRJYlqV1j41GA0qlEhaLBS6XC16vFzabbVv29jjl6Ih85vr6OpLJJP78809UKhWqCWqxWGA2m2G326kAtlqthkKhgFKpfGHSt1otakdBEKjYd6FQwMbGBh4/fgyWZSGKItRqNWw2G7xeL3Vsx7kV7eT5Xtw0m80wGo27cvMwojnCzXq9Tm26GzdVKhXMZjPlpt1uR3d397EfnXQuDk+ePEGhUKD8fPz4MS03Jc+80WiEzWaj1UNEpF2hUBzKgkGcKSk/I1zlOA6lUonylDQRAaDa3l6vFy6XC1ar9UW0O45exPwfF/l7VSFjTYjeKrnxjY0N1Go1bG1toV6v0x+RjDfZKWzcGSl3FkuTkqNOEeNGo0Gvp1AoqFo8EfsmRDCZTFAqlTTaeVlW5N1AtkFk604IVSwWwbIsrUcljSSkjKnTpp22JF2EwNPoqnNkCmmwkMlk1I56vR52u506fIvFQp3Ty7Kb2Qsk2hFFEZVKZRs3S6USarUaBEF4Ljc7p0CQ++3q6nomNwk/n8dNIlZOuEkWgP8aN4Htzo/IUJKFhHSkkUkdpDNSJpNtE4rvfO7J+zt52mnbTr4CoM0qpAzQarVuW1S1Wi119uT6h4Djd7oHQWdkSgryO+ckESOTV+eDTX6cTqdCIpOds6lOCgjx6/U67fYj9iRjUchngKfjUHaOOiL1t4cVjfwX8aLc7HQkEjefgtiL2LVzXFenbQFsm5FGXjttS1qzifM+Bq6+kNPdvZdVggQJEiTsRA7A1ee9uZfTlSBBggQJh4iTu6eRIEGChGOA5HQlSJAg4QghOV0JEiRIOEJITleCBAkSjhCS05UgQYKEI4TkdCVIkCDhCPE/VEbtxV3zpP8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAB1CAYAAADtADJYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydeXhU93nvP7OPRqOZ0WhfRvuGEBKGSOxgDMbGjh3sgh3XdRK3jevm9mnT9ubp7b23N+nT5ul9+rTNzc1t3CaOmziODbHDEgx4Ywm4Eoh9lZDQvi8jzb7PnPvHyTlIgEBgwGCfz/OcR8vMmTnL73zPe97fu6gEQUBBQUFB4e6g/rQ3QEFBQeHzhCK6CgoKCncRRXQVFBQU7iKK6CooKCjcRRTRVVBQULiLKKKroKCgcBfR3uB1JZ5MQUFB4eZRzfTCjURXQUFB4Za5Mg9AEATC4TDhcJhIJEIkEiEajRKLxYhGo4RCIcLhMNFoFEEQiMfjxONxAHQ6HSqVCo1Gg16vx2g0YjAY0Gg06HQ69Hq9vBiNRrTaq+VNpZpRC+8aqhskRyiWroKCwg2RxDQUCuHz+RgZGWFychK3243T6SQYDBKNRonH42g0GrRarSyQOp0OnU6HVquVhXSqwGo0GgRBIBaLyT+nCnQ8HicajV4l4oD8uVarFbvdjs1mIz09nbS0NJKSkjAajWg0mjshxjN+oCK6CgoKs0ayPkOhEMPDwwwMDNDf38/ExAR+vx+VSoXVaiU3N5fU1FRsNpsscJIA6nQ61Gr1HbE6BUGQt1ES30gkgtvtZmJiArfbzeTkpLy9sVgMk8lEVlYWJSUl5OfnY7PZ5G38BCiiq6CgcPNIIjY5OUlXVxdtbW309fWhUqnIysoiPz+fwsJC7HY7FosFjUYjr3svPMpfi6maFwwGcblcjIyM0N3dzfDwMD6fD6PRSGFhIXPnziU/Px+9Xg/c1D4poqugoDB74vE4o6OjnDhxgtbWVtRqNcXFxVRVVZGfn38nH8s/NaZayRMTE3R0dNDa2srY2BhpaWksWLCAqqoqkpKSZrPfiugqKChcH0EQiEajnDt3joMHD6JWq1m4cCFz5szBarVOs2I/L0i+6qGhIY4ePUpnZyfFxcWsXr2ajIyM64mvIroKCgozIwgCbW1t7Nixg9zcXNasWUNWVhYqleqawpJIJBgYGMBqtWKxWK75mfF4nIGBAdLS0khOTp627vDwMIIgkJubi0qlQhAE+vv78Xq9qFQqioqKMBgMTE5OYrfbCYfDdHd3IwgCBQUFRKNRRkdHSUtLw263EwgEGB4eJjMzE7PZjEqlIhAIEI/HMZvNTE5OMj4+Tl5eHklJSQwNDREOh3E4HIyOjhIIBOTXrrW/kk5GIhHOnDnDgQMHKCws5LHHHiM5Ofla68wouprvfOc71zsX131RQUHh/ieRSPDee+9x9OhRnnvuOZYuXUpKSsqMgguiCO3Zs4f+/n6qqqqu+Z6TJ09y4MAB8vLysNls8noXLlxgz549nDlzhpSUFLKyshAEgR/84Afs27eP3bt3s3TpUk6dOsUPf/hDHn30Udrb2/mnf/onTp8+zZw5czhz5gxer5fXX3+duro6fvzjHxOLxfj1r3/NkiVLEASBf//3f8fpdGKxWNi8ebMcLdHS0sLRo0cJh8PY7XY++ugj3G43u3btYtmyZdfcZ+lYaLVacnNzaWhoYHJykq1bt1JeXn4t4f3bmY63EqeroPA5RhAEjh8/zsDAAC+//LI8YXQj1Go1mZmZuN1umpubOXnyJHPnzkWv13PixAnq6uo4cOAAPp+Prq4uDhw4AEBmZib9/f0sXLgQnU7Hb37zG+bNm4dKpeKv/uqviMfj7N27F4fDQUpKCu+++y4AWq2W9evXU1ZWhsPhoKioiFAoxP79+4lGozidTp5++mmOHDlCLBajs7NT9s9+/PHH6PV6bDYbmZmZvPrqq6xYsYK8vDxSU1N59tlnmZiY4MiRI7Pad5VKhU6nY+XKleTn5/P666/zp3/6pxiNxtkdu1m9S0FB4TNLU1MTGzZsmLXgTiUWi/Hzn/+ckpISNm/eTEZGBoWFhRw8eJCqqipWr15NdXU19fX11NfXM2fOHAKBADqdDqPRiM/nAy4L2ZkzZygpKUGtVmM0GmXrMT8/n5qaGlpaWti8eTNarZYDBw6g1+vlG8DOnTsxmUx4vV76+vqoqKhAEARcLhdGo5Genh5+8Ytf4PV6MZvN7Nq1i1OnTqFSqThy5AjPPPPMTU0MqlQqSkpKyM3Npaura9brKaKroKBAIpG4KnvsZtd94YUXeO211xgfH5ezyFQqFeFwGK/Xi9frlX2nTqeTgYEBioqK8Pv9crLDpUuXKC8vv0r8AoEADoeDuXPnMjIyQn9/P4899hgOh4PGxkaGh4d56aWXCIfDnDp1iu7ubvbv38+JEyfIzMwkOTmZyspK3G43ubm5ZGRkkJOTw8TEBDt37sRiseBwOG7pGAiCcFNirbgXFBQ+5yxfvpytW7fy4osvYjAYZiUg8XickZERAoEATz75JJ2dndTX11NQUIBKpcJgMOB0OvF4PKxcuRKHwyGvm52dza9+9SsSiQSbNm3i4MGDlJeXo1armTt3Llqtlng8ztGjR0kkEpw4cQKz2czBgweJx+M888wznD59mn379hGLxXjwwQdJJBK8+eablJeXs3r1ah555BFOnDiB3+9n/vz5vP322xw8eFC26N977z0Aqqqq+Nu//VsCgQCFhYX8r//1vzCZTLM6btLk48jICMXFxbM+3kr0goLC55xEIsHevXs5e/YsmzZtIj8//4bCe7MW4dTPu9G6UjTDJ+GTfMZs9j0ajXLo0CGOHTvG7//+75Oenn7lekrImIKCwswIgkBXVxfbtm0jLS2NNWvWkJub+7mMzZ0JQRAIBoOcOnWKjz/+mIqKCtatWzdTmJkiugoK9wvRaJTdu3ezfv36qya34vE4u3fv5qGHHpoW+3o7kIrJtLS08stf/gaDIcqSJQuZP78Gm83ymctAmw1SckRvby9Hjx6lv7+f8vJyVq1ahd1uV5IjFBQ+LWKxGF1dXaSmphIMBjGbzSQSCVwuF/n5+YyPjxMOhykqKpILqSQSCXp6eohEIoD4WGsymUhLS6OlpYW5c+cyNDSETqfDarUyMDBARkYGXV1dlJSU4HK5EASBjIwMkpOT6e/vx+PxYDabKSoquuV9CYcFXngB9u+fQKXqw2qdYMmSXB54QM+KFVHKyrKwWCyy4HxWhFjSQinxoqOjg/b2dkZHR8nLy2PhwoWUl5fPtg6DUk9XQeFOIggC//Ef/8HDDz/MwMAAtbW1vP3225SUlBCJRBgZGaGhoWFa8RRBEPjFL35BeXk5+/fv59FHH6WxsZE/+7M/46233uKpp56isbGRhQsX0traSnp6OtXV1fzqV7/imWee4bXXXmPdunV0dXWxfPly9u/fj9vtZs2aNTctuoIAiQT4fDAyoiIWg/HxNCANp1PAaIRVq/xcuvQBBw/uIhaLYbfbcTgc5OXlkZWVRXJyMkaj8ZNW57orSFZ9MBjE7XbT398vV0wLh8NYrVbKyspYt24d2dnZcqnJ24EiugoKtwGtVsvTTz/Nvn37mDNnDmazmfb2drKystBoNCQnJ3Pq1CmWLl0qi65Go8Fms1FbW0t/fz91dXWcP38egKSkJDIzM/H7/Vy8eJGSkhI5uys5OZmkpCSsVisLFizg7NmzJBIJfD4fJpMJu91+3W0VBIjHIRSCkRHo7YXubnA6xdczMqC6GnbsgPR0eOklFX/8x5CTYwaeIpFIEAqFcDqd9PX1ceHCBfbv308wGJT31WazkZWVhdVqlReTyYRarZ62XC/r7WaQrNREIjFtiUajeL1e3G43Ho+H8fFxxsbG8Hq9hEIhNBoNVquV/Px8KioqZLeNXq+/Yxa8IroKCrcBlUpFTU0NP/7xj1m3bh1paWkUFRWxdOlSrFYrHR0dHDt2jCNHjmAymVi6dCmCIODxeJiYmMDlcuF0OuWi31K5wRUrVrB//37y8vJYsmQJJ06ckF/zeDy4XC7cbrdcq7a+vp7c3NzfipAoGoIAgQCMjkJXlyiyTieoVKLAOhzw0EOQlgaSm/j0aVGI//IvobYW1Grx/aCShTU5OZmCggJZ8ARBIBAI4PV65W3s7e3F4/Hg8XgIhUIkEgn5eOl0OgwGg1y0XKq1q9VqZVEG5CLmUjxwPB4nkUgQi8XkguaRSETuSBGLxWQx12g0mM1mLBYLVquVrKwsampqSElJwWw2T/OZ3y03ieLTVVC4DSQSCcLhMLt27eLxxx/HaDQyNDTEyMgIDoeDwcFBkpKSCIVCnD9/nmeffZZEIkFraysmkwm3243NZpMLvIyNjZGRkYHT6SQ9PR0Ap9NJfn4+vb29pKSk4PP5yMrKYnx8XM7CstlSOX78HM8//5d0dano6RGtWZUKUlOhtBSKiyE7GwyGqWI6nVhMdDfodNd+/WaQSiZKi2SFhkKhae15JAGNxWKywE5N2pAsY7VaLXefkERaatFjNBplK1V676fke1Ym0hQU7iQ+n49t27ZRV1cn1xK4EkEQGBkZISUlBZPJ9IlEQPLBhsMwPi6wd+9FDhz4DampaqLRedTVLaKkREVhIWRlgckEGs0nF1CFWaOIroLCnWTqdXS7LSpBEJdwGMbGRBdBVxdMTor/T02FvDyBkhLRXZCSwhTr7rZuisLsUURXQeF+QRAgGAS3G3p6RIHt6xMt28xMKCgQl5wc0YKdyUWg8KmiiK6Cwr1IIgHRqCiw/f2iwPb3i6JrtYr+V4cD8vJEgb0dPlaFu4IiugoKnybSZRaPg8sFg4PQ2SlasB6PKLAOB5SUiBaszSb6YEER2fsURXQVFO4mgiBGAHi9MDwshl/19op+2KQkyM8XIwny8kSfrDTJpQjsZwZFdBUU7hSSwIZCMDQ0PdlAoxEFtrhY9MNarWA0in5Yhc80iugq3H2mBs0DcmxmMBiUA9kjkQiRSESO2YxEInLpPEAOlNdqtRgMBpKSkuReV1JgvdFoxGQyyRWx7mRcphRJEImI8a8dHaKbYGJCFNicHCgsFBe7XbRqJT6vVuy1xkEwGJRjdKVxEIvFpo2DRCJBPB4nHo/L/cmmJlVIMbnS39KYkMbIldluSpyuwmcKQRCIRCL4/X5cLheDg4NMTEwwPj6Ox+MhFosRj8dl8bzyoklKSpIvHOnCAuRA+VgsRjgcJhgMEolE5E4DUy9aKWg+OTmZ9PR07HY72dnZpKenYzabp7V/mf1+iQLrcokWbGcnDAyIlq3dDkVFohWbk3P9ZIPPA1K2WCAQwOfzMTIywvj4OJOTkzidToLBoJxNptPppt04p4qnNC40Go18TqVaCdINORwOy+IsjY2pYyGRSKDRaORiQXa7ndTUVLKzs0lNTZXrRNxBIVZEV+H2IV0AHo+Hnp4eenp6GBwcxOfzodVqsdlsOBwO0tLSSEtLw2azyZlDkuV6Owf71JTQWCyGz+djfHyciYkJRkdHcTqd+Hw+NBoNqampVFRUUFxcTFpa2rQce8lNEAiIE1zd3WI0QTAoWqxFReLicIjpslrt51tgo9EoPp+P3t5eent7GR4exuPxIAgCJpOJnJwcMjMzSU1NJS0tDZPJJI8ByWq9/THNovBPTRF2uVxMTEwwOTnJ6OgoLpeLYDCISqXCarVSUlJCUVERWVlZGAyG2zU+FdFVuHWkMeL3++ns7OTcuXP09vai0+koLi6mrKyMnJwcrFarbKHCp1/yb+rYTiQSeDwexsbG6OjooLe3F7fbjcVioapqIZHIA/T1qXE6Veh0KrKyoKxM9MPa7WKoFny+RVYQBCYnJ+no6KClpYWhoSH0ej2FhYUUFxeTk5ODzWab1hX30x4DU7lyPHi9XiYmJuju7qa3t5fx8XE0Gg0Oh4O6ujoKCgo+SeEbRXQVbh7Jmmlra6OxsZHJyUmKi4vlASn107qXLqzZII35WCzG0NAQp0718NFH/eh0o6xfv4BFix7AbE6+K/vl8/nYt28fVqsVs9lMXV0dzc3NOJ1OGhoayMzM/FSPr9RN99ixY5w9exaVSkVFRQVz5swhNzf3mr7T+xGpzoPH46Gjo4Nz584xPDxMdnY2ixYtorS09GbLOyqiq3BzRCIRjh8/zqFDh8jOzmb58uXk5+ff1rqi9xLxeJzx8XGOHDnChQsXqK6uZvXq1ZjN5ju6v+FwmP/23/4b3/jGN/jJT37Cc889xwcffMBDDz3Erl27+J//839+KvVpE4kEQ0NDfPDBB4yPj/PAAw8wf/58UlNTb7t76F5E6hjR09NDU1MTo6OjLFq0iIaGhtn6gpUi5gqzQxAEBgcH2bJlC3l5eXz961/HZrPNOMgEQaCvrw+TyUQoFCIajVJYWHiVUHg8nt9WwbKRm5s77fOi0SiTk5OkpqbS3t5OOBwmPz+fjIwMBEFgfHwcq9WKy+VibGyMrKws0tLS8Pl8DAwM4HA40Ol0tLW1EYvFKCgowGaz0dPTg06nIy8vD5VKRSKRYHR0lKysrKv2R6PRkJWVxRNPPMG6detobGzkBz/4AevXr6euru6OCZ/UOTctLQ2NRsPY2Bjp6emUlZXhdrs/cYPGm0XqA7Zz504GBgZYt24dVVVVM7bqicfjdHV1kZOTM2P7oEgkQk9PDw6HY5rrweVyMTAwIE94Sueovb2dQCCARqOhtLSUrq4u+bympKTQ3d2N0WgkPz+fRCJBb28v0WiU0tJSVCoV4+PjpKeno1arEQRBLn0ZCATo7+8HIC8vj4yMDCYmJrBYLNPcYiCeF6PRSEVFBRUVFXg8Hg4cOMD3v/99nnjiCaqrq2/5xqP5zne+c73Xr/uiwmcLQRDo6OjgzTff5Hd+53dYvnz5TE33ZAYGBtiyZQuBQICPP/5YvkAcDodcL7a/v5/29nY8Hg9btmyhtLSU1NRU+Tv37t3L9u3bmTdvHv/4j//I+fPnyczMpKCgALfbzX//7/+dBQsW0NzcjCAI/OQnP+ELX/gCr776KgD79u3D4XDwz//8z1y4cAGHw0Fvby+HDx/m6NGjZGVlYbfbOXPmDN///vd5+OGHZ2y4KIUmFRUVMW/ePHbt2oXX66W4uPiOWHd+v5/du3dTXl5OU1MTjz76KEePHqW9vZ158+ZRVVV1Z6xKKSTDaJzmqHa73fz7v/875eXlbNq0iezs7Ov2RpNan+t0OvLz86/5nvfff5+Ojg4KCgqmtTdvbGzE7/fzi1/8gurqaiwWC6FQiO9///scO3aMffv2UVNTww9+8AMuXLhAbm4ux44do62tjaamJjIzMzl27BidnZ1Eo1FycnI4ceIE//RP/8Sjjz6KVqslEonw3e9+l9TUVM6dO8eOHTs4cOAAqampqFQqvv3tb9PQ0IDFYrnmtkuuE6PRSGVlJZWVlezcuROfz0dRUdH1zs3fzvSCYuneJwiCwKVLlygsLLyqWaHUyTU7O3vaoL5ZQqEQb7/9Nr//+79/TWvwShKJBI2NjbS0tKBWq6mpqaGgoIDt27czd+5cTp06RTAYpLa2lqVLlxIIBGhqaiIWi8nbPTw8jNPplL9r0aJFlJeXM2fOHOLxOEePHsVqtSIIAo8//jgA7777Lk6nk8nJSdavX89f//Vf4/V6Wbp0KeXl5ZSVlfHaa6/x8MMPMzo6SmNjI5mZmXR1dWEwGGZ1LFQqFampqfzhH/4hP/zhDykvL6ewsPCWj+1MJCcn8w//8A/o9Xq++93vkpyczJ/92Z8RjUZJTr6DfuXeXvjyl6GhAb70JaivJ2GxsHnzZlavXs2CBQtm9d0ajYa0tDQSiQTvvfceXV1d1NfX43K5aG1tZdmyZbz//vsUFRVx8uRJBgYGACgpKWHFihUEAgF+85vfEI/HATAajfzd3/0dHo+H48ePk5qaKp/XiooK9u7dy4YNG+jv76epqYlTp06xfv16MjMzMRqNzJs3TxZQQRA4fvy43K/uySef5KmnnuKDDz5gyZIlmEymGW8U10KlUpGZmclLL73Ev/3bv1FYWEh5eflNH3pFdO8gknP+ygDtRCIhP/oIgjDNRza12PNUVCqV3CBPq9XK60mfPz4+jt1ul0VFukNL2yA1S7zehSQ9As5GcKXvqKqqwu12ywWlDQYDfr+f8+fP09/fz5o1a+TJoOPHjxOLxeRH9Wg0ytGjR5k/fz6XLl3CarWyfPly+vr6+L//9/+yYcMG7HY7VqtVjrs8evQoq1evlkN7NBoNiURCvjg7Ojp45ZVXiMfj6HQ6jEYjXq+XpqYm6urqaGpquqlHdqPRyMOrV9P5059SsGDBzI66W0QDSDaW9OCd9NvljjI4CC0tcPw4vPYalJYSXrMGi8XC/K9//abF3u/388477/D000/z9ttv89JLLzE4OMiZM2eoqqpi5cqVpKSkkJWVBYDNZkOr1XLkyJGrrg+NRsOxY8eoqakhLS2NpUuX0tXVxSuvvMKaNWv48MMPcbvd2O12gsEgaWlpbNu2Da1WS1lZmfx5Ur8zqZOGVqtlaGgItVotN9a8lRbzBoOB9evXc/jwYUV07zXi8Tjf+973qKysRKVSYbFYGBgYYHx8nKqqKnp7e1GpVGzcuBGr1QqIovuzn/2M8fFxRkZGKC4uZnJykhdffJGDBw+SmZnJBx98gNVqpbCwkPPnzzNnzhxOnDiBRqNh69at1NbWEgwG2bBhAz//+c9xuVzMnTuXp5566rrbq9FoZCt0Nky9WBwOB+Pj42i1WoqLi1myZAlz5szh9OnT9PT0kJ2dzbJly/B6vTQ2NpKRkYHf72dwcJCuri7OnTtHf38/KSkpVFVVsWPHDrq7u5mYmOD06dPMnTuXnp4exsfHWbRoESaTiVgshsvlki3/1NRUqqqqOHDgAJWVlYyPj9Pf309ubi79/f1yP6/+/n5KSkpmtY+CIBCJRkmkpYkZEJ8VotHLuch6vdirJzOTcDiMkEhcrrYzS6SxYLVa2bhxI6+++irFxcXTfKVS/Kz0e09PD6tXr2ZyclJ+oklOTpbbEGVnZ+PxeLDZbFRVVfHhhx9SU1NDbW0tb731FuXl5YyNjZGWliZ3YZ5Kb28vHR0dHD16FJ/Px6JFi2hqamLFihWf4MCJxGKxWxJsUET3jqLValm1ahWNjY1UVFTIj72PP/44u3fvRq/Xs3jx4mnuArVaTXJyMsXFxXR2djJv3jyOHDmCy+VCrVbLIS2PP/44IyMjcovv8+fPo1aricVirF27lu9973v09/fj9XqZN2/ejGIq9c9Sq8HhKGB0dBs9PT0UFhbe0NqRJrm8Xi81NTW8//77tLS0sGHDBvnxfNWqVUSjUZqamti7dy+xWIwnn3yS/fv3U19fz8svv8z4+Dg5OTlYLBa2bt0KwFe+8hXmz58PgNlsZtGiRbz22mu0tLTw4Ycf8j/+x/9g7dq1/PKXv+SJJ54A4J133gHghRdeICMjgx07dqDRaNi0aRNms5lgMIhOp5u1m0AQBPx+Px/t388LL7yAKjd3VuvdF9jtsGgRrFgB69fD3LkYk5JQvf46jb8VptlYu9FoFI/Hg8lk4oknnuDixYvU19fjcDjk9kThcJju7m4effRRiouLgcu+/P379xOLxdiwYQMffvghK1euZHh4mPr6etRqNaFQiB07dgDimDh//jxnzpzBarVSX19PVlYWH374oXyzPn78uGwpL1myhKVLl1JSUkJOTg7hcBibzSa3P7p48SKRSISjR4+SkZFxldvuWgiCgM/nY/fu3Tz77LO3dOiVkLE7TCgU4i//8i9Zt24da9eu5Vvf+hYvv/wyFouFYDDIu+++S319PZWVlWRnZwPw9ttvk5mZSXd3N9XV1Rw9epQVK1awa9cunnrqKWKxGG+88QYbNmwgEolw4MAB7HY7S5cuZcuWLfzVX/0V//Iv/8Kf/Mmf8MMf/pAFCxawcuVKefJgKoIAr7wCb74JCxcKFBQMcv78Zr7xjSdYsKD8t5WvZo5cmImrv+fa75VcIDf72s1y5WfN5oYyPDzMz3/+c1auXMmiRYs+W2FSsZjYikKaA/jtvvn9fn7605+Sk5PDY489Jsdiz8TNnp+pn3WjdW90/mczPq71nqmuvJm27Uqk9/b29rJ582YefPBBGhoarrfOjC/cs9ELUjqf1KguFovJOffSY4rUtO5eDc6W/Egej4f58+fLLanb2tpIS0vjzJkzWCwWCgsL2bx5M8uXLweQZ2Oj0SgajYZQKITBYMDj8WAwGGhra6OkpISkpCQ6OztZuHAhk5OT8jGz2+04nU6ysrK4cOECSUlJnD17lgULFgAqYjHw+8XC2aOj8J//KYru4cMqPvoohZaWBRw5EsTt/piiouQZfcHScb/WMtv33uprN7tc+VnXO2dut5s9e/Zw4MABnn766Rl7nt3XqNWiW0E1vZ6kXq9n/vz5DAwMsH37dtRqNRkZGXLa7pXcynmY7bo3es+tfsZM3z8TiUSC4eFhtm3bxokTJ3jmmWdmEzI2Y/TCXbF0pckcSUClttNSP/qJiQkCgYAsqlMFdTZIDnFpMken02E2m7Hb7VgsFiwWC3a7HbPZLL9+N4Q6Eomwd+9e4vG4HMJyLXp6evD7/cyZM+eWt0lqVBiLXa4f0NR0jtOnD1JQkMv27b2sW/cnBINqwmHRZWc0iv20BgdBuvdWVsKLL8IzzyTweFr56KMP0Wq1LF26lMrKyhuGkN2PSJl3fX19NDY2MjQ0xOLFi2loaLihpfdZRUr5PXDgAG1tbZSVlVFfX09OTs5nNkFmKolEAp/PR2trK83NzcRiMR588EHmzp17VUzvDNydjLSppds8Hg+jo6N0dHQwPDzM5OSk7HxOSUnBbreTkpKC1WqVi2FIlYe0Wi0ajUZerrwbXdnK+coCF5FIBJ/Px8TEBB6PRxZ5v99PPB4nKSkJu90ux4JmZWVhsVjkWfXbNaAEQWBsbIyUlBSSkmaej5aO27W+Vzo9kqiGQmJhbJ9PLIg9MiJarF6vKLSxmGi4JCVBamoCk2mMtLQYOTnppKToSUpSkZQkGjkSLS3w0tyR5S8AACAASURBVEvwwgvw1FNic0OV6vJx7uvr4/Dhw/T09GCz2aipqaGqqgqr1Xrbj9ndQNqvQCBAb28vZ86coauri7S0NJYsWUJlZeXnQlhmg5Qscf78eU6ePInL5SInJ4d58+ZRVFRESkrKfZ+hJo2HWCzGyMgIbW1ttLa24vF4qKyspKGhQU7ouYn9vLOiG41GGR0dpaWlhY6ODlwuF0lJSWRlZVFYWEhGRgZpaWmYzeZPpQ/91HqeiUQCv9+P0+lkcHCQzs5OxsfHiUajZGZmUlpaSkVFBenp6TNaprd/+0T3Wih0uSHh+LgoqpOT4t+RiDjhrNWC2SxaqDab2F7bYhGXlBSxvKBKdXliejaHOBIRv1vsIjvTNorW4MDAAGfPnqWzs5NIJEJmZqZc8MRut5OcnHzLs7p3Cilkzu1209vbS09PD/39/QiCQF5eHrW1tRQXF38mrfjbiSTAvb29ctGjWCxGRkYGDoeDwsJC+Tq/l29a0niQEnekyJZgMEhqaqqcBJGenn7dxJAbcHtFV7oAu7u7OXr0KIODg5jNZioqKigtLZ1WIu1eRxLicDjM6Ogoly5d4sKFCwSDQUpKSqivr5djY2/m4EuHVXrcj0bFXliTk+JPp1Nspx0IiIIrCJcf+VNTRWvTYhF/t9vF/2u14vJp12yVLINgMMjw8DA9PT10d3czOTlJIpHAZDKRlpYmZ4JJbh6dTic/vUg331t3p4gHWKrPKj3tSE84Uhm/0dFR/H4/iUSClJQUCgoKKCgowOFwyDeIe1Uc7mWk6yYUCjE6OkpfXx89PT04nU4ikQh6vR6LxUJmZqZc3lOqYzv1KfaTjoMrt2dq4fNIJILb7WZychKXyyWX+QwEAgiCgNlsJi8vD4fDgcPhICUl5ZNUFbuS2yO6UrhEY2MjJ0+eJDMzk4aGBnlSB+6vx8xrIR0Pn89HW1sbx44dw+Px0NDQIBe7AJXcQSAeFyelvF5RTMfHRUH1eMQlFBJFUqMRa7CmpooWaloapKeLk8dJSeJy5T3qfjmUU8dQMBjE5/PhdDplt5Lkv5d89SqValoBa8kXL9XalRZA9u1LF9JUF5JUsDoej8uJEiaTidTUVOx2OxkZGWRmZpKSkjItu+t+H6P3IlPHQDQaxe/3yy5Gp9OJy+VicnJyWiFztVo9revDtcaBSqWalkh0pUtRmnCe2n1CWken02GxWGTBz8zMlOd2TCbTnR4Pn1x0o9EoBw8epLm5mS984QssXrz4jldguheQStsdOnSI8+dbSUvbhFpdiN+vJhYTRTcpSRRPs1kUUslKldwAUrFr6VB9xg/ZNKa6dqQlHo/L7VqkyVNpAlVKB5V+SheeZBlJF6Ver8doNJKUlDRtYlQR1nuTa40DqeODFMs7dV5GEldJaAHZbXXlWJAyD6Wb+EwRDHeZWxddqWjJz3/+c3Jycnj00Uen3SWuRSQSYdu2baSnp5Obm8u5c+fYtGkTJ0+e5Pjx4yQSCZ577jlSUlKkLyEWi6HT6QgGg7z11lv87u/+LkajUc6fPnv2LBkZGdTV1fH3f//3RCIRvvnNb5KRkcHu3btJSkrid37ndzh37hwnTpygsLCQNWvWsGfPHgYHB1m+fDkFBQVs27aNSCTChg0bSElJYefOndTV1d0wnU8MJfLy5ptNTEwM8vWvP4vFkoRWq/pcdxBQUFC4JjMqwg2drtFolP/4j/9g8eLFPP300zcswiEIAu3t7Rw+fJjS0lIGBwc5deoUABkZGTz//PO4XC7a2trk2f29e/fS2tpKIpGgubmZjz/+WLZ0vF4vv/71r9m4cSMPPvggkUiEp556im9/+9uUl5ezbds2GhoaCIVCHDlyhK1bt/LUU09x8OBBOXvliSeeYPPmzRw6dAiz2UxZWZlcPers2bM4nc4bH0GVCpvNwssvP8yyZUXs3Pkmen0CnU4RXAUFhdlzQ9E9d+4cOTk5LFy4cNYmuvRoYDabqa6ulqMA8vPzZZO/oKCA999/n0OHDvHAAw9QU1NDb2+vXLVIYmJigkuXLvHOO++wZcsWDAYDg4ODbN++nVOnTjE0NITdbic3N5eWlhZCoRAWiwWDwUBraytms5n09HScTiednZ2kp6fjcDjo6uoiPT2dgoKCm3r0UKvVrFq1Ss4dV1BQULgZbii6AwMDcnHg2aBSqbDb7eTk5Mg5ziBawKFQiAMHDvDEE0+Qnp7OggULsFgsnDp1itHRUd59911cLhe9vb0MDAwgCAI6nY6CggJ+7/d+j9OnT2MymfjqV7/KF7/4Rd59910MBoPcutlqtaJSqWTRt1qtsq9Qp9ORnJwsT8B8khKIarWakpISBgcHb/kzrsfo6ChNTU00NjYyMjJCOBymubmZI0eOEA6H78h3Kigo3B1uKLolJSWcPXtWdmbfCCmN0ul04vV65YZvY2NjbN68mZ/97Ge8+uqrnD59mszMTNasWUNdXR1+v5+NGzcyd+5c0tPTSUlJ4d1338VoNGKz2di3bx9paWn09PTw/vvv09TUxPz581m0aBEHDx6kpaWFZcuWUVJSwkcffYROp6O+vp5IJMKePXuYP38+ixcv5ty5c+zfv5+VK1fi9XoZHh6mt7f3psQsFovR0tJCUVHRrNe5GcLhMNu2bSMUCvHOO+/w3nvv0d7eTmtrK3v27Lkj36mgoHB3uOFEWjwe57XXXqOoqIiHHnrohoHvUuWp4eFhiouLGRkZwefzkZubSyQSYXh4GEAOpr/W+gMDA+Tk5DA0NERWVhahUIje3l7y8vIwGAx0dnai1+sp+m3l9q6uLpKTk8nJySEYDNLT0yN3p5XacJeUlKDT6ejv7ycWi1FUVITf76erqwudTkdJScmsClzHYjF27NhBLBZj06ZNdyQWeWRkhB/96Ec8//zzbNu2DZ1Ox/Lly9FoNOzfv59vfvObt/07FRQUbiufLHohHA7zzjvvyDVa09PTP3chOYlEgpGhIY795CfEBYHHnn8efUaG2JtbylqYGhf2Cbh06RI//OEP2bRpE7t372bdunWcOHGCWCzGypUrqa+vvw17pKCgcAf55HG6iUSCM2fO8MEHH5Cfn8+DDz4oN5P7rAqwFCPY19fH/v37cU9M8Fx5OZmxGCop+yEaFYsi6PWXK8ikp0Nmpvh7SooYtHtlKtl1jlkwGMTlcsnNHtPT05mYmAAgLS3tvsj0uxFTg929Xi8ulwuv14vX68Xn8xGJREgkEuj1epKTk0lJScFsNmO1WuWuA5/lsfdZYWpc7tQstmAwKM+vhMNhOVZbitmNRqPTkmIAOR5bo9HIcdpScs3UZBuDwSDXcvkU43VvX0ZaNBrl3LlzHDp0CEEQqKurY+7cudPE4H69EKYW7BkbG+PUqVOcO3cOk8nEqlWrqKysvJw2KqWkSZVoAgExLc3rvboSTTgsvkelEjMm7HawWsXUtMxMMVXNZLqc6zuV+/RYXol0bCVXUUtLC729vQSDQYxGIxaLhZSUFPmnTqdDrVZPy27yer14PB58Ph9arZbc3FwqKyspLS2V+2Ldr2PvfuVa2Yhut5uhoSFcLpechivdRAVBuEog9Xq9XP1PElLp/KvVark9VTQaBbhKoKWyr5KAS79PzVQ0m83YbDasVqucqWg2m0lOTp5WY+U2jp/bX/AmHo8zOTnJqVOnOHv2LPF4HIfDQXl5OUVFRSQnJ9/OPOY7hiAIRCIRPB4PXV1dtLW1MTQ0RHJyMnPnzmXevHnYbLZbty6lXGGpCIPXK4qxywUTE2IBBr9frHQTi4miazSK1nF6uijMUt5wUtJld4ZGc98IcjQapauri6amJoaHh8nMzKS2tpbCwsJpAjsbpIsvFArR399PW1sbHR0dcheOmpoajEbjPT/urofk0rtWWUnpNb1e/6k88UjXi9/vl4vFDA4O4vF4iEaj6PV67HY7+fn5pKamYrVaSU1NxWAwXLNy4J3YPri6Jof0NCWVkh0fH8ftdhMMBtFqtaSnp1NcXExBQQEZGRkYjcZPWrjpzlYZEwQBr9dLT08P7e3tcjSAxWIhKytL3hG73S5XcrpbVvFU6zWRSBAIBOTJtd7eXkZGRvD7/SQlJVFUVERFRQX5+fl3thPrtTdUdFUEg5frN46NXS4zNjl5uXajRiNazCaTaDFnZYlFHSRXhtF42ZVxm/zMN7874qPkhQsX5HYqK1asoLi4+LbejKXvGR4eprGxkY6ODhoaGli2bNl9cdO/FpFIRE7yuXJyNx6P86tf/YrHHnsMs9l81brXKxN6K0iP+GNjY7S3t9Pe3s7Y2BgGg4H8/HwKCgrIz8/HYrFgMpnuuQpz10O6gfh8PsbHx+nt7aW/v5+JiQlUKtW0EpZSFu49U9px2gpT8qqdTicjIyP09vYyNjaG2+2WuyEkJydjtVplX530aGkyma6qpysJ9JVFUK4shBKLxQgEArjdbnw+n+wjdLvdBAIB2Udos9mm1dJNTU29o3ffT8zUorpSHUifTxRml0ts/+ByXS60G4mI1rBeL1rJFstlV4bNJoqyySTWgbySm9l/abuuYY15PB62bNmCSqXiiSeekLvA3snjKxVk2rt3L21tbWzatEmOcLnTSGF+tbW1jIyM4HA4mJycZGBggMWLF8uJO2vXrpWLYMdiMd577z3C4TB+vx+LxUJSUhINDQ0cPHiQlStX0tTUhNlsJjc3lzNnzlBdXc3FixeZN28ep0+fxmAwUFpaisPh4ODBg4yMjFBXVyf3l7sVpOuqv7+f5uZmurq65CqCFRUVZGdn37VGAHeTqQaaZEReuHCBvr4+TCYT8+fPp7a2lpSUlNns94xvuO0FY6UTodfrycnJIScnh/nz58sWiVTQQvL9SBMnQ0NDXLx4kWAwKBc+kapSTRVZYJoITxVnnU6HyWTCYrFgNpvlcm1Sl1GpOMZ9N1iunHwzmcQlM3P6+6a2j4jHRYt5clIUZLcbzp0Tf/f7RWHWaEThNRrFKj12+2V3htUqujIkd8bU75d45x3x+x55RHy/SuxHNTg4yOuvv866deuYP3/+XbN+VCoVKSkpfOlLX6K/v58333yTNWvW3FQ25a2i0+mYmJigv78fv99Pfn4+b731FmvWrOGVV14hHA7z5S9/edp2aDQa2tvbWbp0KTt37mTjxo2888471NbW0tzcjM1m48yZMzz55JNs376d6upqbDYbp0+fpry8nKamJl544QW2b9/OkiVLGBgYIBwO43a7b2kfpASmEydOyGK/aNEiHnvsMUwm02diAvd6SOdGo9Fgs9mw2WzU1tYSi8UYHx/n+PHj/Nu//ZucX5CXl3dLx+SudQOeWinKYDBgNpvlRowKtwmVShRSSeRMJlFIr0TyM0uV0wMB0ZXhcsGlS6Kv2eMR3yNFZiQnixZzVpboa7ZYYMcO+OUv4YEH4OWXEZ58Erdazeuvv86Xv/zla1qZgiDgdDpxu90kJSXhdDopKSkhOTl52vsikQiXLl0CoKysDK/XS0dHB4IgkJOTQ0ZGBp2dndjtdjmK5vJhUJGfn88f//Ef86Mf/Yjk5GSqqqruqPCqVCrWr1/Pv/7rv7J27VoMBgPj4+MEAgFWr15NKBTijTfe4C/+4i+mRf1I5Qcl/2dSUhKJRAKNRkN5eTnDw8O88cYbPPvss7z//vu43e5p11FaWhrxeJysrCzeffddcnNzKSsru+ntl6KT3nvvPSorK3nxxRex2+0zHrNEIiEf/2vF24NoyXd0dMidnqeu293dDUBRUZEsXNFolM7OTkA85xqNBr/fTzAYxG6309PTQzQapaSkRJ5cGxwcJDs7G7fbTX9/PwUFBVitVrmkZGlpKXq9nmAwSDAYJC0tDb/fT2dnJxkZGWRlZSEIAqOjo2RkZFxlIEjnKCcnh8cff5xHHnmEixcvsnXrVmw2G1/60pfkTNjZcs82plS4g6hUlxsTSn7hnBwoLoaqKliwABYvhiVLoKEB5swBh0O0gEMh6O+HCxdg2zYxSmNgAHbtgj17aL5wgZqNG6maO/eaAzEej/PKK68Qj8d55513sFqt7N27l/r6elQqFZFIhPb2dgKBABcuXKC9vZ22tjZ8Ph87duygubkZlUpFc3MzkUiE7du388ADD1zVDkl62qqoqODNN9+kvr7+jncCSUlJ4dChQ6xdu5acnBy6urqw2Wzk5+fT0tKCXq9HEASOHDlCbW0t8Xicffv2kZSUxMWLF0lNTeXs2bNkZmZy7NgxzGYzExMTaLVaecJHp9PR0dGB2WymtbWVwsJCTp8+TWlpKd3d3fINLDMz8xo3PPHeOvW+rFKJ52THjh1cvHiRF154gQceeOCGlQQFQWyh3tfXR1VV1TXfc+zYMZqbmykoKJArCgKcPn2aAwcO0NLSgsFgICcnB0EQ2Lp1Ky6Xi1gsRl5eHgCvvfYa3d3d6HQ6duzYwcDAAD6fj+LiYnp7e/n2t7/NsmXL+D//5/+QmprKtm3bqK6u5tVXX5VvJKWlpfzoRz+iq6uLuro6/vVf/xWNRsP27dupra3l4sWLfPe73+Xhhx++boKUZDhmZGRQX19PNBply5YtFBcXY7FYrjxefzvT59w1S1fhPuFKV4bkgphShAgQXRQ/+5k4gVdeDjU1eGpq6BkY4Pna2hkv2N7eXg4dOoTVasVqtbJmzRo+/PBDPB6P3E+vrKyMgoICioqKaGxspK2tjVWrVrF69Wr2799PXV0dP/3pT8nNzZXDyq69KyrS09MpKSmhtbX1E/k5b0QkEmFwcJDFixdTWFiIRqPhm9/8JpFIBIPBQFlZGWq1mo6ODrnuh/QetVrNypUr0Wg0LFq0CK1Wy4IFC9DpdESjUdkHHI1GMRgMPPnkk2g0Gtk/XFNTw+HDh1m0aBE2m4333nuPefPmXbWN8Th861swNARf+hKsXQtFRQLHjx/B6/XyB3/wB7NtuoharSYtLQ23282hQ4c4e/YslZWVJCcnc/z4cWpqamhqasLr9XLp0iU++ugjALKysujt7ZX37+OPP2bBggXEYjH27dvHhg0b5Djs1tZWNBqN3KUkOTlZtlTD4TDnzp3DYrHIf69atYr9+/dz7NgxLBYLjzzyCH/zN3/DM888w6JFi7hw4QKxWIzBwUG+9rWv0dfXx6VLl3jggQem1Ym5EZL4So0633jjDb75zW9etw/iVBTRVbg1dDr4u7+DvDzIzobkZEba29EcPYr6OtZCXl4e1dXVfPnLX+aNN97g7bffZnx8nImJCU6ePMnixYspKSlBo9HIQvzkk0+i1WrlVit2u5309HSOHDkiNzudCZVKxZw5c/jlLwf57ZPrHSEUEvj44w7mzWvg17/WIM6j6H+7AOgAgVCoDL1ex7Zt0k1ppsJLhit+Tv39SmHUMTHxAIcPH0almuSBB74y5fMvE49Days0N8NHH4leosWLwWYb5x//8clZC+5UYrEYmzdv5otf/CLbtm3jz//8z3E4HBw+fJiqqiosFgvl5eWypWs2m2lvb0er1WIwGAgGg/LnRKNR8vPzefPNN+WGqBUVFXR2dmIymRgdHWVycpJly5Zx+PBhKioqOHDgAFarlblz57JlyxbZp67VatFqtUQiEQD5KUev17NmzRrefvttzp8/T21t7S33c5PcWEVFRXR1dVFdXT2r9RTRVbg19HpYtWrav9RqtZw9dOPV9Xz1q1/F6XTS09NDQUEBX/3qV2ltbWXfvn3U1dXx61//moULF8qTqE1NTdTV1RGPx2lpaeFb3/oWb775Jt3d3de1VKLRKLm5cAuuzpvASE3N2hu8RwUY78B3qwA7DQ2PXfddsZjomgfxnpmRIR6TRCJINBpGEIRZi8/U0DQpGumP/uiPeO211+SJcwlpkhtE8XM4HIyOjiIIAqWlpbjdbvR6PZmZmXIftcHBQUZGRjh+/Dgulwu1Ws3ixYvlRgRlZWX09vbS2tpKZ2cnGzduJBwOc+nSJaqrq9m5cycjIyNkZ2dfdVNesWIFGo2Gf/mXf6GwsPCq/bpZAQ6HwzflulJEV+G2kZOTw+DgoPxIfS2cTicGg4FLly7JXUR+7/d+T07rnTdvHtXV1Vy6dImmpiYOHz7MihUr+N3f/V2SkpLkmsxf/OIX2bFjB2azmTlz5sy4TYIgcPbsWR56aDEzuB4/N8RiUFsLlZWie6G+XvQOXbgwl23bfskf/uEfzrojciKRwOl0EgwG2bhxI21tbSxcuJA5c+YQj8dJSUnB4/EQCARYuXLltEnz9PR0tm/fTiKR4Omnn6a5uZnS0lK+8pWvsGvXLoqLi1m7di16vZ6LFy8yNDREZWUlu3btoqenh40bN1JVVSX3WauqqmLnzp0Eg0GeeeYZKioqqKqq4oMPPuC5556T5wcmJiYYHBzk7NmzDA0NsWzZMnJzczl16hR6vZ6jR4+yatWqWUfbSGNrcnLypioO3vY4XYXPL9JkiOSr/bTD8gRBoLu7m61bt/Knf/qnt/T4/FlCEMRAlSuboCYSCRobG2lsbOSpp56S62d/2ufvXkUKrfvoo49ob2/nxRdfJDU19cq33b3kCIXPN8FgkFdeeYVVq1axYMGCT+3CFQSB4eFh3vrud3m2pobcBx9ElZMjZu3NoujQ3eIG199dO35S+NXWrVvRarWsXbuW4uJipUX9b5HOk9fr5ciRIxw7dowFCxbw4IMPzpT5qIiuwt1Bykb76U9/SllZmRyzejeJx+OcPXuW3bt389zjj1MUDKLq6BDjpWIxMXa5rAyKisTZJKPxU0uXdrvd/L//9/+orKwkHA6zYcMGtmzZQigUoqGh4a4kdkhICUxdXV3s378fl8tFdXU1dXV1ZGZm3vKE0/1MIpHA7/fT0dHBsWPHmJiYoLa2lqVLl94oM00RXYW7Szgc5r333qOtrY1HHnmE6urqO2o1SZNtg4OD7N69m0QiwTPPPDM9cD+REEPdxsagsxO6usSMPb1ejFOuqID8fNHROTWQ9Q4SiUT4m7/5G/7rf/2vfO973+Oxxx7j3LlzbNiwgX/+53/mf//v//2p1DOQOsCcO3eOCxcu4HK5SEtLo6amhuLi4ns/df4WkG464XCYwcFB2tvb6ezslOOCFyxYQGFh4WzdVHcvDVhBAZDjSUdGRtizZw/vv/8+CxcuZP78+bL/65NerJLBEAgEaG1tpampiXg8zrp166isrLz689XqyzWOi4vF/yUSYor0wIAYT/Wb34gJIKmpUFgIJSViFt4ddEtIEQCSgEn1RD5NH7TY/drGsmXLWLZsmVzVraWlhebmZvx+v1wTIi8vj7y8PKxW61XpwveSIF9pYEYiEbxeLxMTE/T09DA0NMTIyAiCIJCZmUllZSWbNm2Sy9bern1RLF2FO04ikWBycpJjx45x7tw51Go1paWllJaWkpubS1JSkuwXu1EWlFTWcXx8nM7OTjlbraioiMWLF5OXl/fJLMNEQqxLMTEBfX3Q3i5axtGoGGNVVCQKseSW+IRWqOReeOyxx9izZw9f+9rX2LdvH36/n6VLl1JTU3PPCZd0HtxuNwMDAwwMDDA4OIjX6yUej2M0GklJSSEjI0OuYSCFgl2rmNXt2L+pRdKlui3xeJxIJCLX9JV6N0rx3tK22u12CgoK5FoxUlnHT7hdintB4d4gHo/jcrno7Oyks7OToaEhwuGw3K1ZKmItXYzxeFwW2kAgQCgUQqvVkpaWRnFxMeXl5WRkZNxZq1BySzidogj39YmV3QwG0QouK4OCAtFXfBvbNt1vSI/mwWAQr9fL2NgYLpdLXvx+vyyGgiDI8bt6vV5OZtBoNHL4oGQxS1lpUnPcK2vlSokVUudvqXaF1GHCYrHI4m+320lLS8NkMpGUlCR/1x1AEV2Fe4+ppfT8fj8+n49QKEQkEpGTLNRqtdxRIDk5GbPZ/On6EqXrJZEQS2kODsLFi2I9Cr9frHPscIjui5wcsV7FTFXaPidM1Zip51zq8BCJRGTxlET5etUFJQtZEmqpJoXUkWKqkF5ZCOkuooiugsIdQ6pzHI2KQtzfL07S9faKRemtVtE/XFAgpk0nJ4spYZ9TEf6coIiugsJdRxDESTm3WxTg7m7xZzQq+oQLCkQfcU6OWO3tLkVMKNwVFNFVUPjUkSziSATGx0XfcGen6KKIxcTCQfn5UFoqivKUcoiKEN93KKKroHBPIhWUj0TEyTlJiEdGxBC3zEzRGi4sFMtrmkzTc3gV7lUU0VVQuK+Ixy8ncnR3Q0eHaB0nJYlCXFYmWsXp6eJEndSIVOFeQRFdBYX7lqmNST0e0Qru6hIn7CYnRdHNzhat4cJCMXTNOKWEpCLGnwaK6CoofOYQBNEXHAyKfuGeHnGizu0WhTgrSxThoqLLXaAV18TdQhFdBYXPBZKPWApdkyIm3G5RdDMzxRoT10vmcLvFJqUOxycS6alZYlPrGgSDQUKhkByjG4lEiEaj0+J2o9Go3ApeSoqQEmCkpAqDwYDBYECn003722AwYDQaMZlM07LepDjfuxSvq4iugsLnkqnJHC4XDA+LHZ8HB0VhNhovNyXNzxeF+De/gZdfhv/yX+BrXxPrUFyjq7NEPB7H5/Ph8/mYmJhgeHgYt9uN1+vF4/HIAgrIiS5TBVLKSNPr9RiNRvR6vVzRTK1Wy2ndUi88KYkiHA4TCoXkbDRJtKUlGAxOS6wwmUxYLBYsFovcRdpisWA2m6f1N7tNoqyIroKCwm+RrvloVJys6+8XIyb6+sS/m5th61bRCq6vh7/+a4SHHyau1RIIBBgdHaWvr4+hoSEmJiYIBAJyy5709HSys7OxWq1YLBasVquc1i0J6J3OJpya9SYtsVgMn88n3wxcLhcjIyO43W58Ph+CIGAymcjLy8PhcOBwOLBYLBiNxlvdVkV0FRQUboCUzPHCCwi/+hUCkAASFgvDTzzBr6ur8anVZGVlUVRURH5+FMILwgAACIRJREFUvlzHwGAw3FOFeWaL5PYIBAL4fD4GBwcZGBigv7+fQCCAXq/H4XAwZ84cCgoKMBqN06qoXQdFdBUUFK6PIAjE/X76v/ENTv/nfxLMziatoYGC5cvJrq5Gn5eHwWy+Zl2DzxJTLWWn00l3dzcXL15kcHAQs9lMbW0ttbW1WCwWYMbjoIiugoLCtREEQW5Dc/LECVItFuoXL6asvPyq+rifVwRBIBaLMT4+zsmTJ7lw4QIWi4UHH3yQkpKSa3UDVkRXQUFhOlJd3EOHDtHc3MyCBQtYvHgxFotlRitWEAS6urrQ6/Xk5+df8z3xeJxLly5ht9vJyMiYtm5PTw9+v5+Kigo5GiEUCtHW1kZ6ejpZWVmcPn0an8+HVqtl4cKF+Hw+uru7yc3NxW63c/HiRUwmEyUlJQQCAS5dukRGRga5ubmMjY0xMDBASUkJKSkpdHd3EwgEKC8vZ3x8nI6ODgBKSkoIhUIMDg5itVqZN28e/f39OJ1OKisrMZlM1z1u8Xic7u5u9u3bJ7dZkjpV/5YZRVfzne985zqnheu+qKCgcP/i9/v58Y9/jMFg4Pnnn6eqqmpWE0fnz5/n448/5gtf+MI1X+/o6GD37t0UFxdjs9nk//f09PD/27ufnyb2NY7j7ym0RSilRfoDKRILAlaiRDAquNCEKEaNMRpjYnSnGxO3Llzdv8HERFYmbKywgcACTUAjEJDqSdEFaOOxSGNLoS1mSn937uJmGvTIFe/xcO89fl9Jw4KhTL+LTyfzfeZ5+vv7+fz5M6FQiIaGBgDcbjeJRILh4WF2796N2+3G5/MxMTFBW1sbvb291NXVkc1mefr0KaFQiOnpacxmM/39/eh0Ovr7+3G5XNy9exer1crAwAB2u52HDx+SSCT48OEDwWCQ8fFxZmZmMJvNTE5OMjs7SzKZxGKx0NPTg8lkYmxsjPb29g3XQa2qMJvNHDhwAIvFgtvtRqPRrA/ef2y0fmJcjyD8gvL5PG63m/b2dg4fPrzpWwiSJGE2m5EkiVevXjE5OYnNZqO5uZnx8XFsNhuyLDM3N0dbWxvPnz8HwGg0oigKTqeTgwcP0tPTQ3d3d2GI6O3btwkGg/j9fu7cuUMkEmF2dpZQKMTS0hKxWIzq6mr8fj9nz57FZrPh8XhIpVJUVlYiSVKhF29VVVWhjEyr1WI0GllbW+PSpUucOXOGkZEROjo6iEajNDY2snPnTpaXlzGbzRw7doyRkRFSqdQXZWQbrYUkSTidTm7evMn9+/ex2+04nc5/+3fiZo0g/ILUsqkfCdz18vk8fX197Nixg9HRUbRaLQ0NDUxNTdHU1ERnZydNTU2Fl9PpJJPJFJqOp1KpwvvkcrlCU/J0Oo1Go8Hj8dDS0sLa2homkwmLxcK9e/fo7u5mbGyMiYkJ0uk0JpMJr9dbCMCSkhI8Hk9hNFA8Hsfn81FWVoZGoyEQCKDX6ykvL6erqwuXy8WDBw8wGAyUlpbS19dHNBr9obWQJAmDwcDJkyeZmpr67vEidAXhF1RUVFS4MvwR6/eAdDodZWVl3Lp1i0ePHpFMJpEkqXCMTqejoqKCiooKDAYDdXV1hMNhFhYWcDqdyLJMKpWiqqqKlZUVwuEwDoeDlZUV4vE4FouFmpoaAGw2G5lMhvr6eq5fv47D4cDlcuHz+bh48SImk4nXr1+jKApXrlxhcXERr9eLy+Xi/PnzeDweFEVhenqa1tZWACKRSGEmmkaj4dq1a3R2dtLS0oJOp/vhdYnH45Ss73mxAbGRJgi/IEVRGBoaIh6Pc+HChU3PmFMUhdHRUebn5+no6ODFixfs3buXSCSCLMssLy9TW1tLKBTi6tWrX2xIpVIpBgcHkWWZ06dP4/f7C8E8OjqKxWLh1KlTzM/PYzQaqa2tJZfL8fjxYwKBAO3t7eTzebxeL1arlRMnTjA9Pc3bt28L06efPHlCLBbD4XDQ1tbG0NAQ6XSaI0eO4HQ6efnyJUePHgVgYGCAaDSK1Wqls7OTgYEBJEmiq6uL6urqTZfEKYrC4uIivb293Lhxg6qqKhDVC4IgfC2TyTA8PIzf7+fcuXPs3Lnzvzd77v+QoiikUikmJiaYmZnh8uXL1NXVqesnQlcQhD9SS8AGBwfR6/UcP36c+vr6Qt2pCOAvqXm5urr6r7rm335jz549dHV1UVpauqmSMRG6giCQzWZZWFjg2bNnLC0tsWvXLvbv34/D4fg6TH5J2WyWWCzG3NwcXq+XRCJBa2srhw4dwrDuKb11ROgKgvB96obQu3fvePPmDcFgkJKSEpxOJ42NjYWNJ7Vxzd+N2o5SDdmFhQV8Ph+BQABJkmhubmbfvn3Y7fZvPYW2nghdQRA2T82FXC5HOBzm/fv3+Hw+lpaWUBSFyspKbDYbtbW1WK1WjEbjH66I/xdD+eu8y2QyyLJMNBolEAjw6dMngsEgiUQCg8GAw+GgubmZmpoaDAYDsOnPJUJXEIQ/R+3IlU6nCYfDhEIhPn78SDgcRpZlstkser0eg8HA9u3bC+ViJpOJ8vLyQrPx4uJiioqKfnqbR/X81FK4bDZLJpMhmUwSi8VYXV1ldXWVaDRKJBIhmUySz+fR6/WYTCYcDgfV1dXY7XYMBgPFxcV/5vxE6AqC8NdRp0Ikk0lkWWZlZaUQcrFYDFmWC0+J5XK5wqO0xcXFaLVaioqKvhnG6nFqoKr/S70FkMvlvghYddKEJEmF91ZDVf0SMJvNmM1mtm3bVpg88RdclYvQFQRh630rX9TAVMfyqGGphqjaeFy9v7p++oP6U22Irr7UyRNarRadTrfhU3ZbeMvjPw7d33/+uQiCIPzt7droF98LXUEQBOEnEr0XBEEQtpAIXUEQhC0kQlcQBGELidAVBEHYQiJ0BUEQtpAIXUEQhC30T0CHkjg6spDVAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ZNGEpWAE2n"
      },
      "source": [
        "# Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aplFob-SAGwQ"
      },
      "source": [
        "One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a plot_importance() function that allows you to do exactly this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo_dRHatAHZ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "ef068977-26cd-4224-9a73-ae2a6916f14f"
      },
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
        "\n",
        "# Train the model: xg_reg\n",
        "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
        "\n",
        "# Plot the feature importances\n",
        "xgb.plot_importance(xg_reg)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19:34:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6e1XuOQD2m-"
      },
      "source": [
        "# XGB + Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkCOek6KU1Wl"
      },
      "source": [
        "## Boosting rounds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyRS5YiOUHxx"
      },
      "source": [
        "Increasing number of boosting rounds. \n",
        "\n",
        "increasing the number of boosting rounds decreases the RMSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUrArHTfD6Jl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "2821a2e7-6c2e-49dc-dad3-71401e5d8b00"
      },
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary for each tree: params \n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
        "\n",
        "# Create list of number of boosting rounds\n",
        "num_rounds = [5, 10, 15]\n",
        "\n",
        "# Empty list to store final round rmse per XGBoost model\n",
        "final_rmse_per_round = []\n",
        "\n",
        "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
        "for curr_num_rounds in num_rounds:\n",
        "\n",
        "    # Perform cross-validation: cv_results\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "    \n",
        "    # Append final round RMSE\n",
        "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
        "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19:37:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:37:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:37:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:37:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:37:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:37:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:37:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:37:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:37:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "   num_boosting_rounds      rmse\n",
            "0                    5  5.950357\n",
            "1                   10  3.784688\n",
            "2                   15  3.525731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nD5dvQyUblV"
      },
      "source": [
        "you can very easily have XGBoost automatically select the number of boosting rounds for you within xgb.cv(). This is done using a technique called early stopping.\n",
        "\n",
        "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. Here you will use the early_stopping_rounds parameter in xgb.cv() with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when num_boost_rounds is reached, then early stopping does not occur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU3jP6R4Ueiq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "34f695f4-0208-40ba-aeda-e7e4919c96f2"
      },
      "source": [
        "# Create your housing DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "\n",
        "# Create the parameter dictionary for each tree: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
        "\n",
        "# Perform cross-validation with early stopping: cv_results\n",
        "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=50, early_stopping_rounds=10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19:37:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:37:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:37:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
            "0         17.131358        0.020615       17.223398       0.067772\n",
            "1         12.382815        0.025833       12.619156       0.132110\n",
            "2          9.063982        0.037021        9.505188       0.117294\n",
            "3          6.726434        0.034949        7.339914       0.120753\n",
            "4          5.102424        0.045690        5.954197       0.137525\n",
            "5          3.976571        0.049928        5.043778       0.198929\n",
            "6          3.195956        0.061089        4.436178       0.214429\n",
            "7          2.694112        0.066253        4.066125       0.295735\n",
            "8          2.341284        0.050819        3.837729       0.326883\n",
            "9          2.099043        0.052466        3.667520       0.352620\n",
            "10         1.931934        0.042599        3.573348       0.360856\n",
            "11         1.824035        0.047074        3.510915       0.370727\n",
            "12         1.713532        0.058416        3.469601       0.372424\n",
            "13         1.628079        0.058896        3.441818       0.394071\n",
            "14         1.559479        0.058446        3.403518       0.407603\n",
            "15         1.482433        0.049533        3.367522       0.431987\n",
            "16         1.437707        0.039150        3.351383       0.439358\n",
            "17         1.370599        0.033117        3.350095       0.440003\n",
            "18         1.346624        0.027598        3.349142       0.440584\n",
            "19         1.305355        0.025663        3.334845       0.432809\n",
            "20         1.256261        0.041769        3.315190       0.430103\n",
            "21         1.218349        0.037456        3.305477       0.429272\n",
            "22         1.183180        0.030335        3.302783       0.435573\n",
            "23         1.134490        0.028117        3.306710       0.432787\n",
            "24         1.102499        0.025421        3.309918       0.429725\n",
            "25         1.072964        0.026608        3.304234       0.427132\n",
            "26         1.047389        0.032165        3.298778       0.430648\n",
            "27         1.022133        0.029575        3.293409       0.427261\n",
            "28         0.986890        0.030724        3.288524       0.429088\n",
            "29         0.957546        0.035885        3.290293       0.428385\n",
            "30         0.933272        0.032563        3.293034       0.431208\n",
            "31         0.913488        0.038415        3.290024       0.431993\n",
            "32         0.897809        0.037734        3.286117       0.432014\n",
            "33         0.875283        0.038628        3.287901       0.428564\n",
            "34         0.852653        0.041018        3.271714       0.428855\n",
            "35         0.831815        0.036928        3.266255       0.428812\n",
            "36         0.810852        0.031442        3.258712       0.431901\n",
            "37         0.784156        0.024416        3.259458       0.437193\n",
            "38         0.765717        0.026063        3.258147       0.440076\n",
            "39         0.738290        0.025509        3.259074       0.441318\n",
            "40         0.713401        0.012099        3.260375       0.444337\n",
            "41         0.700882        0.016973        3.255900       0.442181\n",
            "42         0.686943        0.017920        3.261377       0.441253\n",
            "43         0.664417        0.025525        3.257313       0.438848\n",
            "44         0.641893        0.022671        3.257818       0.441389\n",
            "45         0.625917        0.013791        3.260048       0.439502\n",
            "46         0.607496        0.013050        3.259310       0.439397\n",
            "47         0.597147        0.007860        3.259014       0.437781\n",
            "48         0.588216        0.008274        3.257008       0.440020\n",
            "49         0.570306        0.008310        3.256143       0.439984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEDMtuaOU3yp"
      },
      "source": [
        "## Common tree tunable parameters\n",
        "\n",
        "1- **learning rate (eta):** Learning rate affects how quickly the model fits the residual error using additional base learners. A low learning rate will require more boosting rounds to achieve the same reduction in residual error.\n",
        "\n",
        "2- **Gamma** (Have effect on how strongly regulatized the trained model will be)\n",
        "\n",
        "3- **Lambda** (Have effect on how strongly regulatized the trained model will be)\n",
        "\n",
        "\n",
        "4- **Alpha** (Have effect on how strongly regulatized the trained model will be)\n",
        "\n",
        "5- **max_depth**: max depth per tree\n",
        "\n",
        "6- **subsample**: % samples used per tree. Between 0 and 1. To low underfitting, to high overfitting.\n",
        "\n",
        "7- **colsample_bytree**: % *features* used per tree. Between 0 and 1. To low simplifies (underfitting), to high could potentially overfit the model. It select only some features. This could be really important.\n",
        "\n",
        "8- num_boost_round: tunable parameter of xgBoosting\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-f-KwWbWazI"
      },
      "source": [
        "## Common linear tunable parameters\n",
        "\n",
        "1- Lambda: l2 regulatization term on weights\n",
        "\n",
        "2- Alpha: l1 regulatization term on weights\n",
        "\n",
        "3- ambda_bias: l2 reg term on bias\n",
        "\n",
        "4- num_boost_round: tunable parameter of xgBoosting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JzDkFcCXhuM"
      },
      "source": [
        "### Tuning eta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3pXlmCIXbHo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "f6423483-4c9f-420b-ba3d-b1e730a0e9ce"
      },
      "source": [
        "# Create your housing DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary for each tree (boosting round)\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
        "\n",
        "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
        "eta_vals = [0.001, 0.01, 0.1]\n",
        "best_rmse = []\n",
        "\n",
        "# Systematically vary the eta\n",
        "for curr_val in eta_vals:\n",
        "\n",
        "    params[\"eta\"] = curr_val\n",
        "    \n",
        "    # Perform cross-validation: cv_results\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,\n",
        "                        num_boost_round=10, early_stopping_rounds=5,\n",
        "                        metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "    \n",
        "    # Append the final round rmse to best_rmse\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19:53:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:53:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:53:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:53:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:53:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:53:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:53:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:53:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:53:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "     eta  best_rmse\n",
            "0  0.001  23.649514\n",
            "1  0.010  21.740975\n",
            "2  0.100   9.473431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35nEfPyQYJOb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnXPsJEuYTkO"
      },
      "source": [
        "###  Tuning max_depth\n",
        "\n",
        "tune max_depth, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxbSn-cFYIjM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "f01a7adc-69d2-474f-ba18-5437ffed16e2"
      },
      "source": [
        "# Create your housing DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "\n",
        "# Create the parameter dictionary\n",
        "params = {\"objective\":\"reg:linear\"}\n",
        "\n",
        "# Create list of max_depth values\n",
        "max_depths = [2, 5, 10, 20]\n",
        "best_rmse = []\n",
        "\n",
        "# Systematically vary the max_depth\n",
        "for curr_val in max_depths:\n",
        "\n",
        "    params[\"max_depth\"] = curr_val\n",
        "    \n",
        "    # Perform cross-validation\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
        "                 num_boost_round=10, early_stopping_rounds=5,\n",
        "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "    \n",
        "    # Append the final round rmse to best_rmse\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19:54:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:54:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:54:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:54:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:54:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:54:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:54:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:54:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "   max_depth  best_rmse\n",
            "0          2   4.097683\n",
            "1          5   3.867900\n",
            "2         10   3.840242\n",
            "3         20   3.806038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJXtKONEYpRd"
      },
      "source": [
        "### Tuning colsample_bytree\n",
        "\n",
        "You've already seen this if you've ever worked with scikit-learn's RandomForestClassifier or RandomForestRegressor, where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbTFOaeTY1cR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "decdbb96-8453-424a-f43b-4b5f47f659c3"
      },
      "source": [
        "# Create your housing DMatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "\n",
        "# Create the parameter dictionary\n",
        "params={\"objective\":\"reg:linear\",\"max_depth\":5}\n",
        "\n",
        "# Create list of hyperparameter values\n",
        "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
        "best_rmse = []\n",
        "\n",
        "# Systematically vary the hyperparameter value \n",
        "for curr_val in colsample_bytree_vals:\n",
        "\n",
        "    params[\"colsample_bytree\"] = curr_val\n",
        "    \n",
        "    # Perform cross-validation\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
        "                 num_boost_round=10, early_stopping_rounds=5,\n",
        "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "    \n",
        "    # Append the final round rmse to best_rmse\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19:56:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:56:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:56:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:56:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:56:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:56:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:56:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:56:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "   colsample_bytree  best_rmse\n",
            "0               0.1   6.151299\n",
            "1               0.5   4.132432\n",
            "2               0.8   3.990314\n",
            "3               1.0   3.867900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBMvoEMZ3s3"
      },
      "source": [
        "### Tuning  several hyperparameter simultaneously"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ARC0zaaWZVG"
      },
      "source": [
        "GridSearch CV and RandomSearch\n",
        "\n",
        "GridSearch searches exhaustively over a given set of hyperparameters.\n",
        "\n",
        "Number of models = number of disctint values per hyperparameter multiplied across each hyperparameter. Ask to give the best result of all possibilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aam6d_sNcAMr"
      },
      "source": [
        "Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6caWAyApUvj"
      },
      "source": [
        "#### GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMqrH3boa2t2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5081b83c-28b0-4849-a7f2-0576f02a2af1"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create the parameter grid: gbm_param_grid\n",
        "gbm_param_grid = {\n",
        "    'colsample_bytree': [0.3, 0.7,1],\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [2, 5,6,8],\n",
        "    'num_boost_round': [10,15,20]\n",
        "}\n",
        "\n",
        "# Instantiate the regressor: gbm\n",
        "gbm = xgb.XGBRegressor()\n",
        "\n",
        "# Perform grid search: grid_mse\n",
        "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
        "                        scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
        "grid_mse.fit(X, y)\n",
        "\n",
        "# Print the best parameters and lowest RMSE\n",
        "print(\"Best parameters found: \", grid_mse.best_params_)\n",
        "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 36 candidates, totalling 144 fits\n",
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:10:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 2, 'n_estimators': 100, 'num_boost_round': 10}\n",
            "Lowest RMSE found:  4.464045126212519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 144 out of 144 | elapsed:    6.9s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k70HPpsDpSC2"
      },
      "source": [
        "#### RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLkwRUG6panE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "69e4cfe7-ab7c-4ae5-dc87-d8623388e2ff"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "gbm_param_grid = {\n",
        "    'colsample_bytree': [0.3, 0.7,1],\n",
        "    'n_estimators': [200],\n",
        "    'max_depth': range(12),\n",
        "    'num_boost_round': [10,15,20]\n",
        "}\n",
        "\n",
        "# Instantiate the regressor: gbm\n",
        "gbm = xgb.XGBRegressor(n_estimators=10)\n",
        "\n",
        "# Perform random search: grid_mse\n",
        "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,\n",
        "                                    n_iter=5, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
        "randomized_mse.fit(X, y)\n",
        "\n",
        "# Print the best parameters and lowest RMSE\n",
        "print(\"Best parameters found: \",randomized_mse.best_params_)\n",
        "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
            "[21:12:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[21:12:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[21:12:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Best parameters found:  {'num_boost_round': 20, 'n_estimators': 200, 'max_depth': 4, 'colsample_bytree': 0.7}\n",
            "Lowest RMSE found:  4.516988876058619\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.8s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70H-Q9IfqgoE"
      },
      "source": [
        "## Limits of GridSearch and randomSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcJAoLcyqsIf"
      },
      "source": [
        "Limits of gridsearch is eficiency and processing resources.\n",
        "\n",
        "Limits of RandomSearch. Parameter space to explire can be massive. Because it jumps randomnly can never fin an optimal configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxhez85zrf6l"
      },
      "source": [
        "# Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzc_4SbBrhuk"
      },
      "source": [
        "Pipelines are usefull because it can be put into another scikit learn object as cross_val_score or GridSearchCV, or RandomSearchCV.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGiB5XZfsHlo"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvaU6uHlsU9k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "68a812ce-35eb-4f2c-e730-ebbf196ed79c"
      },
      "source": [
        "# Ames Housing Dataset from Datacamp\n",
        "\n",
        "df = pd.read_csv('https://assets.datacamp.com/production/repositories/943/datasets/17a7c5c0acd7bfa253827ea53646cf0db7d39649/ames_unprocessed_data.csv')\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1460 entries, 0 to 1459\n",
            "Data columns (total 21 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   MSSubClass    1460 non-null   int64  \n",
            " 1   MSZoning      1460 non-null   object \n",
            " 2   LotFrontage   1201 non-null   float64\n",
            " 3   LotArea       1460 non-null   int64  \n",
            " 4   Neighborhood  1460 non-null   object \n",
            " 5   BldgType      1460 non-null   object \n",
            " 6   HouseStyle    1460 non-null   object \n",
            " 7   OverallQual   1460 non-null   int64  \n",
            " 8   OverallCond   1460 non-null   int64  \n",
            " 9   YearBuilt     1460 non-null   int64  \n",
            " 10  Remodeled     1460 non-null   int64  \n",
            " 11  GrLivArea     1460 non-null   int64  \n",
            " 12  BsmtFullBath  1460 non-null   int64  \n",
            " 13  BsmtHalfBath  1460 non-null   int64  \n",
            " 14  FullBath      1460 non-null   int64  \n",
            " 15  HalfBath      1460 non-null   int64  \n",
            " 16  BedroomAbvGr  1460 non-null   int64  \n",
            " 17  Fireplaces    1460 non-null   int64  \n",
            " 18  GarageArea    1460 non-null   int64  \n",
            " 19  PavedDrive    1460 non-null   object \n",
            " 20  SalePrice     1460 non-null   int64  \n",
            "dtypes: float64(1), int64(15), object(5)\n",
            "memory usage: 239.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrGgVSx635kK"
      },
      "source": [
        "## Label Encoder and oneHot encoder (DON'T USE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgGcbVTTvXqM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "3950aac9-a421-4fcb-ed77-c4720b47324f"
      },
      "source": [
        "# Import LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Fill missing values with 0\n",
        "df.LotFrontage = df.LotFrontage.fillna(0)\n",
        "\n",
        "# Create a boolean mask for categorical columns\n",
        "categorical_mask = (df.dtypes == object)\n",
        "\n",
        "# Get list of categorical column names\n",
        "categorical_columns = df.columns[categorical_mask].tolist()\n",
        "\n",
        "# Print the head of the categorical columns\n",
        "print(df[categorical_columns].head())\n",
        "\n",
        "# Create LabelEncoder object: le\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Apply LabelEncoder to categorical columns\n",
        "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
        "\n",
        "# Print the head of the LabelEncoded categorical columns\n",
        "print(df[categorical_columns].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  MSZoning Neighborhood BldgType HouseStyle PavedDrive\n",
            "0       RL      CollgCr     1Fam     2Story          Y\n",
            "1       RL      Veenker     1Fam     1Story          Y\n",
            "2       RL      CollgCr     1Fam     2Story          Y\n",
            "3       RL      Crawfor     1Fam     2Story          Y\n",
            "4       RL      NoRidge     1Fam     2Story          Y\n",
            "   MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive\n",
            "0         3             5         0           5           2\n",
            "1         3            24         0           2           2\n",
            "2         3             5         0           5           2\n",
            "3         3             6         0           5           2\n",
            "4         3            15         0           5           2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w83G1JqYxvhG"
      },
      "source": [
        "In the categorical columns of this dataset, there is no natural ordering between the entries. As an example: Using LabelEncoder, the CollgCr Neighborhood was encoded as 5, while the Veenker Neighborhood was encoded as 24, and Crawfor as 6. Is Veenker \"greater\" than Crawfor and CollgCr? No - and allowing the model to assume this natural ordering may result in poor performance.\n",
        "\n",
        "As a result, there is another step needed: You have to apply a one-hot encoding to create binary, or \"dummy\" variables. You can do this using scikit-learn's OneHotEncoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0AB2W-bxw1q"
      },
      "source": [
        "# Import OneHotEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Create OneHotEncoder: ohe\n",
        "ohe = OneHotEncoder(handle_unknown='ignore')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_Ym9w130-Av",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "32efc109-0564-4fa0-cf42-751cb11b9dee"
      },
      "source": [
        "df_encoded = pd.DataFrame(ohe.fit_transform(df[categorical_columns]).toarray())\n",
        "df_encoded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1460 rows  46 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1    2    3    4    5    6   ...   39   40   41   42   43   44   45\n",
              "0     0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
              "1     0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
              "2     0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
              "3     0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
              "4     0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
              "1455  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
              "1456  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
              "1457  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
              "1458  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
              "1459  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
              "\n",
              "[1460 rows x 46 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy_RTpIzyOXQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "3c646ebb-3374-462b-8493-f6c87d9874b6"
      },
      "source": [
        "df_encoded = df.join(df_encoded)\n",
        "df_encoded.drop(categorical_columns,axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>Remodeled</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>PavedDrive</th>\n",
              "      <th>SalePrice</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60</td>\n",
              "      <td>3</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>0</td>\n",
              "      <td>1710</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>548</td>\n",
              "      <td>2</td>\n",
              "      <td>208500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>460</td>\n",
              "      <td>2</td>\n",
              "      <td>181500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>3</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>1</td>\n",
              "      <td>1786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>608</td>\n",
              "      <td>2</td>\n",
              "      <td>223500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70</td>\n",
              "      <td>3</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>1</td>\n",
              "      <td>1717</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>642</td>\n",
              "      <td>2</td>\n",
              "      <td>140000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>3</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>0</td>\n",
              "      <td>2198</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>836</td>\n",
              "      <td>2</td>\n",
              "      <td>250000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>60</td>\n",
              "      <td>3</td>\n",
              "      <td>62.0</td>\n",
              "      <td>7917</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1999</td>\n",
              "      <td>1</td>\n",
              "      <td>1647</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>460</td>\n",
              "      <td>2</td>\n",
              "      <td>175000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>85.0</td>\n",
              "      <td>13175</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1978</td>\n",
              "      <td>1</td>\n",
              "      <td>2073</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>500</td>\n",
              "      <td>2</td>\n",
              "      <td>210000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>70</td>\n",
              "      <td>3</td>\n",
              "      <td>66.0</td>\n",
              "      <td>9042</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>1941</td>\n",
              "      <td>1</td>\n",
              "      <td>2340</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>252</td>\n",
              "      <td>2</td>\n",
              "      <td>266500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>68.0</td>\n",
              "      <td>9717</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1950</td>\n",
              "      <td>1</td>\n",
              "      <td>1078</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>240</td>\n",
              "      <td>2</td>\n",
              "      <td>142125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9937</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1965</td>\n",
              "      <td>0</td>\n",
              "      <td>1256</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>276</td>\n",
              "      <td>2</td>\n",
              "      <td>147500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1460 rows  67 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      MSSubClass  MSZoning  LotFrontage  LotArea  ...   42   43   44   45\n",
              "0             60         3         65.0     8450  ...  0.0  0.0  0.0  1.0\n",
              "1             20         3         80.0     9600  ...  0.0  0.0  0.0  1.0\n",
              "2             60         3         68.0    11250  ...  0.0  0.0  0.0  1.0\n",
              "3             70         3         60.0     9550  ...  0.0  0.0  0.0  1.0\n",
              "4             60         3         84.0    14260  ...  0.0  0.0  0.0  1.0\n",
              "...          ...       ...          ...      ...  ...  ...  ...  ...  ...\n",
              "1455          60         3         62.0     7917  ...  0.0  0.0  0.0  1.0\n",
              "1456          20         3         85.0    13175  ...  0.0  0.0  0.0  1.0\n",
              "1457          70         3         66.0     9042  ...  0.0  0.0  0.0  1.0\n",
              "1458          20         3         68.0     9717  ...  0.0  0.0  0.0  1.0\n",
              "1459          20         3         75.0     9937  ...  0.0  0.0  0.0  1.0\n",
              "\n",
              "[1460 rows x 67 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR5XRrAr3gOV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "b301be4f-962b-4712-cf95-6a5f53bd3b21"
      },
      "source": [
        "df_encoded.drop(categorical_columns,axis=1,inplace=True)\n",
        "df_encoded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>Remodeled</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>SalePrice</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>0</td>\n",
              "      <td>1710</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>548</td>\n",
              "      <td>208500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>460</td>\n",
              "      <td>181500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>1</td>\n",
              "      <td>1786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>608</td>\n",
              "      <td>223500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>1</td>\n",
              "      <td>1717</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>642</td>\n",
              "      <td>140000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>0</td>\n",
              "      <td>2198</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>836</td>\n",
              "      <td>250000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>60</td>\n",
              "      <td>62.0</td>\n",
              "      <td>7917</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1999</td>\n",
              "      <td>1</td>\n",
              "      <td>1647</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>460</td>\n",
              "      <td>175000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>20</td>\n",
              "      <td>85.0</td>\n",
              "      <td>13175</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1978</td>\n",
              "      <td>1</td>\n",
              "      <td>2073</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>500</td>\n",
              "      <td>210000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>70</td>\n",
              "      <td>66.0</td>\n",
              "      <td>9042</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>1941</td>\n",
              "      <td>1</td>\n",
              "      <td>2340</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>252</td>\n",
              "      <td>266500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>20</td>\n",
              "      <td>68.0</td>\n",
              "      <td>9717</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1950</td>\n",
              "      <td>1</td>\n",
              "      <td>1078</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>240</td>\n",
              "      <td>142125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>20</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9937</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1965</td>\n",
              "      <td>0</td>\n",
              "      <td>1256</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>276</td>\n",
              "      <td>147500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1460 rows  62 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      MSSubClass  LotFrontage  LotArea  OverallQual  ...   42   43   44   45\n",
              "0             60         65.0     8450            7  ...  0.0  0.0  0.0  1.0\n",
              "1             20         80.0     9600            6  ...  0.0  0.0  0.0  1.0\n",
              "2             60         68.0    11250            7  ...  0.0  0.0  0.0  1.0\n",
              "3             70         60.0     9550            7  ...  0.0  0.0  0.0  1.0\n",
              "4             60         84.0    14260            8  ...  0.0  0.0  0.0  1.0\n",
              "...          ...          ...      ...          ...  ...  ...  ...  ...  ...\n",
              "1455          60         62.0     7917            6  ...  0.0  0.0  0.0  1.0\n",
              "1456          20         85.0    13175            6  ...  0.0  0.0  0.0  1.0\n",
              "1457          70         66.0     9042            7  ...  0.0  0.0  0.0  1.0\n",
              "1458          20         68.0     9717            5  ...  0.0  0.0  0.0  1.0\n",
              "1459          20         75.0     9937            5  ...  0.0  0.0  0.0  1.0\n",
              "\n",
              "[1460 rows x 62 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUnTzj7X3_tj"
      },
      "source": [
        "## DictVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLli1Rcx31vu"
      },
      "source": [
        "he two step process you just went through - LabelEncoder followed by OneHotEncoder - can be simplified by using a DictVectorizer.\n",
        "\n",
        "Besides simplifying the process into one step, DictVectorizer has useful attributes such as vocabulary_ which maps the names of the features to their indices. With the data preprocessed, it's time to move onto pipelines!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHVCZAJu4F9I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "a4d71693-5ffc-44eb-9288-03127504395e"
      },
      "source": [
        "# Import DictVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "# Convert df into a dictionary: df_dict\n",
        "df_dict = df.to_dict(\"records\")\n",
        "\n",
        "# Create the DictVectorizer object: dv\n",
        "dv = DictVectorizer(sparse=False)\n",
        "\n",
        "# Apply dv on df: df_encoded\n",
        "df_encoded = dv.fit_transform(df_dict)\n",
        "\n",
        "# Print the resulting first five rows\n",
        "print(df_encoded[:5,:])\n",
        "\n",
        "# Print the vocabulary\n",
        "print(dv.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 2.000e+00 5.480e+02 1.710e+03 1.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
            "  8.450e+03 6.500e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 2.085e+05 2.003e+03]\n",
            " [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  1.000e+00 1.000e+00 2.000e+00 4.600e+02 1.262e+03 0.000e+00 0.000e+00\n",
            "  0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  9.600e+03 8.000e+01 2.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 8.000e+00 6.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 1.815e+05 1.976e+03]\n",
            " [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  0.000e+00 1.000e+00 2.000e+00 6.080e+02 1.786e+03 1.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
            "  1.125e+04 6.800e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 1.000e+00 2.235e+05 2.001e+03]\n",
            " [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  0.000e+00 1.000e+00 1.000e+00 6.420e+02 1.717e+03 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
            "  9.550e+03 6.000e+01 7.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 1.000e+00 1.400e+05 1.915e+03]\n",
            " [4.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  0.000e+00 1.000e+00 2.000e+00 8.360e+02 2.198e+03 1.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
            "  1.426e+04 8.400e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 8.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 2.500e+05 2.000e+03]]\n",
            "{'MSSubClass': 23, 'MSZoning=RL': 27, 'LotFrontage': 22, 'LotArea': 21, 'Neighborhood=CollgCr': 34, 'BldgType=1Fam': 1, 'HouseStyle=2Story': 18, 'OverallQual': 55, 'OverallCond': 54, 'YearBuilt': 61, 'Remodeled': 59, 'GrLivArea': 11, 'BsmtFullBath': 6, 'BsmtHalfBath': 7, 'FullBath': 9, 'HalfBath': 12, 'BedroomAbvGr': 0, 'Fireplaces': 8, 'GarageArea': 10, 'PavedDrive=Y': 58, 'SalePrice': 60, 'Neighborhood=Veenker': 53, 'HouseStyle=1Story': 15, 'Neighborhood=Crawfor': 35, 'Neighborhood=NoRidge': 44, 'Neighborhood=Mitchel': 40, 'HouseStyle=1.5Fin': 13, 'Neighborhood=Somerst': 50, 'Neighborhood=NWAmes': 43, 'MSZoning=RM': 28, 'Neighborhood=OldTown': 46, 'Neighborhood=BrkSide': 32, 'BldgType=2fmCon': 2, 'HouseStyle=1.5Unf': 14, 'Neighborhood=Sawyer': 48, 'Neighborhood=NridgHt': 45, 'Neighborhood=NAmes': 41, 'BldgType=Duplex': 3, 'Neighborhood=SawyerW': 49, 'Neighborhood=IDOTRR': 38, 'PavedDrive=N': 56, 'Neighborhood=MeadowV': 39, 'BldgType=TwnhsE': 5, 'MSZoning=C (all)': 24, 'Neighborhood=Edwards': 36, 'Neighborhood=Timber': 52, 'PavedDrive=P': 57, 'HouseStyle=SFoyer': 19, 'MSZoning=FV': 25, 'Neighborhood=Gilbert': 37, 'HouseStyle=SLvl': 20, 'BldgType=Twnhs': 4, 'Neighborhood=StoneBr': 51, 'HouseStyle=2.5Unf': 17, 'Neighborhood=ClearCr': 33, 'Neighborhood=NPkVill': 42, 'HouseStyle=2.5Fin': 16, 'Neighborhood=Blmngtn': 29, 'Neighborhood=BrDale': 31, 'Neighborhood=SWISU': 47, 'MSZoning=RH': 26, 'Neighborhood=Blueste': 30}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrmFYP6IUNaF"
      },
      "source": [
        "## DictVectorizer ad XGb in Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yBogX4oUVyS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "095c7082-db7d-419d-d929-34412102adbd"
      },
      "source": [
        "# Ames Housing Dataset from Datacamp\n",
        "\n",
        "df = pd.read_csv('https://assets.datacamp.com/production/repositories/943/datasets/17a7c5c0acd7bfa253827ea53646cf0db7d39649/ames_unprocessed_data.csv')\n",
        "\n",
        "y= df.SalePrice\n",
        "X = df.drop('SalePrice',axis=1)\n",
        "\n",
        "# Import necessary modules\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Fill LotFrontage missing values with 0\n",
        "X.LotFrontage = X.LotFrontage.fillna(0)\n",
        "\n",
        "# Setup the pipeline steps: steps\n",
        "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
        "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n",
        "\n",
        "# Create the pipeline: xgb_pipeline\n",
        "xgb_pipeline = Pipeline(steps)\n",
        "\n",
        "# Cross-validate the model\n",
        "cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict(\"records\"), y, cv=10, scoring=\"neg_mean_squared_error\")\n",
        "\n",
        "# Print the 10-fold RMSE\n",
        "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[00:30:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:30:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:30:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:30:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:30:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:30:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:30:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:30:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:30:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:30:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "10-fold RMSE:  29867.603720688923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXaBZP1GVzLC"
      },
      "source": [
        "## Scikit-learn pipeline example with XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvgkd_avX-54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "88105ded-e850-4158-a1d6-0d5c10413a52"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "df = pd.read_csv('https://assets.datacamp.com/production/repositories/943/datasets/82c231cd41f92325cf33b78aaa360824e6b599b9/chronic_kidney_disease.csv',header=None)\n",
        "df.head()\n",
        "\n",
        "df.replace('?',np.nan,inplace=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>48</td>\n",
              "      <td>80</td>\n",
              "      <td>1.020</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>121</td>\n",
              "      <td>36</td>\n",
              "      <td>1.2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.4</td>\n",
              "      <td>44</td>\n",
              "      <td>7800</td>\n",
              "      <td>5.2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>50</td>\n",
              "      <td>1.020</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18</td>\n",
              "      <td>0.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.3</td>\n",
              "      <td>38</td>\n",
              "      <td>6000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>62</td>\n",
              "      <td>80</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>423</td>\n",
              "      <td>53</td>\n",
              "      <td>1.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.6</td>\n",
              "      <td>31</td>\n",
              "      <td>7500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>48</td>\n",
              "      <td>70</td>\n",
              "      <td>1.005</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>normal</td>\n",
              "      <td>abnormal</td>\n",
              "      <td>present</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>117</td>\n",
              "      <td>56</td>\n",
              "      <td>3.8</td>\n",
              "      <td>111</td>\n",
              "      <td>2.5</td>\n",
              "      <td>11.2</td>\n",
              "      <td>32</td>\n",
              "      <td>6700</td>\n",
              "      <td>3.9</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51</td>\n",
              "      <td>80</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>106</td>\n",
              "      <td>26</td>\n",
              "      <td>1.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.6</td>\n",
              "      <td>35</td>\n",
              "      <td>7300</td>\n",
              "      <td>4.6</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0   1      2  3  4       5         6   ...   18   19  20    21   22   23   24\n",
              "0  48  80  1.020  1  0     NaN    normal  ...  yes  yes  no  good   no   no  ckd\n",
              "1   7  50  1.020  4  0     NaN    normal  ...   no   no  no  good   no   no  ckd\n",
              "2  62  80  1.010  2  3  normal    normal  ...   no  yes  no  poor   no  yes  ckd\n",
              "3  48  70  1.005  4  0  normal  abnormal  ...  yes   no  no  poor  yes  yes  ckd\n",
              "4  51  80  1.010  2  0  normal    normal  ...   no   no  no  good   no   no  ckd\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYMJ3u42kqYx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cde1af1e-232e-4355-f6ea-bd81d0427aac"
      },
      "source": [
        "# Import modules\n",
        "import pandas as pd\n",
        "from sklearn_pandas import DataFrameMapper, CategoricalImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import xgboost as xgb\n",
        "\n",
        "# Create list of column names for kidney data: kidney_cols\n",
        "kidney_cols = ['age', 'bp', 'sg', 'al', 'su', 'rbc', 'pc', 'pcc', 'ba', 'bgr',\n",
        "               'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wc', 'rc', 'htn', 'dm',\n",
        "               'cad', 'appet', 'pe', 'ane', 'label']\n",
        "\n",
        "# Load dataset: df_kidney\n",
        "df_kidney = pd.read_csv('https://assets.datacamp.com/production/repositories/943/datasets/82c231cd41f92325cf33b78aaa360824e6b599b9/chronic_kidney_disease.csv'\n",
        ", names=kidney_cols, na_values='?')\n",
        "\n",
        "# Replace label values with 0 (ckd) and 1\n",
        "df_kidney['label'].replace({'ckd':0, 'notckd':1}, inplace=True)\n",
        "\n",
        "# Define X and y: X, y\n",
        "X, y = df_kidney.iloc[:, :-1], df_kidney['label'].values\n",
        "\n",
        "# Define new column order for X: col_order\n",
        "col_order = ['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot',\n",
        "             'hemo', 'pcv', 'wc', 'rc', 'rbc', 'pc', 'pcc', 'ba', 'htn', 'dm',\n",
        "             'cad', 'appet', 'pe', 'ane']\n",
        "\n",
        "# Rearrange columns of X\n",
        "X = X[col_order]\n",
        "\n",
        "# Create a boolean mask for categorical columns\n",
        "categorical_feature_mask = X.dtypes == object\n",
        "\n",
        "# Get a list of categorical column names\n",
        "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
        "\n",
        "# Get a list of non-categorical column names\n",
        "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
        "\n",
        "# Create empty list to hold column imputers: transformers\n",
        "transformers = []\n",
        "\n",
        "# Create numeric imputers and add to list of transformers\n",
        "transformers.extend([([num_col], [SimpleImputer(strategy='median'),\n",
        "                                                 StandardScaler()]) for num_col\n",
        "                    in non_categorical_columns])\n",
        "\n",
        "# Create categorical imputers and add to list of transformers\n",
        "transformers.extend([(cat_col, [CategoricalImputer()]) for cat_col in\n",
        "                    categorical_columns])\n",
        "\n",
        "# Use list of transformers to create a DataFrameMapper object\n",
        "numeric_categorical_union = DataFrameMapper(transformers, input_df=True,\n",
        "                                            df_out=True)\n",
        "\n",
        "# Define Dictifier class to turn df into dictionary as part of pipeline\n",
        "class Dictifier(BaseEstimator, TransformerMixin):       \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.to_dict('records')\n",
        "\n",
        "# Create full pipeline\n",
        "pipeline = Pipeline([('featureunion', numeric_categorical_union),\n",
        "                    ('dictifier', Dictifier()),\n",
        "                    ('vectorizer', DictVectorizer(sort=False)),\n",
        "                    ('clf', xgb.XGBClassifier(max_depth=3))])\n",
        "\n",
        "# Perform cross-validation\n",
        "cross_val_scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=3)\n",
        "\n",
        "print('ROC_AUC mean is: {}'.format(cross_val_scores.mean()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC_AUC is: 0.998637406769937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2p5XHFCcFlz"
      },
      "source": [
        "you'll be able to impute missing categorical values directly using the Categorical_Imputer() class in sklearn_pandas, and the DataFrameMapper() class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOMH3Bp2ckaA"
      },
      "source": [
        "Notice the keyword arguments input_df=True and df_out=True? This is so that you can work with DataFrames instead of arrays. By default, the transformers are passed a numpy array of the selected columns as input, and as a result, the output of the DataFrame mapper is also an array. Scikit-learn transformers have historically been designed to work with numpy arrays, not pandas DataFrames, even though their basic indexing interfaces are similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1wjkwF3ctOI"
      },
      "source": [
        "Having separately imputed numeric as well as categorical columns, your task is now to use scikit-learn's FeatureUnion to concatenate their results, which are contained in two separate transformer objects - numeric_imputation_mapper, and categorical_imputation_mapper, respectively.\n",
        "\n",
        "Just like with pipelines, you have to pass it a list of (string, transformer) tuples, where the first half of each tuple is the name of the transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZNskAZvnF9n"
      },
      "source": [
        "# Tuning XGBoost hyperparameters in a pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp_M5GWnrUhg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "cc4460c8-ae08-463b-804c-e0a2e4800101"
      },
      "source": [
        "# Import modules\n",
        "import pandas as pd\n",
        "from sklearn_pandas import DataFrameMapper, CategoricalImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import xgboost as xgb\n",
        "\n",
        "# Create list of column names for kidney data: kidney_cols\n",
        "kidney_cols = ['age', 'bp', 'sg', 'al', 'su', 'rbc', 'pc', 'pcc', 'ba', 'bgr',\n",
        "               'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wc', 'rc', 'htn', 'dm',\n",
        "               'cad', 'appet', 'pe', 'ane', 'label']\n",
        "\n",
        "# Load dataset: df_kidney\n",
        "df_kidney = pd.read_csv('https://assets.datacamp.com/production/repositories/943/datasets/82c231cd41f92325cf33b78aaa360824e6b599b9/chronic_kidney_disease.csv'\n",
        ", names=kidney_cols, na_values='?')\n",
        "\n",
        "# Replace label values with 0 (ckd) and 1\n",
        "df_kidney['label'].replace({'ckd':0, 'notckd':1}, inplace=True)\n",
        "\n",
        "# Define X and y: X, y\n",
        "X, y = df_kidney.iloc[:, :-1], df_kidney['label'].values\n",
        "\n",
        "# Define new column order for X: col_order\n",
        "col_order = ['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot',\n",
        "             'hemo', 'pcv', 'wc', 'rc', 'rbc', 'pc', 'pcc', 'ba', 'htn', 'dm',\n",
        "             'cad', 'appet', 'pe', 'ane']\n",
        "\n",
        "# Rearrange columns of X\n",
        "X = X[col_order]\n",
        "\n",
        "# Create a boolean mask for categorical columns\n",
        "categorical_feature_mask = X.dtypes == object\n",
        "\n",
        "# Get a list of categorical column names\n",
        "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
        "\n",
        "# Get a list of non-categorical column names\n",
        "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
        "\n",
        "# Create empty list to hold column imputers: transformers\n",
        "transformers = []\n",
        "\n",
        "# Create numeric imputers and add to list of transformers\n",
        "transformers.extend([([num_col], [SimpleImputer(strategy='median'),\n",
        "                                                 StandardScaler()]) for num_col\n",
        "                    in non_categorical_columns])\n",
        "\n",
        "# Create categorical imputers and add to list of transformers\n",
        "transformers.extend([(cat_col, [CategoricalImputer()]) for cat_col in\n",
        "                    categorical_columns])\n",
        "\n",
        "# Use list of transformers to create a DataFrameMapper object\n",
        "numeric_categorical_union = DataFrameMapper(transformers, input_df=True,\n",
        "                                            df_out=True)\n",
        "\n",
        "# Define Dictifier class to turn df into dictionary as part of pipeline\n",
        "class Dictifier(BaseEstimator, TransformerMixin):       \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.to_dict('records')\n",
        "\n",
        "# Create full pipeline\n",
        "pipeline = Pipeline([('featureunion', numeric_categorical_union),\n",
        "                    ('dictifier', Dictifier()),\n",
        "                    ('vectorizer', DictVectorizer(sort=False)),\n",
        "                    ('clf', xgb.XGBClassifier(max_depth=3))])\n",
        "\n",
        "# Create the parameter grid\n",
        "gbm_param_grid = {\n",
        "    'clf__learning_rate': np.arange(.05, 1, .05),\n",
        "    'clf__max_depth': np.arange(3,10, 1),\n",
        "    'clf__n_estimators': np.arange(50, 200, 50)\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,\n",
        "                                        param_distributions=gbm_param_grid,\n",
        "                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)\n",
        "\n",
        "# Fit the estimator\n",
        "randomized_roc_auc.fit(X, y)\n",
        "\n",
        "# Compute metrics\n",
        "print(randomized_roc_auc.best_score_)\n",
        "print(randomized_roc_auc.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9973866666666666\n",
            "Pipeline(memory=None,\n",
            "         steps=[('featureunion',\n",
            "                 DataFrameMapper(default=False, df_out=True,\n",
            "                                 features=[(['age'],\n",
            "                                            [SimpleImputer(add_indicator=False,\n",
            "                                                           copy=True,\n",
            "                                                           fill_value=None,\n",
            "                                                           missing_values=nan,\n",
            "                                                           strategy='median',\n",
            "                                                           verbose=0),\n",
            "                                             StandardScaler(copy=True,\n",
            "                                                            with_mean=True,\n",
            "                                                            with_std=True)]),\n",
            "                                           (['bp'],\n",
            "                                            [SimpleImputer(add_indicator=False,\n",
            "                                                           copy=True,\n",
            "                                                           fill_value=None,\n",
            "                                                           missing_va...\n",
            "                               colsample_bylevel=1, colsample_bynode=1,\n",
            "                               colsample_bytree=1, gamma=0,\n",
            "                               learning_rate=0.15000000000000002,\n",
            "                               max_delta_step=0, max_depth=6,\n",
            "                               min_child_weight=1, missing=None,\n",
            "                               n_estimators=150, n_jobs=1, nthread=None,\n",
            "                               objective='binary:logistic', random_state=0,\n",
            "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
            "                               seed=None, silent=None, subsample=1,\n",
            "                               verbosity=1))],\n",
            "         verbose=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjNj-hWnslQr"
      },
      "source": [
        "#  Not covered in the course and how you should proceed\n",
        "\n",
        "Using XGBoost for ranking/recommendation problems\n",
        "\n",
        "Using more sophisticated hyperparameter tuning strategies for tuning XGBoost models (Bayesian Optimization)\n",
        "\n",
        "Using XGBoost as part of an ensemble of other models for regression/classification. Even if XGBoost is an ensemble model itself it can be combined with others."
      ]
    }
  ]
}