{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperparameter tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMVXV7LTIj9IohoQUJVImMw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ferrariagustinpablo/Python-Machine-Learning-notebooks/blob/main/Hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uukJrH9aDRs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "8a6506e0-94a9-4830-e7e2-b480738bef43"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://assets.datacamp.com/production/repositories/3983/datasets/bb158f1c76682286f938e02d71de21a3e5389cbf/credit-card-full.csv')\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 25)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>LIMIT_BAL</th>\n",
              "      <th>SEX</th>\n",
              "      <th>EDUCATION</th>\n",
              "      <th>MARRIAGE</th>\n",
              "      <th>AGE</th>\n",
              "      <th>PAY_0</th>\n",
              "      <th>PAY_2</th>\n",
              "      <th>PAY_3</th>\n",
              "      <th>PAY_4</th>\n",
              "      <th>PAY_5</th>\n",
              "      <th>PAY_6</th>\n",
              "      <th>BILL_AMT1</th>\n",
              "      <th>BILL_AMT2</th>\n",
              "      <th>BILL_AMT3</th>\n",
              "      <th>BILL_AMT4</th>\n",
              "      <th>BILL_AMT5</th>\n",
              "      <th>BILL_AMT6</th>\n",
              "      <th>PAY_AMT1</th>\n",
              "      <th>PAY_AMT2</th>\n",
              "      <th>PAY_AMT3</th>\n",
              "      <th>PAY_AMT4</th>\n",
              "      <th>PAY_AMT5</th>\n",
              "      <th>PAY_AMT6</th>\n",
              "      <th>default payment next month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>20000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>-2</td>\n",
              "      <td>3913</td>\n",
              "      <td>3102</td>\n",
              "      <td>689</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>689</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>120000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>26</td>\n",
              "      <td>-1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2682</td>\n",
              "      <td>1725</td>\n",
              "      <td>2682</td>\n",
              "      <td>3272</td>\n",
              "      <td>3455</td>\n",
              "      <td>3261</td>\n",
              "      <td>0</td>\n",
              "      <td>1000</td>\n",
              "      <td>1000</td>\n",
              "      <td>1000</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>90000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29239</td>\n",
              "      <td>14027</td>\n",
              "      <td>13559</td>\n",
              "      <td>14331</td>\n",
              "      <td>14948</td>\n",
              "      <td>15549</td>\n",
              "      <td>1518</td>\n",
              "      <td>1500</td>\n",
              "      <td>1000</td>\n",
              "      <td>1000</td>\n",
              "      <td>1000</td>\n",
              "      <td>5000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>50000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46990</td>\n",
              "      <td>48233</td>\n",
              "      <td>49291</td>\n",
              "      <td>28314</td>\n",
              "      <td>28959</td>\n",
              "      <td>29547</td>\n",
              "      <td>2000</td>\n",
              "      <td>2019</td>\n",
              "      <td>1200</td>\n",
              "      <td>1100</td>\n",
              "      <td>1069</td>\n",
              "      <td>1000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>50000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>57</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8617</td>\n",
              "      <td>5670</td>\n",
              "      <td>35835</td>\n",
              "      <td>20940</td>\n",
              "      <td>19146</td>\n",
              "      <td>19131</td>\n",
              "      <td>2000</td>\n",
              "      <td>36681</td>\n",
              "      <td>10000</td>\n",
              "      <td>9000</td>\n",
              "      <td>689</td>\n",
              "      <td>679</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  LIMIT_BAL  SEX  ...  PAY_AMT5  PAY_AMT6  default payment next month\n",
              "0   1      20000    2  ...         0         0                           1\n",
              "1   2     120000    2  ...         0      2000                           1\n",
              "2   3      90000    2  ...      1000      5000                           0\n",
              "3   4      50000    2  ...      1069      1000                           0\n",
              "4   5      50000    1  ...       689       679                           0\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35yG-lIaeWOP"
      },
      "source": [
        "# What is Hyperparameter tuning\n",
        "\n",
        "Somthing you set before the the modelling process.\n",
        "\n",
        "The algorithm does not learn the value of hyperparameters.\n",
        "\n",
        "Some hyperparameters are more important than others. There are some generally accepted important hyperparameters to tune.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpRqJRBFfo81"
      },
      "source": [
        "## Random Forest\n",
        "\n",
        "The most import hyperparameters on a Random Forest:\n",
        "\n",
        "n_estimators (should be set to a high value)\n",
        "\n",
        "max_features (how many features to consider when splitting, vital to ensure tree diversity)\n",
        "\n",
        "max_depth & min_sample_leaf (important to control overfitting)\n",
        "\n",
        "criterion: not a generally a primary hyperparameter to consider."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufcSQn7jhDle"
      },
      "source": [
        "Understanding what hyperparameters are available and the impact of different hyperparameters is a core skill for any data scientist. As models become more complex, there are many different settings you can set, but only some will have a large impact on your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytSPIIYEhIdM"
      },
      "source": [
        "\n",
        "<script.py> output:\n",
        "    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "                max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                min_samples_leaf=1, min_samples_split=2,\n",
        "                min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=None,\n",
        "                oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwzKTJ2Ihy4P"
      },
      "source": [
        "##  Hyperparameters of KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n769r0q6h9vf"
      },
      "source": [
        "The k-nearest-neighbors algorithm is not as popular as it used to be but can still be an excellent choice for data that has groups of data that behave similarly. Could this be the case for our credit card users?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HgdE7YEjQIP"
      },
      "source": [
        "## LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T75Xp5jTjTs5"
      },
      "source": [
        "For example in logistic regression parameter solver gets in conflictor with penalty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_f3NS7Mlx41"
      },
      "source": [
        "# GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQGTnfs1myIo"
      },
      "source": [
        "Scikit-learn GridSearchCV\n",
        "\n",
        "Steps in GridSearchCV:\n",
        "\n",
        "1- An algorithm to tune the hyperparameters (estimator)\n",
        "2- Defining which hyperparameters we will tune.\n",
        "3- Defining a range of values for each hyperparameter\n",
        "4- Setting a cross-validation scheme\n",
        "5- Define a score function so we can decide which square on our grid is the 'best'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPlCfTmLoLQ4"
      },
      "source": [
        "\n",
        "Important arguments in GridSearchCV\n",
        "\n",
        "1- estimator: KNN, Logregression, GBM, RandomForest, and others\n",
        "\n",
        "2- param_grid: dictionary of hyperparameter names and list of values. (The keys must be valid hyperparameters)\n",
        "\n",
        "3- cv: provide an integer for cross-validation. Typically 5 or 10\n",
        "\n",
        "4- scoring: Which score to use to choose best grid squares. Sci-kit learn metrics module. \n",
        "\n",
        "5- refit: True means that it fits the best hyperparameters to the training data. This means that the GridSearchCV object transforms into an estimator where it can be used for prediction with .predict(). Really handy.\n",
        "\n",
        "6- n_jobs: Using different CPUs\n",
        "\n",
        "7- return_train_score: Logs statistics about the training runs. **Useful for analyzing bias-variance trade-off but adds computational expense.** Super informative but computational expensive and would not assist in finding the model, onnly for information. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEg0kpaMgpPZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3b72605e-408e-46b1-a4c9-f448e3dff4f8"
      },
      "source": [
        "# All scoring metrics from scikit-learn\n",
        "from sklearn import metrics\n",
        "print(sorted(metrics.SCORERS.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted', 'max_error', 'mutual_info_score', 'neg_brier_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_gamma_deviance', 'neg_mean_poisson_deviance', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'neg_root_mean_squared_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'roc_auc_ovo', 'roc_auc_ovo_weighted', 'roc_auc_ovr', 'roc_auc_ovr_weighted', 'v_measure_score']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ7MYFDFqS9k"
      },
      "source": [
        "# Create a Random Forest Classifier with specified criterion\n",
        "rf_class = RandomForestClassifier(criterion='entropy')\n",
        "\n",
        "# Create the parameter grid\n",
        "param_grid = {'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_rf_class=GridSearchCV(\n",
        "    estimator=rf_class,\n",
        "    param_grid=param_grid,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=1,\n",
        "    cv=5,\n",
        "    refit=True, return_train_score=True)\n",
        "print(grid_rf_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IFcB-PErBNa"
      },
      "source": [
        "## GridSearchCV outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6BaBv5JrDfz"
      },
      "source": [
        "### Results log cv_results_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjXi9m-zra1Y"
      },
      "source": [
        "Dictionary that we can read into a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vcgiGURrZkQ"
      },
      "source": [
        "cv_results = pd.DataFrame(grid_rf_class.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFoyxBcirMvF"
      },
      "source": [
        "### best results: best_index_, best_params_ and best_score_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwCs2PSgsRXB"
      },
      "source": [
        "best_index_: the row in our grid that was the best.\n",
        "\n",
        "best_params_: the dictionary of parameters that gave the best score.\n",
        "\n",
        "best_score_: the actual best score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGC6r3VwwOEF"
      },
      "source": [
        "# Print out the ROC_AUC score from the best-performing square\n",
        "best_score = grid_rf_class.best_score_\n",
        "print(best_score)\n",
        "\n",
        "# Create a variable from the row related to the best-performing square\n",
        "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
        "best_row = cv_results_df.loc[[grid_rf_class.best_index_]]\n",
        "print(best_row)\n",
        "\n",
        "# Get the n_estimators parameter from the best-performing square and print\n",
        "best_n_estimators = grid_rf_class.best_params_[\"n_estimators\"]\n",
        "print(best_n_estimators)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UBKQjrOxOgg"
      },
      "source": [
        "Using the best_estimator_ to predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jry1SJ-cxM4h"
      },
      "source": [
        "# See what type of object the best_estimator_ property is\n",
        "print(type(grid_rf_class.best_estimator_))\n",
        "\n",
        "# Create an array of predictions directly using the best_estimator_ property\n",
        "predictions = grid_rf_class.best_estimator_.predict(X_test)\n",
        "\n",
        "# Take a look to confirm it worked, this should be an array of 1's and 0's\n",
        "print(predictions[0:5])\n",
        "\n",
        "# Now create a confusion matrix \n",
        "print(\"Confusion Matrix \\n\", confusion_matrix(y_test, predictions))\n",
        "\n",
        "# Get the ROC-AUC score \n",
        "# Be careful. Use predict \n",
        "predictions_proba = grid_rf_class.best_estimator_.predict_proba(X_test)[:,1]\n",
        "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test, predictions_proba))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iBkKe-UrQ9A"
      },
      "source": [
        "### Extra information: scorer_, n_splits_ and refit_time_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VrINoIxyj7M"
      },
      "source": [
        "# RandomSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKFR2iePyqEu"
      },
      "source": [
        "When it comes to undertaking the search rather than trying every single combination we randomly sample N combinations and try these out.\n",
        "\n",
        "Why doing Random Search? Paper of Bengio & Bergstra (2012), shows empirically and theoretically that randomly chosen trials are more efficient for hyperparameter optimization than trials on a grid.\n",
        "\n",
        "Two main reasons:\n",
        "\n",
        "1- Not every hyperparameter is as important\n",
        "\n",
        "2- Probability. How many trials to have a 95% confidence of getting the region of best parameters. The probability of missing is (1-0.05)^n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c45crnkZ05hn"
      },
      "source": [
        "## Additionally we can create a random sample of hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xwzXyF31ep0"
      },
      "source": [
        "To undertake a random search, we firstly need to undertake a random sampling of our hyperparameter space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEPn2lgd1mii"
      },
      "source": [
        " firstly create some lists of hyperparameters that can be zipped up to a list of lists. Then you will randomly sample hyperparameter combinations preparation for running a random search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hh5Yhd-1RKu"
      },
      "source": [
        "from itertools import product\n",
        "\n",
        "# Create a list of values for the learning_rate hyperparameter\n",
        "learn_rate_list = list(np.linspace(0.01,1.5,200))\n",
        "\n",
        "# Create a list of values for the min_samples_leaf hyperparameter\n",
        "min_samples_list = list(range(10,41))\n",
        "\n",
        "# Combination list\n",
        "# product from itertools\n",
        "combinations_list = [list(x) for x in product(learn_rate_list, min_samples_list)]\n",
        "\n",
        "# Sample hyperparameter combinations for a random search.\n",
        "combinations_random_chosen = random.sample(combinations_list, 250)\n",
        "\n",
        "# Print the result\n",
        "print(combinations_random_chosen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HQKcSgeymR0"
      },
      "source": [
        "# Create lists for criterion and max_features\n",
        "criterion_list = [\"gini\", \"entropy\"]\n",
        "max_feature_list = [\"auto\", \"sqrt\", \"log2\", None]\n",
        "\n",
        "# Create a list of values for the max_depth hyperparameter\n",
        "max_depth_list = list(range(3,56))\n",
        "\n",
        "# Combination list\n",
        "combinations_list = [list(x) for x in product(criterion_list, max_feature_list, max_depth_list)]\n",
        "\n",
        "# Sample hyperparameter combinations for a random search\n",
        "combinations_random_chosen = random.sample(combinations_list, 150)\n",
        "\n",
        "# Print the result\n",
        "print(combinations_random_chosen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMpKGeGwejYi"
      },
      "source": [
        "## RandomSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SWJfK9eenho"
      },
      "source": [
        "Only one different when undertaking random seatch \n",
        "\n",
        "1- Decide how many samples to take. n_iter.\n",
        "\n",
        "\n",
        "And in arguments:\n",
        "param_distributions if you want to tell computer how to sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGQxbOQQex-P"
      },
      "source": [
        "# Create the parameter grid\n",
        "param_grid = {'learning_rate': np.linspace(0.1, 2, 150), 'min_samples_leaf': list(range(20, 65))} \n",
        "\n",
        "# Create a random search object\n",
        "random_GBM_class = RandomizedSearchCV(\n",
        "    estimator = GradientBoostingClassifier(),\n",
        "    param_distributions = param_grid,\n",
        "    n_iter = 10,\n",
        "    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)\n",
        "\n",
        "# Fit to the training data\n",
        "random_GBM_class.fit(X_train, y_train)\n",
        "\n",
        "# Print the values used for both hyperparameters\n",
        "print(random_GBM_class.cv_results_['param_learning_rate'])\n",
        "print(random_GBM_class.cv_results_['param_min_samples_leaf'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GMh-b3lf5dc"
      },
      "source": [
        "# Create the parameter grid\n",
        "param_grid = {'max_depth': list(range(5,26)), 'max_features': ['auto' , 'sqrt']} \n",
        "\n",
        "# Create a random search object\n",
        "random_rf_class = RandomizedSearchCV(\n",
        "    estimator = RandomForestClassifier(n_estimators=80),\n",
        "    param_distributions = param_grid, n_iter = 5,\n",
        "    scoring='roc_auc', n_jobs=4, cv = 3, refit=True, return_train_score = True)\n",
        "\n",
        "# Fit to the training data\n",
        "random_rf_class.fit(X_train, y_train)\n",
        "\n",
        "# Print the values used for both hyperparameters\n",
        "print(random_rf_class.cv_results_['param_max_depth'])\n",
        "print(random_rf_class.cv_results_['param_max_features'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdW9cKMyg3B4"
      },
      "source": [
        "If you want the absolute best --> GridSearchCV\n",
        "\n",
        "If lot of data, or many hyperparameters to tune and less resources and time --> RandomSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfcSM_j1hsbK"
      },
      "source": [
        "# Informed Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bk7PxSihvOE"
      },
      "source": [
        "Hyperparameter tuning techniques know as informed search\n",
        "\n",
        "Grid and randomSearch are uninformed search. Each iteration does not learn from the previous iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXFnUTh8lAxW"
      },
      "source": [
        "## **Coarse to fine tuning** Is a technique of informed search:\n",
        "1- Undertake a RandomSearch.\n",
        "2- Then fin promising areas\n",
        "3- Grid search in the smaller area.\n",
        "4- Continue until optimal score obtained.\n",
        "\n",
        "**Uses the advantages of both methods**\n",
        "\n",
        "Uses batches in iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX3S5arbk899"
      },
      "source": [
        "## Bayesian Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHUScnqWlDlQ"
      },
      "source": [
        "Important statistical concept.\n",
        "\n",
        "Statistical method of using new evidence to iteratively update our beliefs about some outcome.\n",
        "\n",
        "Bayesian hyperparameter tuning is very new but quite popular for larger and more complex hyperparameter tuning tasks as they work well to find optimal hyperparameter combinations in these situations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wclr6eTCnRlI"
      },
      "source": [
        "Many options to set the grid of parameters:\n",
        "\n",
        "Simple numbers, choose from a list or distribution of values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_SuIcFHnpSu"
      },
      "source": [
        "### hyperopt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69WXyAy0pr_l"
      },
      "source": [
        "we will use hp.quniform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAvEM3FxnEcA"
      },
      "source": [
        "from hyperopt import fmin, hp\n",
        "\n",
        "# Set up space dictionary with specified hyperparameters\n",
        "space = {'max_depth': hp.quniform('max_depth', 2, 10, 2),'learning_rate': hp.uniform('learning_rate', 0.001, 0.9)}\n",
        "\n",
        "# Set up objective function\n",
        "def objective(params):\n",
        "    params = {'max_depth': int(params['max_depth']),'learning_rate': params['learning_rate']}\n",
        "    gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) \n",
        "    best_score = cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=2, n_jobs=4).mean()\n",
        "    loss = 1 - best_score\n",
        "    return loss\n",
        "\n",
        "# Run the algorithm\n",
        "best = fmin(fn=objective,space=space, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest)\n",
        "print(best)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8tqYVecqQk2"
      },
      "source": [
        "This will be a very powerful tool for your machine learning modeling in future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzQA2UmfqXXr"
      },
      "source": [
        "## Genetic Algorithms like TPOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I51gASMIyL55"
      },
      "source": [
        "https://www.datacamp.com/community/tutorials/tpot-machine-learning-python\n",
        "\n",
        "Complete tutorial on datacamp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hZmmmBEtK_O"
      },
      "source": [
        "Another informed search methodology.\n",
        "\n",
        "1- We can create some models\n",
        "2- We pick the best by our scoring function. \n",
        "3- We can create new models that are similar to the best ones. \n",
        "4- We add some randomness so we don't reach a local optimum.\n",
        "5- Repeat until we are happy!\n",
        "\n",
        "Number of advantages.\n",
        "1- Allows to learn from the previous iteration like bayesian. \n",
        "2- It has additional advantage of randomness.\n",
        "\n",
        "** It doesn't require hyperparameter choice!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_e9HAHYuYrx"
      },
      "source": [
        "https://towardsdatascience.com/tpot-automated-machine-learning-in-python-e56800e69c11\n",
        "\n",
        "Consider TPOT your Data Science Assistant. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.\n",
        "\n",
        "In a way, TPOT is your data science assistant. It automates the “boring” stuff and leaves you with the tasks of data gathering and preparation. I know this segment isn’t fun to some of you — but it’s 80% of the job. With libraries like TPOT, that percentage might only get higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDC11cj9vFYr"
      },
      "source": [
        "http://geneticprogramming.com\n",
        "\n",
        "https://epistasislab.github.io/tpot/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dia6qsk_vT7-"
      },
      "source": [
        "It returns the python code of the pipeline for you!!!!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC7HhaLgvegV"
      },
      "source": [
        "The key arguments to a TPOT classifier are:\n",
        "\n",
        "1- Generation: iteration to run training for. Number of cycles.\n",
        "2- Population_size: how many model we keeps\n",
        "3- offspring_size: number of models to produce in each iteration\n",
        "4- mutation_rate: The proportion of pipelines to apply randomness to.\n",
        "5- cv: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixXYmaUoqbWf"
      },
      "source": [
        "from tpot import TPOTClassifier\n",
        "\n",
        "# Assign the values outlined to the inputs\n",
        "number_generations = 3\n",
        "population_size = 4\n",
        "offspring_size = 3\n",
        "scoring_function = 'accuracy'\n",
        "\n",
        "# Create the tpot classifier\n",
        "tpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size, \n",
        "                          offspring_size=offspring_size, scoring=scoring_function,\n",
        "                          verbosity=2, random_state=2, cv=2)\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "tpot_clf.fit(X_train, y_train)\n",
        "\n",
        "# Score on the test set\n",
        "print(tpot_clf.score(X_test, y_test))\n",
        "\n",
        "\n",
        "tpot_clf.fitted_pipeline_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhJLOmkWxgr4"
      },
      "source": [
        "TPOT is quite unstable when not run for a reasonable amount of time.\n",
        "\n",
        "quite unstable when only running with low generations, population size and offspring like in the code.\n",
        "\n",
        "Increasing the generations, population size and offspring and running this for a long time will assist to produce better models and more stable results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEg-IJy5yHXg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "fb2a03fb-c901-475b-e064-d3aeb594b35a"
      },
      "source": [
        "pip install tpot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tpot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/5e/cb87b0257033a7a396e533a634079ee151a239d180efe2a8b1d2e3584d23/TPOT-0.11.5-py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.6/dist-packages (from tpot) (0.16.0)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from tpot) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from tpot) (0.22.2.post1)\n",
            "Collecting stopit>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/35/58/e8bb0b0fb05baf07bbac1450c447d753da65f9701f551dca79823ce15d50/stopit-1.1.2.tar.gz\n",
            "Collecting update-checker>=0.16\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.6/dist-packages (from tpot) (1.1.2)\n",
            "Collecting deap>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/eb/2bd0a32e3ce757fb26264765abbaedd6d4d3640d90219a513aeabd08ee2b/deap-1.3.1-cp36-cp36m-manylinux2010_x86_64.whl (157kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.6/dist-packages (from tpot) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from tpot) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.16->tpot) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->tpot) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->tpot) (2.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->tpot) (1.15.0)\n",
            "Building wheels for collected packages: stopit\n",
            "  Building wheel for stopit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stopit: filename=stopit-1.1.2-cp36-none-any.whl size=11956 sha256=6cb986dddcbdd899f74a33e0642a0e809ca11444146d9641733d4aa664ce0b01\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/85/2b/2580190404636bfc63e8de3dff629c03bb795021e1983a6cc7\n",
            "Successfully built stopit\n",
            "Installing collected packages: stopit, update-checker, deap, tpot\n",
            "Successfully installed deap-1.3.1 stopit-1.1.2 tpot-0.11.5 update-checker-0.18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwbcTi9Rx8nH"
      },
      "source": [
        "from tpot import TPOTClassifier\n",
        "\n",
        "TPOTClassifier?"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}