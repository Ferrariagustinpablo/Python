{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Engineering for NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMpFGItiaBt7XIyFyCnGxyJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ferrariagustinpablo/Python-Machine-Learning-notebooks/blob/main/Feature_Engineering_for_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t0lZG7ByVb4"
      },
      "source": [
        "# Text preprocessing\n",
        "\n",
        "Converting to lowercase, and converting to base-form\n",
        "\n",
        "From text through vectorization to numerical 0 and 1s\n",
        "\n",
        "Basic features for NLP: number of words, number of characters, average length words.\n",
        "\n",
        "POS taggins will label each words with its corresponding part of speech: that is I(pronoun) have(verb) a(article) dog(noun)\n",
        "\n",
        "NER Names entity recognition. A noun is refering to a person, country or organization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqwvE9Bx3gni"
      },
      "source": [
        "## Compute number of characters for each string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgU5iVvoySWW"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Compute the number of characters for each tweet\n",
        "\n",
        "# Create a feature char_count\n",
        "tweets['char_count'] = tweets['content'].apply(len)\n",
        "\n",
        "# Print the average character count\n",
        "print(tweets['char_count'].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivl7zn3I3k3E"
      },
      "source": [
        "## Count the length of each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O21ZfL5_3fEm"
      },
      "source": [
        "# Function that returns number of words in a string\n",
        "def count_words(string):\n",
        "\t# Split the string into words\n",
        "    words = string.split()\n",
        "    \n",
        "    # Return the number of words\n",
        "    return len(words)\n",
        "\n",
        "# Create a new feature word_count\n",
        "ted['word_count'] = ted['transcript'].apply(count_words)\n",
        "\n",
        "# Print the average word count of the talks\n",
        "print(ted['word_count'].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAXEHZgT6OWa"
      },
      "source": [
        "## Check number of hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIo0JfZg6N3C"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function that returns numner of hashtags in a string\n",
        "def count_hashtags(string):\n",
        "\t# Split the string into words\n",
        "    words = string.split()\n",
        "    \n",
        "    # Create a list of words that are hashtags\n",
        "    hashtags = [word for word in words if word.startswith(\"#\")]\n",
        "    \n",
        "    # Return number of hashtags\n",
        "    return(len(hashtags))\n",
        "\n",
        "# Create a feature hashtag_count and display distribution\n",
        "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
        "tweets['hashtag_count'].hist()\n",
        "plt.title('Hashtag count distribution')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqgWmmJZ6xHE"
      },
      "source": [
        "## Count number of words with @"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y-aeriX6wn3"
      },
      "source": [
        "# Function that returns number of mentions in a string\n",
        "def count_mentions(string):\n",
        "\t# Split the string into words\n",
        "    words = string.split()\n",
        "    \n",
        "    # Create a list of words that are mentions\n",
        "    mentions = [word for word in words if word.startswith('@')]\n",
        "    \n",
        "    # Return number of mentions\n",
        "    return(len(mentions))\n",
        "\n",
        "# Create a feature mention_count and display distribution\n",
        "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
        "tweets['mention_count'].hist()\n",
        "plt.title('Mention count distribution')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mWhFrF97GUB"
      },
      "source": [
        "# Readability tests\n",
        "\n",
        "Determine readability of an English passage\n",
        "\n",
        "Scale ranging from primary school up to college graduate level\n",
        "\n",
        "Also used in fake news and opinion spam detection\n",
        "\n",
        "Dependent on two factors: Greater the average sentence length the harder the text is to read. Flesch reading ease score. The higher the score greater the readability\n",
        "\n",
        "Gunning fog index: developed in 1954, also dependent on average sentence length. Greater the percentage of complex words, harder the text is to read. Complex words 3 or more sillables.\n",
        "\n",
        "Use tstatistic library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V7xsPd99cFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "7fb124a1-c828-4592-9787-42964f0f42b1"
      },
      "source": [
        "pip install textatistic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textatistic\n",
            "  Downloading https://files.pythonhosted.org/packages/73/f8/74dade1df8998ce9a42cba21b3cdf284f3f273e7e22fbcab27955d213e61/textatistic-0.0.1.tar.gz\n",
            "Collecting pyhyphen>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/e7/97ff5a575bd0744c67b28316fccbbcf180806bb89424df94fca9c1f6ea8c/PyHyphen-3.0.1.tar.gz\n",
            "Collecting appdirs\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pyhyphen>=2.0.5->textatistic) (1.15.0)\n",
            "Building wheels for collected packages: textatistic, pyhyphen\n",
            "  Building wheel for textatistic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for textatistic: filename=textatistic-0.0.1-cp36-none-any.whl size=29056 sha256=e45b4fba11351e2c3d6c937d7796eb7f575c60bc6b74492209492da5b1f2d529\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/ec/34/69c3cae349149cd91552c4c470efcbd08bbd21ba30b12e08ab\n",
            "  Building wheel for pyhyphen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyhyphen: filename=PyHyphen-3.0.1-cp36-cp36m-linux_x86_64.whl size=57909 sha256=f2292322e4e5331d261ec89149c80ce0ae7ff91ef8358dfbe68b5fe3e9e2f28b\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/46/93/46c556b5f054568b7470c86c4f76ea628a9a8bdf5a355b9c63\n",
            "Successfully built textatistic pyhyphen\n",
            "Installing collected packages: appdirs, pyhyphen, textatistic\n",
            "Successfully installed appdirs-1.4.4 pyhyphen-3.0.1 textatistic-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUsr2uCA8aMx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9362967-3601-47d8-b4fe-51e82455d3f5"
      },
      "source": [
        "sisyphus_essay = '\\nThe gods had condemned Sisyphus to ceaselessly rolling a rock to the top of a mountain, whence the stone would fall back of its own weight. They had thought with some reason that there is no more dreadful punishment than futile and hopeless labor. If one believes Homer, Sisyphus was the wisest and most prudent of mortals. According to another tradition, however, he was disposed to practice the profession of highwayman. I see no contradiction in this. Opinions differ as to the reasons why he became the futile laborer of the underworld. To begin with, he is accused of a certain levity in regard to the gods. He stole their secrets. Egina, the daughter of Esopus, was carried off by Jupiter. The father was shocked by that disappearance and complained to Sisyphus. He, who knew of the abduction, offered to tell about it on condition that Esopus would give water to the citadel of Corinth. To the celestial thunderbolts he preferred the benediction of water. He was punished for this in the underworld. Homer tells us also that Sisyphus had put Death in chains. Pluto could not endure the sight of his deserted, silent empire. He dispatched the god of war, who liberated Death from the hands of her conqueror. It is said that Sisyphus, being near to death, rashly wanted to test his wife\\'s love. He ordered her to cast his unburied body into the middle of the public square. Sisyphus woke up in the underworld. And there, annoyed by an obedience so contrary to human love, he obtained from Pluto permission to return to earth in order to chastise his wife. But when he had seen again the face of this world, enjoyed water and sun, warm stones and the sea, he no longer wanted to go back to the infernal darkness. Recalls, signs of anger, warnings were of no avail. Many years more he lived facing the curve of the gulf, the sparkling sea, and the smiles of earth. A decree of the gods was necessary. Mercury came and seized the impudent man by the collar and, snatching him from his joys, lead him forcibly back to the underworld, where his rock was ready for him. You have already grasped that Sisyphus is the absurd hero. He is, as much through his passions as through his torture. His scorn of the gods, his hatred of death, and his passion for life won him that unspeakable penalty in which the whole being is exerted toward accomplishing nothing. This is the price that must be paid for the passions of this earth. Nothing is told us about Sisyphus in the underworld. Myths are made for the imagination to breathe life into them. As for this myth, one sees merely the whole effort of a body straining to raise the huge stone, to roll it, and push it up a slope a hundred times over; one sees the face screwed up, the cheek tight against the stone, the shoulder bracing the clay-covered mass, the foot wedging it, the fresh start with arms outstretched, the wholly human security of two earth-clotted hands. At the very end of his long effort measured by skyless space and time without depth, the purpose is achieved. Then Sisyphus watches the stone rush down in a few moments toward tlower world whence he will have to push it up again toward the summit. He goes back down to the plain. It is during that return, that pause, that Sisyphus interests me. A face that toils so close to stones is already stone itself! I see that man going back down with a heavy yet measured step toward the torment of which he will never know the end. That hour like a breathing-space which returns as surely as his suffering, that is the hour of consciousness. At each of those moments when he leaves the heights and gradually sinks toward the lairs of the gods, he is superior to his fate. He is stronger than his rock. If this myth is tragic, that is because its hero is conscious. Where would his torture be, indeed, if at every step the hope of succeeding upheld him? The workman of today works everyday in his life at the same tasks, and his fate is no less absurd. But it is tragic only at the rare moments when it becomes conscious. Sisyphus, proletarian of the gods, powerless and rebellious, knows the whole extent of his wretched condition: it is what he thinks of during his descent. The lucidity that was to constitute his torture at the same time crowns his victory. There is no fate that can not be surmounted by scorn. If the descent is thus sometimes performed in sorrow, it can also take place in joy. This word is not too much. Again I fancy Sisyphus returning toward his rock, and the sorrow was in the beginning. When the images of earth cling too tightly to memory, when the call of happiness becomes too insistent, it happens that melancholy arises in man\\'s heart: this is the rock\\'s victory, this is the rock itself. The boundless grief is too heavy to bear. These are our nights of Gethsemane. But crushing truths perish from being acknowledged. Thus, Edipus at the outset obeys fate without knowing it. But from the moment he knows, his tragedy begins. Yet at the same moment, blind and desperate, he realizes that the only bond linking him to the world is the cool hand of a girl. Then a tremendous remark rings out: \"Despite so many ordeals, my advanced age and the nobility of my soul make me conclude that all is well.\" Sophocles\\' Edipus, like Dostoevsky\\'s Kirilov, thus gives the recipe for the absurd victory. Ancient wisdom confirms modern heroism. One does not discover the absurd without being tempted to write a manual of happiness. \"What!---by such narrow ways--?\" There is but one world, however. Happiness and the absurd are two sons of the same earth. They are inseparable. It would be a mistake to say that happiness necessarily springs from the absurd. Discovery. It happens as well that the felling of the absurd springs from happiness. \"I conclude that all is well,\" says Edipus, and that remark is sacred. It echoes in the wild and limited universe of man. It teaches that all is not, has not been, exhausted. It drives out of this world a god who had come into it with dissatisfaction and a preference for futile suffering. It makes of fate a human matter, which must be settled among men. All Sisyphus\\' silent joy is contained therein. His fate belongs to him. His rock is a thing. Likewise, the absurd man, when he contemplates his torment, silences all the idols. In the universe suddenly restored to its silence, the myriad wondering little voices of the earth rise up. Unconscious, secret calls, invitations from all the faces, they are the necessary reverse and price of victory. There is no sun without shadow, and it is essential to know the night. The absurd man says yes and his efforts will henceforth be unceasing. If there is a personal fate, there is no higher destiny, or at least there is, but one which he concludes is inevitable and despicable. For the rest, he knows himself to be the master of his days. At that subtle moment when man glances backward over his life, Sisyphus returning toward his rock, in that slight pivoting he contemplates that series of unrelated actions which become his fate, created by him, combined under his memory\\'s eye and soon sealed by his death. Thus, convinced of the wholly human origin of all that is human, a blind man eager to see who knows that the night has no end, he is still on the go. The rock is still rolling. I leave Sisyphus at the foot of the mountain! One always finds one\\'s burden again. But Sisyphus teaches the higher fidelity that negates the gods and raises rocks. He too concludes that all is well. This universe henceforth without a master seems to him neither sterile nor futile. Each atom of that stone, each mineral flake of that night filled mountain, in itself forms a world. The struggle itself toward the heights is enough to fill a man\\'s heart. One must imagine Sisyphus happy.\\n'\n",
        "\n",
        "# Import Textatistic\n",
        "from textatistic import Textatistic\n",
        "\n",
        "# Compute the readability scores \n",
        "readability_scores = Textatistic(sisyphus_essay).scores\n",
        "\n",
        "# Print the flesch reading ease score\n",
        "flesch = readability_scores['flesch_score']\n",
        "print(\"The Flesch Reading Ease is %.2f\" % (flesch))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Flesch Reading Ease is 81.67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qmwsosf-RZl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "014d1837-eb1a-4122-c497-de3fa403927e"
      },
      "source": [
        "readability_scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dalechall_score': 7.487433762517882,\n",
              " 'flesch_score': 81.67466335836913,\n",
              " 'fleschkincaid_score': 5.485083154506439,\n",
              " 'gunningfog_score': 7.913698140200286,\n",
              " 'smog_score': 8.110721755262034}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9mSoWJDpbvB"
      },
      "source": [
        "# 1- Tokenization and Lemmatization SpaCy\n",
        "\n",
        "Tokenizing a string to words. It also means Don't into Do and n't"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqQw1HtYqGhP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b898a1a5-69db-4bd8-8ea8-6a5b5e9ccece"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "string = \"HellO! I don't know what I'm doing here.\"\n",
        "\n",
        "doc = nlp(string)\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['HellO', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', \"'m\", 'doing', 'here', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuZoJVvBqnPO"
      },
      "source": [
        "Lemmatization means converting a words **into its base form**\n",
        "\n",
        "Powerful process of standardization.\n",
        "\n",
        "reducing, reduces, reduced, reduction --> reduce\n",
        "\n",
        "am,are,is --> be\n",
        "\n",
        "n't --> not\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew1PG_VVq7GZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d101dec-e258-4548-9169-094b96492cda"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "string = \"HellO! I don't know what I'm doing here.\"\n",
        "\n",
        "doc = nlp(string)\n",
        "\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(lemmas)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', '!', '-PRON-', 'do', 'not', 'know', 'what', '-PRON-', 'be', 'do', 'here', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTz22czcskQw"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVolUCNur0Nk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8f0587d3-8242-4261-fa66-bdd56609a357"
      },
      "source": [
        "gettysburg = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\"\n",
        "\n",
        "#Tokenizing\n",
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(gettysburg)\n",
        "\n",
        "# Generate the tokens\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', 'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', 'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', 'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', 'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and', 'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', 'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', 'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', 'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwmWsu0MslSc"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SeAiJvJsWqb"
      },
      "source": [
        "# Lemmatization\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(gettysburg)\n",
        "\n",
        "# Generate lemmas\n",
        "lemmas = [token.lemma_ for token in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_QZY0f-tNid"
      },
      "source": [
        "# 2- Text cleaning (after lemmas)\n",
        "\n",
        "every python string has an isalpha() method that returns true if all the characters of the string are alphabets. Extremely convenient methods to remove all (lemmatized) tokens that are or contain numbers, punctuation and emojis.\n",
        "\n",
        "Be carefull about U.S.A, U.K, word2vec and other words like this, islpha() would return False.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wULYVechuT3T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6fad09fb-ff66-4f67-ac49-a8f331098082"
      },
      "source": [
        "# Only alphabetic\n",
        "\n",
        "string = \"\"\" OMG!!!! This is like    the best thing ever! \\t\\n\n",
        "Wow, such an amazing song! I;m hooked. This is top 5 definitely. I'm amazed. ? :)\n",
        "\"\"\"\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(string)\n",
        "\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Now, we remove tokens that are not alphabetic\n",
        "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() or lemma == '-PRON-']\n",
        "\n",
        "print(a_lemmas)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['OMG', 'this', 'be', 'like', 'the', 'good', 'thing', 'ever', 'wow', 'such', 'an', 'amazing', 'song', 'hook', 'this', 'be', 'top', 'definitely', '-PRON-', 'be', 'amazed']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PElUsErTvYZd"
      },
      "source": [
        "Stopwords: \n",
        "\n",
        "There are many words in english that occur extrely commonly, such as articles as a and the verb am, is, are, he, she. Not usefull for ML.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK_IdpT-wJuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "155fecbb-99c9-4ec7-c957-8f1f72476a9c"
      },
      "source": [
        "# Stop words\n",
        "\n",
        "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "string = \"\"\" OMG!!!! This is like    the best thing ever! \\t\\n\n",
        "Wow, such an amazing song! I;m hooked. This is top 5 definitely. I'm amazed. ? :)\n",
        "\"\"\"\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(string)\n",
        "\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Now, we remove tokens that are not alphabetic\n",
        "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() or lemma == '-PRON-' and lemma not in stopwords]\n",
        "\n",
        "# Print lemmas\n",
        "print(a_lemmas)\n",
        "# Print string after text cleaning\n",
        "print(' '.join(a_lemmas))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['OMG', 'this', 'be', 'like', 'the', 'good', 'thing', 'ever', 'wow', 'such', 'an', 'amazing', 'song', 'hook', 'this', 'be', 'top', 'definitely', '-PRON-', 'be', 'amazed']\n",
            "OMG this be like the good thing ever wow such an amazing song hook this be top definitely -PRON- be amazed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNuj2_JEyoze"
      },
      "source": [
        "Take a look at the cleaned text; it is lowercased and devoid of numbers, punctuations and commonly used stopwords. Also, note that the word U.S. was present in the original text. Since it had periods in between, our text cleaning process completely removed it. This may not be ideal behavior. It is always advisable to use your custom functions in place of isalpha() for more nuanced cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx-GDzWfzDGQ"
      },
      "source": [
        "### Function of preprocess text from string to alphabetic lemmas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZAcjtQBzCLg"
      },
      "source": [
        "# Function to preprocess text\n",
        "def preprocess(text):\n",
        "  \t# Create Doc object\n",
        "    doc = nlp(text, disable=['ner', 'parser'])\n",
        "    # Generate lemmas\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    # Remove stopwords and non-alphabetic characters\n",
        "    a_lemmas = [lemma for lemma in lemmas \n",
        "            if lemma.isalpha() and lemma not in stopwords]\n",
        "\n",
        "# In pandas:\n",
        "# Apply preprocess to ted['transcript']\n",
        "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
        "print(ted['transcript'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C_USLIFziry"
      },
      "source": [
        "it is now in a good shape to perform operations such as vectorization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkSCuDeY3OPb"
      },
      "source": [
        "# POS tagging using spaCy\n",
        "\n",
        "(Part of speech tagging)\n",
        "\n",
        "Applications: \n",
        "\n",
        "The **bear** is a majestic animal \n",
        "\n",
        "Please **bear** with me\n",
        "\n",
        "Identifies one bear as a noun and other as a verb\n",
        "\n",
        "POST tagging also used in sentiment analysis, question answering, fake news and opinion spam detection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqPMqOWr5_m0"
      },
      "source": [
        "## POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqcyAbPe3roh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5584ce7-9834-402d-f4dc-b09a7ab16ea6"
      },
      "source": [
        "string = 'Hane is an amazing guitarist'\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(string)\n",
        "pos = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "print(pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Hane', 'PROPN'), ('is', 'AUX'), ('an', 'DET'), ('amazing', 'ADJ'), ('guitarist', 'NOUN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-yAWoQl5HEl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6f8f3c90-fe35-4904-b220-c43f839b1b4b"
      },
      "source": [
        "passage = 'He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet.'\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(passage)\n",
        "\n",
        "# Generate tokens and pos tags\n",
        "pos = [(token.text, token.pos_) for token in doc]\n",
        "print(pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB'), ('the', 'DET'), ('wearisomeness', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('life', 'NOUN'), (',', 'PUNCT'), ('where', 'ADV'), ('every', 'DET'), ('path', 'NOUN'), ('was', 'AUX'), ('an', 'DET'), ('improvisation', 'NOUN'), ('and', 'CCONJ'), ('a', 'DET'), ('considerable', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('one', 'NUM'), ('’s', 'PART'), ('waking', 'VERB'), ('life', 'NOUN'), ('was', 'AUX'), ('spent', 'VERB'), ('watching', 'VERB'), ('one', 'PRON'), ('’s', 'PART'), ('feet', 'NOUN'), ('.', 'PUNCT')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em-I8Vh06Bps"
      },
      "source": [
        "## Counting number of proper nouns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P2TU2v36EXy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c64d0f0b-daf3-496f-8095-3ea6a40cd310"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Returns number of proper nouns\n",
        "def proper_nouns(text, model=nlp):\n",
        "  \t# Create doc object\n",
        "    doc = model(text)\n",
        "    # Generate list of POS tags\n",
        "    pos = [token.pos_ for token in doc]\n",
        "    \n",
        "    # Return number of proper nouns\n",
        "    # pos is a list and it has a count method\n",
        "    return pos.count('PROPN')\n",
        "\n",
        "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwctFvvc6eJh"
      },
      "source": [
        "## Counting number of nouns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-2HWZhE6d4m"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Returns number of other nouns\n",
        "def nouns(text, model=nlp):\n",
        "  \t# Create doc object\n",
        "    doc = model(text)\n",
        "    # Generate list of POS tags\n",
        "    pos = [token.pos_ for token in doc]\n",
        "    \n",
        "    # Return number of other nouns\n",
        "    return pos.count('NOUN')\n",
        "\n",
        "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JounPzZN7cbV"
      },
      "source": [
        "# Named entity recognition (NER)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTvSSRlm7fkY"
      },
      "source": [
        "With NER we can build efficient search algorithms and question answering.\n",
        "\n",
        "Identifying and classifying named entities into predefined categories. Categories include person, organization , country, etc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngkDjoOA-gWW"
      },
      "source": [
        "## NER with SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWFquupl8rxc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84b29353-8a71-4d6a-af35-f1805fd9e91b"
      },
      "source": [
        "string = 'John Doe is a software engineer working at Google. He lives in France'\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Create a Doc object\n",
        "doc = nlp(string)\n",
        "\n",
        "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "print(ne)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('John Doe', 'PERSON'), ('Google', 'ORG'), ('France', 'GPE')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRVMBmLk-j3C"
      },
      "source": [
        "## Only find persons NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHlD_EDR-fah",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2578e56d-2c73-4791-a4b4-12d084a628cc"
      },
      "source": [
        "def find_persons(text):\n",
        "  # Create Doc object\n",
        "  doc = nlp(text)\n",
        "  \n",
        "  # Identify the persons\n",
        "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
        "  \n",
        "  # Return persons\n",
        "  return persons\n",
        "\n",
        "tc = \"It’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\"\n",
        "\n",
        "print(find_persons(tc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sheryl Sandberg', 'Mark Zuckerberg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWkE_7C_pD4D"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfkPGPptpIZG"
      },
      "source": [
        "# Building a bag of words with CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R-D1JSFpOB5"
      },
      "source": [
        "For any ML: Data must be in tabular form, training features must be numerical\n",
        "\n",
        "Bag of words that convert text documents into vectors.\n",
        "\n",
        "Bag of words model: \n",
        "\n",
        "1- Extract word tokens\n",
        "\n",
        "2- Compute frequency of word tokens\n",
        "\n",
        "3- Construct a word vector out of these frequencies and vocabulary of corpus.\n",
        "\n",
        "Of course we will have to preprocess for -no punctuations -no stopwords -lemmatization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncFJ4u3YtSGL"
      },
      "source": [
        "With CountVectorizer, automatically lowercases words and ignores single character tokens such as 'a'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYESV0ogqriF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ae6445ed-c99e-406d-93c8-003a7b24e704"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus = pd.Series(['The lion is the king of the jungle',\n",
        "                    'Lions have lifespans of a decade',\n",
        "                    'The lion is an endangered species'])\n",
        "\n",
        "# Let's ignore text pre-processing for now.\n",
        "# With CountVectorizer, automatically lowercases words and ignores single\n",
        "# character tokens such as 'a'.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bag_of_word_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# .toarray() because bag of words is a sparse matrix\n",
        "print(bag_of_word_matrix.toarray())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 1 1 1 0 1 0 1 0 3]\n",
            " [0 1 0 1 0 0 0 1 0 1 1 0 0]\n",
            " [1 0 1 0 1 0 0 0 1 0 0 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpnhVxC2vXz7"
      },
      "source": [
        "CountVectorizer doesn't necessarily index the vocabulary in alphabetical order.\n",
        "\n",
        "Map each feature index to its corresponding feature name from the vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFH78B2awk-b"
      },
      "source": [
        "## Creating a DataFrame with column names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VadSxoZ-uLpR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "eb7de1e1-1e59-4e5e-e1a4-3692faaad481"
      },
      "source": [
        "# map each feature index to its corresponding feature name from the vocabulary.\n",
        "\n",
        "corpus = pd.Series(['The lion is the king of the jungle',\n",
        "                    'Lions have lifespans of a decade',\n",
        "                    'The lion is an endangered species'])\n",
        "\n",
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert bow_matrix into a DataFrame\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
        "\n",
        "# Map the column names to vocabulary \n",
        "bow_df.columns = vectorizer.get_feature_names()\n",
        "\n",
        "# Print bow_df\n",
        "print(bow_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   an  decade  endangered  have  is  ...  lion  lions  of  species  the\n",
            "0   0       0           0     0   1  ...     1      0   1        0    3\n",
            "1   0       1           0     1   0  ...     0      1   1        0    0\n",
            "2   1       0           1     0   1  ...     1      0   0        1    1\n",
            "\n",
            "[3 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF9tokvAwoBT"
      },
      "source": [
        "# Building a BoW Naive Bayes classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mde3kiFiyzL2"
      },
      "source": [
        "CountVectorizer it's main job is to convert a corpus into a matrix of numerical vectors but can use with preprocessing as well.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1- Text preprocessing\n",
        "\n",
        "2- Building a bag-of-words model (or representation)\n",
        "\n",
        "3- Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9RSfQJvwsyb"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# The theacher doesn't preprocess lowercases because we are going to make\n",
        "# an ML for classification of Spam/Not spam. So lowercase=False\n",
        "# If lowercase=True then will convert every words into lowercase\n",
        "\n",
        "# So here we do not have lemmatization and not lowering cases\n",
        "vectorizer = CountVectorizer(strip_accents='ascii',stop_words='english',lowercase=False)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, stratify=y)\n",
        "\n",
        "# Generate matrix of word vectors+\n",
        "# There were words present in X_test that were not in X_train. \n",
        "# CountVectorizer chose to ignore them in order to ensure that \n",
        "# the dimensions of both sets remain the same.\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# We will use naive Bayes Classifier\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_bow, y_train)\n",
        "\n",
        "accuracy = clf.score(X_test_bow, y_test)\n",
        "\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMjl2CFz28DJ"
      },
      "source": [
        "# Building n-gram models with CountVectorizer\n",
        "\n",
        "e.g. The movie was good and not boring. The movie was not good and boring, will get the same BoW vector therefore the model would not be accurate.\n",
        "\n",
        "Context of the words is lost.\n",
        "\n",
        "Typical bag-of-words is an 1-gram matrix.\n",
        "\n",
        "Use n-grams to catch more context.\n",
        "\n",
        "Adding higher order n-grams increases substantially the number of dimensions even more.  leading to increased computational costs and a problem known as the curse of dimensionality.\n",
        "\n",
        "**Higher order n-grams are rare**. **Keep n small**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjBH1XOP6Z68"
      },
      "source": [
        "# Generate n-grams upto n=2\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
        "\n",
        "# Generate n-grams upto n=3\n",
        "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuNCayh6_rSU"
      },
      "source": [
        "# Bag-of-words vs TF-IDF\n",
        "\n",
        "with BoW you create vectors from text corpus. These vectors are arrays for machine learning to fit and predict and the values of the matrix corresponds to the frequency of each words appearance inside the document. Is like a counting of frequency similar to histogram. BoW needs preprocessing that is lowercase, stop words and lemmatization. \n",
        "\n",
        "TF-IDF, on the contrary it automatically detect stop words. It creates an array of **weighted importance** for each words inside a text in comparison to the same text and with others texts inside the document. In many cases, TF-IDF because it's **weighted importance**, generates better performance in predictive models. TF-IDF is Term Frequency-inverse document frequency. **weighted importance** suggests the word exclusivity. TF-IDF also needs preprocessing that is lowercase, stop words(better) and lemmatization. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAF5LHRL9flB"
      },
      "source": [
        "# TF-IDF\n",
        "\n",
        "tfidf "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW9KmZ_lCoV5"
      },
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "TfidfVectorizer(strip_accents='ascii',stop_words='english',lowercase=False)\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(X_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc1gGWcPLwXj"
      },
      "source": [
        "# Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxu0F36vL0nR"
      },
      "source": [
        "How similar are two vectors(two documents) to each other?\n",
        "\n",
        "Cosine similarity. Cosine of the angle between to vectors.\n",
        "\n",
        "Cosine goes from -1 to 1. On NLP vector almost always are non negative weights. so cosine goes from 0(no similarity) to 1(identical).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV9v67cHOQsr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0fa4d6c1-c88e-491f-e6ee-37b357868f75"
      },
      "source": [
        "# Only accepts 2D arrays\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "A = (4,7,1)\n",
        "B = (5,2,3)\n",
        "\n",
        "score = cosine_similarity([A],[B])\n",
        "\n",
        "print(score)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.73881883]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejzSOJdYpOsF"
      },
      "source": [
        "Cosine Similarity Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN9sUPceovaH"
      },
      "source": [
        "# Initialize an instance of tf-idf Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate the tf-idf vectors for the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Compute and print the cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix,tfidf_matrix)\n",
        "print(cosine_sim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jOC-VL8p3Qg"
      },
      "source": [
        "# Example preprocessing, TF-IDF and cosine similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx9Ii0ETqBM1"
      },
      "source": [
        "This example it's used for recommender systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgp2PDaLvBXW"
      },
      "source": [
        "Notice how both linear_kernel and cosine_similarity produced the same result. However, linear_kernel took a smaller amount of time to execute. When you're working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to linear_kernel to improve performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-efaFVLp9SL"
      },
      "source": [
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Initialize the TfidfVectorizer \n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Construct the TF-IDF matrix\n",
        "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
        "\n",
        "# Generate the cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        " \n",
        "# Generate recommendations\n",
        "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwEP4uGmvhVk"
      },
      "source": [
        "# Generate mapping between titles and index\n",
        "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
        "\n",
        "def get_recommendations(title, cosine_sim, indices):\n",
        "    # Get index of movie that matches title\n",
        "    idx = indices[title]\n",
        "    # Sort the movies based on the similarity scores\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    # Get the scores for 10 most similar movies\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    # Get the movie indices\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    # Return the top 10 most similar movies\n",
        "    return metadata['title'].iloc[movie_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amsLsr13yICa"
      },
      "source": [
        "# Beyond n-grams: Word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGKtFJHmyRO2"
      },
      "source": [
        "I am happy\n",
        "\n",
        "I am joyous\n",
        "\n",
        "I am sad\n",
        "\n",
        "Would be vectorized and have the same score of cosine similarity. \n",
        "\n",
        "Happy joyous and sad are considered completely different words. But we now that happy and joyous are more similar than sad.\n",
        "\n",
        "Here is where Word embedding comes in. It maps words into an n-dimensional vector space produced with deep learning and huge amounts of data.\n",
        "Discern how similar two words are to each other\n",
        "\n",
        "Used to detect synonyms and antonyms\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI7RCyiGzU0g"
      },
      "source": [
        "We are going to use spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfbFeYHszUN8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "36990027-57e3-42b0-ae3e-6c622abe74d4"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "sent1 = nlp('I am happy')\n",
        "sent2 = nlp('I am joyous')\n",
        "sent3 = nlp('I am sad')\n",
        "\n",
        "sent1.similarity(sent2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9343192857712277"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVKb5Isq2NZV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "aeefe8c9-278a-42fc-c2e6-1d00c58e0f0c"
      },
      "source": [
        "sent1.similarity(sent3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8973932346386804"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWw_HeJa2tbr"
      },
      "source": [
        "word embedding for words (not sentences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5H9gyhn2sSs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e8813e5-cb54-4d18-8cd5-2f066790e470"
      },
      "source": [
        "# Create the doc object\n",
        "doc = nlp('I like apples and oranges')\n",
        "\n",
        "# Compute pairwise similarity scores\n",
        "for token1 in doc:\n",
        "  for token2 in doc:\n",
        "    print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I I 1.0\n",
            "I like 0.111476846\n",
            "I apples 0.0067715086\n",
            "I and -0.117666654\n",
            "I oranges -0.0021670829\n",
            "like I 0.111476846\n",
            "like like 1.0\n",
            "like apples 0.052829094\n",
            "like and -0.15288135\n",
            "like oranges -0.043253582\n",
            "apples I 0.0067715086\n",
            "apples like 0.052829094\n",
            "apples apples 1.0\n",
            "apples and -0.0011159935\n",
            "apples oranges 0.635426\n",
            "and I -0.117666654\n",
            "and like -0.15288135\n",
            "and apples -0.0011159935\n",
            "and and 1.0\n",
            "and oranges 0.089370795\n",
            "oranges I -0.0021670829\n",
            "oranges like -0.043253582\n",
            "oranges apples 0.635426\n",
            "oranges and 0.089370795\n",
            "oranges oranges 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}